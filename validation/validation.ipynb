{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.join('..','models')) #need this in order to get to the other file in other directory\n",
    "\n",
    "from simpleModel import SimpleNeuralNet\n",
    "\n",
    "sys.path.insert(0, os.path.join('..','preprocessing')) #need this in order to get to the other file in other directory\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, get_whole_Corona_dataset, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model_with_data(train_loader, vocabsize, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5, DEBUG_MODE = False):\n",
    "    '''\n",
    "    train with the given dataset\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples\n",
    "            if DEBUG_MODE:\n",
    "                if (i + 1) % print_epoch_mod == 0: \n",
    "                    # leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                    # for the other datasets so don't get spammed \n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(train_loader, test_loader, model, debug=False):\n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    k = 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            if debug:\n",
    "                print('data:', data)\n",
    "                print('data shape:', data.shape) # size of train data set\n",
    "                print('label:', labels)\n",
    "                print('label shape:', labels.shape)\n",
    "\n",
    "            outputs = model(data)\n",
    "            \n",
    "            if debug:\n",
    "                print('outputs:', outputs)\n",
    "                print('outputs data:', outputs.data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            if debug:\n",
    "                print('predicted:', predicted)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if debug:\n",
    "                print('label size:', labels.size(0))\n",
    "                print('correct labels:', (predicted == labels).sum().item())\n",
    "                break\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct / total\n",
    "\n",
    "        if debug:\n",
    "            print(\"train accuracy: {:.4f}%\".format(train_accuracy * 100))\n",
    "            print(\"test accuracy: {:.4f}%\".format(test_accuracy * 100))\n",
    "            print(\"difference in accuracies: {:.4f}%\".format(abs(test_accuracy - train_accuracy) * 100))\n",
    "\n",
    "        return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_epoch_diff(*args_list, debug=False):\n",
    "    X, Y, vectorizer_train = get_whole_Corona_dataset()\n",
    "    X = X.todense()\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    if debug:\n",
    "        print('X:', X.shape, type(X))\n",
    "        print('Y:', Y.shape, type(Y))\n",
    "\n",
    "    \n",
    "    # X,Y = getLiarText()\n",
    "    # X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    # X,Y = getFNNText()\n",
    "    # X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    # print('X type:', type(X))\n",
    "    # print('X:', X)\n",
    "    # print('Y type:', type(Y))\n",
    "    # print('Y:', Y)\n",
    "\n",
    "    numFold = 5\n",
    "    epoch_list = np.array([5, 10, 25, 50, 100])\n",
    "\n",
    "    total_train_acc_list = []\n",
    "    total_test_acc_list = []\n",
    "\n",
    "    for n_epoch in epoch_list:\n",
    "        print('beginning a test for', n_epoch, \"epoches\")\n",
    "        skf = StratifiedKFold(n_splits=numFold)\n",
    "        skf.get_n_splits(X, Y)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_test_acc = 0\n",
    "        \n",
    "        for i, (train_ind, test_ind) in enumerate(skf.split(X, Y), start=1):\n",
    "            X_train, X_test = X[train_ind], X[test_ind]\n",
    "            Y_train, Y_test = Y[train_ind], Y[test_ind]\n",
    "\n",
    "            if debug:\n",
    "                print('X train shape:', X_train.shape)\n",
    "                print('X test shape:', X_test.shape)\n",
    "                print('Y train shape:', Y_train.shape)\n",
    "                print('Y test shape:', Y_test.shape)\n",
    "\n",
    "            # transform our training and test data into tensors for the classifier to learn off of\n",
    "            X_train_tensor = torch.from_numpy(X_train).float()\n",
    "            Y_train_tensor = torch.from_numpy(Y_train)\n",
    "            X_test_tensor = torch.from_numpy(X_test).float()\n",
    "            Y_test_tensor = torch.from_numpy(Y_test)\n",
    "            \n",
    "            device = torch.device(\"cpu\")\n",
    "            # use TensorDataset to be able to use our DataLoader\n",
    "            train_data = torch.utils.data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "            train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=False)\n",
    "            test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "            test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "            vocabsize = X_train.shape[1]\n",
    "\n",
    "            model = train_simple_model_with_data(train_loader, vocabsize, num_epochs=n_epoch)\n",
    "            train_acc, test_acc = get_model_accuracy(train_loader, test_loader, model)\n",
    "\n",
    "            total_train_acc += train_acc\n",
    "            total_test_acc += test_acc\n",
    "\n",
    "            if debug:\n",
    "                print(i, '-Fold:', sep='')\n",
    "                print(\"train accuracy: {:.4f}%\".format(train_acc * 100))\n",
    "                print(\"test accuracy: {:.4f}%\".format(test_acc * 100))\n",
    "                print(\"difference in accuracies: {:.4f}%\".format(abs(test_acc - train_acc) * 100))\n",
    "\n",
    "        total_train_acc /= numFold\n",
    "        total_test_acc /= numFold\n",
    "\n",
    "        total_train_acc_list.append(total_train_acc)\n",
    "        total_test_acc_list.append(total_test_acc)\n",
    "\n",
    "        if debug:\n",
    "            print(\"final train accuracy: {:.4f}%\".format(total_train_acc * 100))\n",
    "            print(\"final test accuracy: {:.4f}%\".format(total_test_acc * 100))\n",
    "\n",
    "    if debug:\n",
    "        print(\"tr:\", total_train_acc_list)\n",
    "        print(\"te:\", total_test_acc_list)\n",
    "\n",
    "    total_train_acc_list = np.array(total_train_acc_list) * 100\n",
    "    total_test_acc_list = np.array(total_test_acc_list) * 100\n",
    "\n",
    "    plt_data = pd.DataFrame({'Number of Epoch': epoch_list, 'Train Accuracy': total_train_acc_list, \\\n",
    "                             'Test Accuracy': total_test_acc_list})\n",
    "    plt.plot('Number of Epoch', 'Train Accuracy', data=plt_data, marker='.', markerfacecolor='skyblue', markersize=10, color='skyblue', linewidth=2)\n",
    "    plt.plot('Number of Epoch', 'Test Accuracy', data=plt_data, marker='.', markerfacecolor='olive', markersize=10, color='olive', linewidth=2)\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                  title  \\\n0     Due to the recent outbreak for the Coronavirus...   \n1                                                   NaN   \n2                                                   NaN   \n3                                                   NaN   \n4                                                   NaN   \n...                                                 ...   \n1159   Could the Power of the Sun Slow the Coronavirus?   \n1160  Key evidence for coronavirus spread is flawed ...   \n1161  Summer Heat May Not Diminish Coronavirus Strength   \n1162               How Long Will a Vaccine Really Take?   \n1163  Why Funding the Covid-19 Response Could Be the...   \n\n                                                   text  \\\n0     You just need to add water, and the drugs and ...   \n1     Hydroxychloroquine has been shown to have a 10...   \n2     Fact: Hydroxychloroquine has been shown to hav...   \n3     The Corona virus is a man made virus created i...   \n4     Doesn’t @BillGates finance research at the Wuh...   \n...                                                 ...   \n1159  A study suggests that ultraviolet rays could s...   \n1160  Last week, a medical journal reported that a b...   \n1161  A new report, sent to the White House science ...   \n1162  A vaccine would be the ultimate weapon against...   \n1163  Developing and delivering coronavirus vaccines...   \n\n                         source label  \n0     coronavirusmedicalkit.com  Fake  \n1                  RudyGiuliani  Fake  \n2                   CharlieKirk  Fake  \n3       JoanneWrightForCongress  Fake  \n4       JoanneWrightForCongress  Fake  \n...                         ...   ...  \n1159   https://www.nytimes.com/  TRUE  \n1160   https://www.nytimes.com/  TRUE  \n1161   https://www.nytimes.com/  TRUE  \n1162   https://www.nytimes.com/  TRUE  \n1163   https://www.nytimes.com/  TRUE  \n\n[1164 rows x 4 columns]\n                                                  title  \\\n0     Due to the recent outbreak for the Coronavirus...   \n1                                                   NaN   \n2                                                   NaN   \n3                                                   NaN   \n4                                                   NaN   \n...                                                 ...   \n1159   Could the Power of the Sun Slow the Coronavirus?   \n1160  Key evidence for coronavirus spread is flawed ...   \n1161  Summer Heat May Not Diminish Coronavirus Strength   \n1162               How Long Will a Vaccine Really Take?   \n1163  Why Funding the Covid-19 Response Could Be the...   \n\n                                                   text  \\\n0     You just need to add water, and the drugs and ...   \n1     Hydroxychloroquine has been shown to have a 10...   \n2     Fact: Hydroxychloroquine has been shown to hav...   \n3     The Corona virus is a man made virus created i...   \n4     Doesn’t @BillGates finance research at the Wuh...   \n...                                                 ...   \n1159  A study suggests that ultraviolet rays could s...   \n1160  Last week, a medical journal reported that a b...   \n1161  A new report, sent to the White House science ...   \n1162  A vaccine would be the ultimate weapon against...   \n1163  Developing and delivering coronavirus vaccines...   \n\n                         source label  \n0     coronavirusmedicalkit.com  Fake  \n1                  RudyGiuliani  Fake  \n2                   CharlieKirk  Fake  \n3       JoanneWrightForCongress  Fake  \n4       JoanneWrightForCongress  Fake  \n...                         ...   ...  \n1159   https://www.nytimes.com/  TRUE  \n1160   https://www.nytimes.com/  TRUE  \n1161   https://www.nytimes.com/  TRUE  \n1162   https://www.nytimes.com/  TRUE  \n1163   https://www.nytimes.com/  TRUE  \n\n[1164 rows x 4 columns]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1164",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 1164 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-460f7ce0c513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchart_epoch_diff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-03821b303d99>\u001b[0m in \u001b[0;36mchart_epoch_diff\u001b[1;34m(debug, *args_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchart_epoch_diff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_whole_Corona_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\CynSchool\\CS 175\\Verity\\preprocessing\\covidPreprocess.py\u001b[0m in \u001b[0;36mget_whole_Corona_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m#these are stored as NaN so replace with an empty string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mftext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mftext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[0mnanText\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1071\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3736\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3738\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3740\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1164"
     ]
    }
   ],
   "source": [
    "chart_epoch_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_num_layer_diff(*args_list, debug=False):\n",
    "    X, Y, vectorizer_train = get_whole_Corona_dataset()\n",
    "    X = X.todense()\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    if debug:\n",
    "        print('X:', X.shape, type(X))\n",
    "        print('Y:', Y.shape, type(Y))\n",
    "\n",
    "    \n",
    "    # X,Y = getLiarText()\n",
    "    # X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    # X,Y = getFNNText()\n",
    "    # X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    # print('X type:', type(X))\n",
    "    # print('X:', X)\n",
    "    # print('Y type:', type(Y))\n",
    "    # print('Y:', Y)\n",
    "\n",
    "    numFold = 5\n",
    "    epoch_list = np.array([5, 10, 25, 50, 100])\n",
    "\n",
    "    total_train_acc_list = []\n",
    "    total_test_acc_list = []\n",
    "\n",
    "    for n_epoch in epoch_list:\n",
    "        print('beginning a test for', n_epoch, \"epoches\")\n",
    "        skf = StratifiedKFold(n_splits=numFold)\n",
    "        skf.get_n_splits(X, Y)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_test_acc = 0\n",
    "        \n",
    "        for i, (train_ind, test_ind) in enumerate(skf.split(X, Y), start=1):\n",
    "            X_train, X_test = X[train_ind], X[test_ind]\n",
    "            Y_train, Y_test = Y[train_ind], Y[test_ind]\n",
    "\n",
    "            if debug:\n",
    "                print('X train shape:', X_train.shape)\n",
    "                print('X test shape:', X_test.shape)\n",
    "                print('Y train shape:', Y_train.shape)\n",
    "                print('Y test shape:', Y_test.shape)\n",
    "\n",
    "            # transform our training and test data into tensors for the classifier to learn off of\n",
    "            X_train_tensor = torch.from_numpy(X_train).float()\n",
    "            Y_train_tensor = torch.from_numpy(Y_train)\n",
    "            X_test_tensor = torch.from_numpy(X_test).float()\n",
    "            Y_test_tensor = torch.from_numpy(Y_test)\n",
    "            \n",
    "            device = torch.device(\"cpu\")\n",
    "            # use TensorDataset to be able to use our DataLoader\n",
    "            train_data = torch.utils.data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "            train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=False)\n",
    "            test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "            test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "            vocabsize = X_train.shape[1]\n",
    "\n",
    "            model = train_simple_model_with_data(train_loader, vocabsize, num_epochs=n_epoch)\n",
    "            train_acc, test_acc = get_model_accuracy(train_loader, test_loader, model)\n",
    "\n",
    "            total_train_acc += train_acc\n",
    "            total_test_acc += test_acc\n",
    "\n",
    "            if debug:\n",
    "                print(i, '-Fold:', sep='')\n",
    "                print(\"train accuracy: {:.4f}%\".format(train_acc * 100))\n",
    "                print(\"test accuracy: {:.4f}%\".format(test_acc * 100))\n",
    "                print(\"difference in accuracies: {:.4f}%\".format(abs(test_acc - train_acc) * 100))\n",
    "\n",
    "        total_train_acc /= numFold\n",
    "        total_test_acc /= numFold\n",
    "\n",
    "        total_train_acc_list.append(total_train_acc)\n",
    "        total_test_acc_list.append(total_test_acc)\n",
    "\n",
    "        if debug:\n",
    "            print(\"final train accuracy: {:.4f}%\".format(total_train_acc * 100))\n",
    "            print(\"final test accuracy: {:.4f}%\".format(total_test_acc * 100))\n",
    "\n",
    "    if debug:\n",
    "        print(\"tr:\", total_train_acc_list)\n",
    "        print(\"te:\", total_test_acc_list)\n",
    "\n",
    "    total_train_acc_list = np.array(total_train_acc_list) * 100\n",
    "    total_test_acc_list = np.array(total_test_acc_list) * 100\n",
    "\n",
    "    plt_data = pd.DataFrame({'Number of Epoch': epoch_list, 'Train Accuracy': total_train_acc_list, \\\n",
    "                             'Test Accuracy': total_test_acc_list})\n",
    "    plt.plot('Number of Epoch', 'Train Accuracy', data=plt_data, marker='.', markerfacecolor='skyblue', markersize=10, color='skyblue', linewidth=2)\n",
    "    plt.plot('Number of Epoch', 'Test Accuracy', data=plt_data, marker='.', markerfacecolor='olive', markersize=10, color='olive', linewidth=2)\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ]
}