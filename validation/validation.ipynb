{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../models/') #need this in order to get to the other file in other directory\n",
    "\n",
    "from simpleModel import SimpleNeuralNet\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, get_whole_Corona_dataset, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model_with_data(train_loader, vocabsize, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5, DEBUG_MODE = False):\n",
    "    '''\n",
    "    train with the given dataset\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples\n",
    "            if DEBUG_MODE:\n",
    "                if (i + 1) % print_epoch_mod == 0: \n",
    "                    # leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                    # for the other datasets so don't get spammed \n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(train_loader, test_loader, model, debug=False):\n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    k = 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            if debug:\n",
    "                print('data:', data)\n",
    "                print('data shape:', data.shape) # size of train data set\n",
    "                print('label:', labels)\n",
    "                print('label shape:', labels.shape)\n",
    "\n",
    "            outputs = model(data)\n",
    "            \n",
    "            if debug:\n",
    "                print('outputs:', outputs)\n",
    "                print('outputs data:', outputs.data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            if debug:\n",
    "                print('predicted:', predicted)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if debug:\n",
    "                print('label size:', labels.size(0))\n",
    "                print('correct labels:', (predicted == labels).sum().item())\n",
    "                break\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct / total\n",
    "\n",
    "        if debug:\n",
    "            print(\"train accuracy: {:.4f}%\".format(train_accuracy * 100))\n",
    "            print(\"test accuracy: {:.4f}%\".format(test_accuracy * 100))\n",
    "            print(\"difference in accuracies: {:.4f}%\".format(abs(test_accuracy - train_accuracy) * 100))\n",
    "\n",
    "        return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_epoch_diff(*args_list, debug=False):\n",
    "    X, Y, vectorizer_train = get_whole_Corona_dataset()\n",
    "    X = X.todense()\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    if debug:\n",
    "        print('X:', X.shape, type(X))\n",
    "        print('Y:', Y.shape, type(Y))\n",
    "\n",
    "    \n",
    "    # X,Y = getLiarText()\n",
    "    # X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    # X,Y = getFNNText()\n",
    "    # X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    # print('X type:', type(X))\n",
    "    # print('X:', X)\n",
    "    # print('Y type:', type(Y))\n",
    "    # print('Y:', Y)\n",
    "\n",
    "    numFold = 5\n",
    "    epoch_list = np.array([5, 10, 25, 50, 75, 100])\n",
    "\n",
    "    total_train_acc_list = []\n",
    "    total_test_acc_list = []\n",
    "\n",
    "    for n_epoch in epoch_list:\n",
    "        print('begnning testing for', n_epoch, \"epoches\")\n",
    "        skf = StratifiedKFold(n_splits=numFold)\n",
    "        skf.get_n_splits(X, Y)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_test_acc = 0\n",
    "        \n",
    "        for i, (train_ind, test_ind) in enumerate(skf.split(X, Y), start=1):\n",
    "            X_train, X_test = X[train_ind], X[test_ind]\n",
    "            Y_train, Y_test = Y[train_ind], Y[test_ind]\n",
    "\n",
    "            if debug:\n",
    "                print('X train shape:', X_train.shape)\n",
    "                print('X test shape:', X_test.shape)\n",
    "                print('Y train shape:', Y_train.shape)\n",
    "                print('Y test shape:', Y_test.shape)\n",
    "\n",
    "            # transform our training and test data into tensors for the classifier to learn off of\n",
    "            X_train_tensor = torch.from_numpy(X_train).float()\n",
    "            Y_train_tensor = torch.from_numpy(Y_train)\n",
    "            X_test_tensor = torch.from_numpy(X_test).float()\n",
    "            Y_test_tensor = torch.from_numpy(Y_test)\n",
    "            \n",
    "            device = torch.device(\"cpu\")\n",
    "            # use TensorDataset to be able to use our DataLoader\n",
    "            train_data = torch.utils.data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "            train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=False)\n",
    "            test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "            test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "            vocabsize = X_train.shape[1]\n",
    "\n",
    "            model = train_simple_model_with_data(train_loader, vocabsize, num_epochs=n_epoch)\n",
    "            train_acc, test_acc = get_model_accuracy(train_loader, test_loader, model)\n",
    "\n",
    "            total_train_acc += train_acc\n",
    "            total_test_acc += test_acc\n",
    "\n",
    "            if debug:\n",
    "                print(i, '-Fold:', sep='')\n",
    "                print(\"train accuracy: {:.4f}%\".format(train_acc * 100))\n",
    "                print(\"test accuracy: {:.4f}%\".format(test_acc * 100))\n",
    "                print(\"difference in accuracies: {:.4f}%\".format(abs(test_acc - train_acc) * 100))\n",
    "\n",
    "        total_train_acc /= numFold\n",
    "        total_test_acc /= numFold\n",
    "\n",
    "        total_train_acc_list.append(total_train_acc)\n",
    "        total_test_acc_list.append(total_test_acc)\n",
    "\n",
    "        if debug:\n",
    "            print(\"final train accuracy: {:.4f}%\".format(total_train_acc * 100))\n",
    "            print(\"final test accuracy: {:.4f}%\".format(total_test_acc * 100))\n",
    "\n",
    "    print(\"tr:\", total_train_acc_list)\n",
    "    print(\"te:\", total_test_acc_list)\n",
    "    \n",
    "\n",
    "    # for args in args_list:\n",
    "    #     train_loader, test_loader, model = trainSimpleModel(*args_list)\n",
    "    #     train_acc, test_acc = testModel(train_loader, test_loader, model, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy: 68.4814%\n",
      "test accuracy: 76.5714%\n",
      "difference in accuracies: 8.0901%\n",
      "1-Fold:\n",
      "train accuracy: 68.4814%\n",
      "test accuracy: 76.5714%\n",
      "difference in accuracies: 8.0901%\n",
      "train accuracy: 68.7679%\n",
      "test accuracy: 58.2857%\n",
      "difference in accuracies: 10.4822%\n",
      "2-Fold:\n",
      "train accuracy: 68.7679%\n",
      "test accuracy: 58.2857%\n",
      "difference in accuracies: 10.4822%\n",
      "train accuracy: 65.3295%\n",
      "test accuracy: 52.5714%\n",
      "difference in accuracies: 12.7581%\n",
      "3-Fold:\n",
      "train accuracy: 65.3295%\n",
      "test accuracy: 52.5714%\n",
      "difference in accuracies: 12.7581%\n",
      "train accuracy: 70.1001%\n",
      "test accuracy: 47.1264%\n",
      "difference in accuracies: 22.9737%\n",
      "4-Fold:\n",
      "train accuracy: 70.1001%\n",
      "test accuracy: 47.1264%\n",
      "difference in accuracies: 22.9737%\n",
      "train accuracy: 68.6695%\n",
      "test accuracy: 50.5747%\n",
      "difference in accuracies: 18.0948%\n",
      "5-Fold:\n",
      "train accuracy: 68.6695%\n",
      "test accuracy: 50.5747%\n",
      "difference in accuracies: 18.0948%\n",
      "final train accuracy: 68.2697%\n",
      "final test accuracy: 57.0259%\n"
     ]
    }
   ],
   "source": [
    "chart_epoch_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}