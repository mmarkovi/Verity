{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "import covidPreprocess\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText, get_whole_Corona_dataset\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    DEBUG_MODE = False\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    # transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    # use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "    # train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "    # test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples\n",
    "            if DEBUG_MODE:\n",
    "                if (i + 1) % print_epoch_mod == 0: \n",
    "                    # leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                    # for the other datasets so don't get spammed \n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "    return train_loader, test_loader, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model_with_data(X, Y, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    DEBUG_MODE = False\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    # transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    # use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "    # train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "    # test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples\n",
    "            if DEBUG_MODE:\n",
    "                if (i + 1) % print_epoch_mod == 0: \n",
    "                    # leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                    # for the other datasets so don't get spammed \n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "    return train_loader, test_loader, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(train_loader, test_loader, model, debug=False):\n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    k = 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            if debug:\n",
    "                print('data:', data)\n",
    "                print('data shape:', data.shape) # size of train data set\n",
    "                print('label:', labels)\n",
    "                print('label shape:', labels.shape)\n",
    "\n",
    "            outputs = model(data)\n",
    "            \n",
    "            if debug:\n",
    "                print('outputs:', outputs)\n",
    "                print('outputs data:', outputs.data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            if debug:\n",
    "                print('predicted:', predicted)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if debug:\n",
    "                print('label size:', labels.size(0))\n",
    "                print('correct labels:', (predicted == labels).sum().item())\n",
    "                break\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct / total\n",
    "\n",
    "        print(\"train accuracy: {:.4f}%\".format(train_accuracy * 100))\n",
    "        print(\"test accuracy: {:.4f}%\".format(test_accuracy * 100))\n",
    "        print(\"difference in accuracies: {:.4f}%\".format(abs(test_accuracy - train_accuracy) * 100))\n",
    "\n",
    "        return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1164",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 1164 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-33270341e367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msave_model_and_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-33270341e367>\u001b[0m in \u001b[0;36msave_model_and_vectorizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_model_and_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_whole_Corona_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\CynSchool\\CS 175\\Verity\\preprocessing\\covidPreprocess.py\u001b[0m in \u001b[0;36mget_whole_Corona_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mcoronafile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m43\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fake'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mcoronafile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m131\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mcoronafile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m242\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fake'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1071\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3736\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3738\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3740\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1164"
     ]
    }
   ],
   "source": [
    "def save_model_and_vectorizer():\n",
    "    X, Y, vectorizer_train = get_whole_Corona_dataset()\n",
    "\n",
    "    print(type(X))\n",
    "\n",
    "save_model_and_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy: 74.5704%\ntest accuracy: 67.4685%\ndifference in accuracies: 7.1019%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7457044673539519, 0.6746849942726232)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_loader, test_loader, model = trainSimpleModel('corona', num_epochs=10)\n",
    "testModel(train_loader, test_loader, model, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy: 83.1615%\ntest accuracy: 74.6850%\ndifference in accuracies: 8.4765%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.8316151202749141, 0.7468499427262314)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_loader, test_loader, model = trainSimpleModel('corona', num_epochs=20)\n",
    "testModel(train_loader, test_loader, model, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy: 97.9381%\ntest accuracy: 83.5052%\ndifference in accuracies: 14.4330%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.979381443298969, 0.8350515463917526)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_loader, test_loader, model = trainSimpleModel('corona', num_epochs=30)\n",
    "testModel(train_loader, test_loader, model, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trainAndTestSimpleModel' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d45d8cd040b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainAndTestSimpleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corona'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trainAndTestSimpleModel' is not defined"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6330\n",
      "Accuracy of the network on the 10000 test images: 88.31615120274914 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.20160366552119 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.54524627720504 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.20160366552119 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.54524627720504 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/5], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/5], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/5], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/5], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/5], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/5], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/5], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/5], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/5], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/5], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/5], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/5], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/5], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/5], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/5], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/5], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/5], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/5], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/5], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/5], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/5], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/5], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/5], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/5], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/5], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/5], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/5], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/5], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/5], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/5], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/5], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/5], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/5], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/5], Step [900/941], Loss: 0.6396\n",
      "Test accuracy of the network: 68.48341232227489 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('liar', num_epochs=5, print_epoch_mod=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/10], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/10], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/10], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/10], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/10], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/10], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/10], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/10], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/10], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/10], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/10], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/10], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/10], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/10], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/10], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/10], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/10], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/10], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/10], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/10], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/10], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/10], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/10], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/10], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/10], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/10], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/10], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/10], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/10], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/10], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/10], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/10], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/10], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/10], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/10], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/10], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/10], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/10], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/10], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/10], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/10], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/10], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/10], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/10], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/10], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/10], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [900/941], Loss: 0.6391\n",
      "Test accuracy of the network: 68.00947867298578 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('liar', num_epochs=10, print_epoch_mod=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}