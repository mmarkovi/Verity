{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/5], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6444\n",
      "Test accuracy of the network: 65.75028636884306 %\n",
      "Train accuracy of the network: 76.97594501718213 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=5)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/10], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/10], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/10], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/10], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/10], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/10], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/10], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/10], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/10], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/10], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/10], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/10], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/10], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/10], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/10], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/10], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/10], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/10], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/10], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/10], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/10], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/10], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/10], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/10], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/10], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/10], Step [15/19], Loss: 0.6455\n",
      "Test accuracy of the network: 67.46849942726232 %\n",
      "Train accuracy of the network: 74.5704467353952 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=10)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/20], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/20], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/20], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/20], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/20], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/20], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/20], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/20], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/20], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/20], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/20], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/20], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/20], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/20], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/20], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/20], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/20], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/20], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/20], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/20], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/20], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/20], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/20], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/20], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/20], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/20], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/20], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/20], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/20], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/20], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/20], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/20], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/20], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/20], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/20], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/20], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/20], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/20], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/20], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/20], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/20], Step [15/19], Loss: 0.6403\n",
      "Test accuracy of the network: 78.35051546391753 %\n",
      "Train accuracy of the network: 88.65979381443299 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=20)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/30], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/30], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/30], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/30], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/30], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/30], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/30], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/30], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/30], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/30], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/30], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/30], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/30], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/30], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/30], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/30], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/30], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/30], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/30], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/30], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/30], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/30], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/30], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/30], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/30], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/30], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/30], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/30], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/30], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/30], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/30], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/30], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/30], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/30], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/30], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/30], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/30], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/30], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/30], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/30], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/30], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/30], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/30], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/30], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/30], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/30], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/30], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/30], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/30], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/30], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/30], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/30], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/30], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/30], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/30], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/30], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/30], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/30], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/30], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/30], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/30], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/30], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/30], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/30], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/30], Step [15/19], Loss: 0.6351\n",
      "Test accuracy of the network: 86.5979381443299 %\n",
      "Train accuracy of the network: 98.62542955326461 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=30)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6330\n",
      "Test accuracy of the network: 88.31615120274914 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=40)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=50)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=60)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=75)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=100)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_used = [5, 10, 20, 30, 40, 50, 60, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/5], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/5], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/5], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/5], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/5], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/5], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/5], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/5], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/5], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/5], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/5], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/5], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/5], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/5], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/5], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/5], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/5], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/5], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/5], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/5], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/5], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/5], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/5], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/5], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/5], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/5], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/5], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/5], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/5], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/5], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/5], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/5], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/5], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/5], Step [900/941], Loss: 0.6396\n",
      "Test accuracy of the network: 68.48341232227489 %\n",
      "Train accuracy of the network: 77.77039596066967 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/10], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/10], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/10], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/10], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/10], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/10], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/10], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/10], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/10], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/10], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/10], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/10], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/10], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/10], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/10], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/10], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/10], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/10], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/10], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/10], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/10], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/10], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/10], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/10], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/10], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/10], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/10], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/10], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/10], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/10], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/10], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/10], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/10], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/10], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/10], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/10], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/10], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/10], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/10], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/10], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/10], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/10], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/10], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/10], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/10], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/10], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [900/941], Loss: 0.6391\n",
      "Test accuracy of the network: 68.00947867298578 %\n",
      "Train accuracy of the network: 78.22880680308265 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/20], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/20], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/20], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/20], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/20], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/20], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/20], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/20], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/20], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/20], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/20], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/20], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/20], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/20], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/20], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/20], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/20], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/20], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/20], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/20], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/20], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/20], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/20], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/20], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/20], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/20], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/20], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/20], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/20], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/20], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/20], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/20], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/20], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/20], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/20], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/20], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/20], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/20], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/20], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/20], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/20], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/20], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/20], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/20], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/20], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/20], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/20], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/20], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/20], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/20], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/20], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/20], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/20], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/20], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/20], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/20], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/20], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/20], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/20], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/20], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/20], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/20], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/20], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/20], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/20], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/20], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/20], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/20], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/20], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/20], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/20], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [900/941], Loss: 0.6399\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 83.23810789263885 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/30], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/30], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/30], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/30], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/30], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/30], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/30], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/30], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/30], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/30], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/30], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/30], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/30], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/30], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/30], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/30], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/30], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/30], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/30], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/30], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/30], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/30], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/30], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/30], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/30], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/30], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/30], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/30], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/30], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/30], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/30], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/30], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/30], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/30], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/30], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/30], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/30], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/30], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/30], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/30], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/30], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/30], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/30], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/30], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/30], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/30], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/30], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/30], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/30], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/30], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/30], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/30], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/30], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/30], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/30], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/30], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/30], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/30], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/30], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/30], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/30], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/30], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/30], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/30], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/30], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/30], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/30], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/30], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/30], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/30], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/30], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/30], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/30], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/30], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/30], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/30], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/30], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/30], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/30], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/30], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/30], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/30], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/30], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/30], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/30], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/30], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/30], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/30], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/30], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/30], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/30], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/30], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/30], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/30], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/30], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/30], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/30], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/30], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/30], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/30], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/30], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/30], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/30], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/30], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/30], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/30], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/30], Step [900/941], Loss: 0.6356\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 90.10762689343609 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/40], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/40], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/40], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/40], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/40], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/40], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/40], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/40], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/40], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/40], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/40], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/40], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/40], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/40], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/40], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/40], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/40], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/40], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/40], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/40], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/40], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/40], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/40], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/40], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/40], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/40], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/40], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/40], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/40], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/40], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/40], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/40], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/40], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/40], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/40], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/40], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/40], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/40], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/40], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/40], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/40], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/40], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/40], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/40], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/40], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/40], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/40], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/40], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/40], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/40], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/40], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/40], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/40], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/40], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/40], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/40], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/40], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/40], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/40], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/40], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/40], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/40], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/40], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/40], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/40], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/40], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/40], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/40], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/40], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/40], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/40], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/40], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/40], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/40], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/40], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/40], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/40], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/40], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/40], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/40], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/40], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/40], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/40], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/40], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/40], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/40], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/40], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/40], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/40], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/40], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/40], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/40], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/40], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/40], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/40], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/40], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/40], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/40], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/40], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/40], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/40], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/40], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/40], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/40], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/40], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/40], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/40], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/40], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/40], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/40], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/40], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/40], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/40], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/40], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/40], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/40], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/40], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/40], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/40], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/40], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/40], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/40], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/40], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/40], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/40], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 67.53554502369668 %\n",
      "Train accuracy of the network: 92.33988838692532 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/50], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/50], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/50], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/50], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/50], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/50], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/50], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/50], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/50], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/50], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/50], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/50], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/50], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/50], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/50], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/50], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/50], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/50], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/50], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/50], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/50], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/50], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/50], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/50], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/50], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/50], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/50], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/50], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/50], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/50], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/50], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/50], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/50], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/50], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/50], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/50], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/50], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/50], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/50], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/50], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/50], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/50], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/50], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/50], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/50], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/50], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/50], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/50], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/50], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/50], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/50], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/50], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/50], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/50], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/50], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/50], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/50], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/50], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/50], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/50], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/50], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/50], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/50], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/50], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/50], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/50], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/50], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/50], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/50], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/50], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/50], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/50], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/50], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/50], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/50], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/50], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/50], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/50], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/50], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/50], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/50], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/50], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/50], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/50], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/50], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/50], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/50], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/50], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/50], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/50], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/50], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/50], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/50], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/50], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/50], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/50], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/50], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/50], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/50], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/50], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/50], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/50], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/50], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/50], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/50], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/50], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/50], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/50], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/50], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/50], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/50], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/50], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/50], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/50], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/50], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/50], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/50], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 66.66666666666667 %\n",
      "Train accuracy of the network: 93.58224820621844 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/60], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/60], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/60], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/60], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/60], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/60], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/60], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/60], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/60], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/60], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/60], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/60], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/60], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/60], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/60], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/60], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/60], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/60], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/60], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/60], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/60], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/60], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/60], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/60], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/60], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/60], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/60], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/60], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/60], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/60], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/60], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/60], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/60], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/60], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/60], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/60], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/60], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/60], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/60], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/60], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/60], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/60], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/60], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/60], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/60], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/60], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/60], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/60], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/60], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/60], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/60], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/60], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/60], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/60], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/60], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/60], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/60], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/60], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/60], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/60], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/60], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/60], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/60], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/60], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/60], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/60], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/60], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/60], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/60], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/60], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/60], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/60], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/60], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/60], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/60], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/60], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/60], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/60], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/60], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/60], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/60], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/60], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/60], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/60], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/60], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/60], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/60], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/60], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/60], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/60], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/60], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/60], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/60], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/60], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/60], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/60], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/60], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/60], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/60], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/60], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/60], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/60], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/60], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/60], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/60], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/60], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/60], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/60], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/60], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/60], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/60], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/60], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/60], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/60], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/60], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/60], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/60], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 66.5086887835703 %\n",
      "Train accuracy of the network: 94.41270263087962 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/75], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/75], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/75], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/75], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/75], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/75], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/75], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/75], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/75], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/75], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/75], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/75], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/75], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/75], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/75], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/75], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/75], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/75], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/75], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/75], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/75], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/75], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/75], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/75], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/75], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/75], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/75], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/75], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/75], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/75], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/75], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/75], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/75], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/75], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/75], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/75], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/75], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/75], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/75], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/75], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/75], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/75], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/75], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/75], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/75], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/75], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/75], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/75], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/75], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/75], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/75], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/75], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/75], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/75], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/75], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/75], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/75], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/75], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/75], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/75], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/75], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/75], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/75], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/75], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/75], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/75], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/75], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/75], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/75], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/75], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/75], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/75], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/75], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/75], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/75], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/75], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/75], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/75], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/75], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/75], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/75], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/75], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/75], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/75], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/75], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/75], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/75], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/75], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/75], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/75], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/75], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/75], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/75], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/75], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/75], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/75], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/75], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/75], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/75], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/75], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/75], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/75], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/75], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/75], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/75], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/75], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/75], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/75], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/75], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/75], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/75], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/75], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/75], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/75], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/75], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/75], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/75], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/75], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/75], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 64.77093206951027 %\n",
      "Train accuracy of the network: 94.93090619186819 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/100], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/100], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/100], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/100], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/100], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/100], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/100], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/100], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/100], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/100], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/100], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/100], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/100], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/100], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/100], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/100], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/100], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/100], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/100], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/100], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/100], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/100], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/100], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/100], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/100], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/100], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/100], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/100], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/100], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/100], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/100], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/100], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/100], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/100], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/100], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/100], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/100], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/100], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/100], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/100], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/100], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/100], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/100], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/100], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/100], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/100], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/100], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/100], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/100], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/100], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/100], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/100], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/100], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/100], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/100], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/100], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/100], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/100], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/100], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/100], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/100], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/100], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/100], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/100], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/100], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/100], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/100], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/100], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/100], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/100], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/100], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/100], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/100], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/100], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/100], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/100], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/100], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/100], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/100], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/100], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/100], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/100], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/100], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/100], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/100], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/100], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/100], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/100], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/100], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/100], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/100], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/100], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/100], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/100], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/100], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/100], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/100], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/100], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/100], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/100], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/100], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/100], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/100], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/100], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/100], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/100], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/100], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/100], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/100], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/100], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/100], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/100], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/100], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/100], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/100], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/100], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [76/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [76/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [76/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [76/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [76/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [76/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [77/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [77/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [77/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [77/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [77/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [77/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [77/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [78/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [78/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [78/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [78/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [78/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [78/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [78/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [200/941], Loss: 0.6343\n",
      "Epoch [79/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [79/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [79/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [79/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [79/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [79/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [80/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [80/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [80/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [80/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [80/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [80/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [81/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [81/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [81/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [81/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [81/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [81/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [82/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [82/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [82/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [82/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [82/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [82/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [83/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [83/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [83/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [83/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [83/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [83/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [84/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [84/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [84/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [84/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [84/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [84/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [84/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [85/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [85/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [85/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [85/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [85/100], Step [900/941], Loss: 0.6348\n",
      "Epoch [86/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [86/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [86/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [86/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [86/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [86/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [86/100], Step [900/941], Loss: 0.6349\n",
      "Epoch [87/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [87/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [87/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [87/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [87/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [87/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [87/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [88/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [88/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [88/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [88/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [88/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [88/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [88/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [89/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [89/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [89/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [89/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [89/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [89/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [89/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [90/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [90/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [90/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [90/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [90/100], Step [900/941], Loss: 0.6347\n",
      "Epoch [91/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [200/941], Loss: 0.6349\n",
      "Epoch [91/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [91/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [91/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [91/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [91/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [91/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [92/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [92/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [92/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [92/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [92/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [92/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [92/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [93/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [93/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [93/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [93/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [93/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [93/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [93/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [94/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [94/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [94/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [94/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [94/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [94/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [94/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [95/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [95/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [95/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [95/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [95/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [95/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [95/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [95/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [95/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [96/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [96/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [96/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [96/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [96/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [96/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [96/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [97/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [97/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [97/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [97/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [97/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [97/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [97/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [98/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [98/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [98/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [98/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [98/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [98/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [98/100], Step [900/941], Loss: 0.6379\n",
      "Epoch [99/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [99/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [99/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [99/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [99/100], Step [500/941], Loss: 0.6334\n",
      "Epoch [99/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [99/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [99/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [99/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [100/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [100/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [100/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [100/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [100/100], Step [700/941], Loss: 0.6353\n",
      "Epoch [100/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [100/100], Step [900/941], Loss: 0.6339\n",
      "Test accuracy of the network: 61.45339652448657 %\n",
      "Train accuracy of the network: 95.31623704491098 %\n"
     ]
    }
   ],
   "source": [
    "liar_test_accuracies = []\n",
    "liar_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('liar', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    liar_test_accuracies.append(test_accuracy)\n",
    "    liar_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, liar_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, liar_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/5], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/5], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/5], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/5], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/5], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/5], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/5], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/5], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/5], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/5], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/5], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/5], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/5], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/5], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/5], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/5], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/5], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/5], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/5], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/5], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/5], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/5], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/5], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/5], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/5], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/5], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/5], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/5], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/5], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/5], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/5], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/5], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/5], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/5], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/5], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/5], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/5], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/5], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/5], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/5], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/5], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/5], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/5], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/5], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/5], Step [900/951], Loss: 0.6357\n",
      "Test accuracy of the network: 82.73244781783681 %\n",
      "Train accuracy of the network: 72.35077570339206 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/10], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/10], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/10], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/10], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/10], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/10], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/10], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/10], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/10], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/10], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/10], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/10], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/10], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/10], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/10], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/10], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/10], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/10], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/10], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/10], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/10], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/10], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/10], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/10], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/10], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/10], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/10], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/10], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/10], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/10], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/10], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/10], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/10], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/10], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/10], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/10], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/10], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/10], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/10], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/10], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/10], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/10], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/10], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/10], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/10], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/10], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/10], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/10], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/10], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/10], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/10], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/10], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/10], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/10], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/10], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/10], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/10], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/10], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/10], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/10], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/10], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/10], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/10], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/10], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/10], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/10], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/10], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/10], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/10], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/10], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/10], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/10], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/10], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/10], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/10], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/10], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/10], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/10], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/10], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/10], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/10], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/10], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/10], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/10], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/10], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/10], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/10], Step [900/951], Loss: 0.6337\n",
      "Test accuracy of the network: 78.17836812144212 %\n",
      "Train accuracy of the network: 79.27294241388378 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/20], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/20], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/20], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/20], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/20], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/20], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/20], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/20], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/20], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/20], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/20], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/20], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/20], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/20], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/20], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/20], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/20], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/20], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/20], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/20], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/20], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/20], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/20], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/20], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/20], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/20], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/20], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/20], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/20], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/20], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/20], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/20], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/20], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/20], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/20], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/20], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/20], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/20], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/20], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/20], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/20], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/20], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/20], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/20], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/20], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/20], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/20], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/20], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/20], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/20], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/20], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/20], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/20], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/20], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/20], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/20], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/20], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/20], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/20], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/20], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/20], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/20], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/20], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/20], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/20], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/20], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/20], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/20], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/20], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/20], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/20], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/20], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/20], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/20], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/20], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/20], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/20], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/20], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/20], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/20], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/20], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/20], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/20], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/20], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/20], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/20], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/20], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/20], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/20], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/20], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/20], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/20], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/20], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/20], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/20], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/20], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/20], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/20], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/20], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/20], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/20], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/20], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/20], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/20], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/20], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/20], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/20], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/20], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/20], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/20], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/20], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/20], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/20], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/20], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/20], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/20], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/20], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/20], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/20], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/20], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/20], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/20], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/20], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/20], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/20], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/20], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/20], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/20], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/20], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/20], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/20], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/20], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/20], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/20], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/20], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/20], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/20], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/20], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/20], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/20], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/20], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/20], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/20], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/20], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/20], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/20], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/20], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/20], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/20], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/20], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/20], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/20], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/20], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/20], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/20], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/20], Step [900/951], Loss: 0.6331\n",
      "Test accuracy of the network: 79.60151802656546 %\n",
      "Train accuracy of the network: 88.48935051275309 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/30], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/30], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/30], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/30], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/30], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/30], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/30], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/30], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/30], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/30], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/30], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/30], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/30], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/30], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/30], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/30], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/30], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/30], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/30], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/30], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/30], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/30], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/30], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/30], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/30], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/30], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/30], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/30], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/30], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/30], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/30], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/30], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/30], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/30], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/30], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/30], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/30], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/30], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/30], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/30], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/30], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/30], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/30], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/30], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/30], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/30], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/30], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/30], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/30], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/30], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/30], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/30], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/30], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/30], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/30], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/30], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/30], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/30], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/30], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/30], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/30], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/30], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/30], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/30], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/30], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/30], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/30], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/30], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/30], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/30], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/30], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/30], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/30], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/30], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/30], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/30], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/30], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/30], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/30], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/30], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/30], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/30], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/30], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/30], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/30], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/30], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/30], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/30], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/30], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/30], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/30], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/30], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/30], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/30], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/30], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/30], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/30], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/30], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/30], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/30], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/30], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/30], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/30], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/30], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/30], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/30], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/30], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/30], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/30], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/30], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/30], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/30], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/30], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/30], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/30], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/30], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/30], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/30], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/30], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/30], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/30], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/30], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/30], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/30], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/30], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/30], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/30], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/30], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/30], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/30], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/30], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/30], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/30], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/30], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/30], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/30], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/30], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/30], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/30], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/30], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/30], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/30], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/30], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/30], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/30], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/30], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/30], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/30], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/30], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/30], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/30], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/30], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/30], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/30], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/30], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/30], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/30], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/30], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/30], Step [900/951], Loss: 0.6332\n",
      "Test accuracy of the network: 79.03225806451613 %\n",
      "Train accuracy of the network: 93.09098080462793 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/40], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/40], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/40], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/40], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/40], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/40], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/40], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/40], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/40], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/40], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/40], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/40], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/40], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/40], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/40], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/40], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/40], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/40], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/40], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/40], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/40], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/40], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/40], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/40], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/40], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/40], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/40], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/40], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/40], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/40], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/40], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/40], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/40], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/40], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/40], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/40], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/40], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/40], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/40], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/40], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/40], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/40], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/40], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/40], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/40], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/40], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/40], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/40], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/40], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/40], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/40], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/40], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/40], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/40], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/40], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/40], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/40], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/40], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/40], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/40], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/40], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/40], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/40], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/40], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/40], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/40], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/40], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/40], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/40], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/40], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/40], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/40], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/40], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/40], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/40], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/40], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/40], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/40], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/40], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/40], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/40], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/40], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/40], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/40], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/40], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/40], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/40], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/40], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/40], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/40], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/40], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/40], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/40], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/40], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/40], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/40], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/40], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/40], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/40], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/40], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/40], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/40], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/40], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/40], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/40], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/40], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/40], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/40], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/40], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/40], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/40], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/40], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/40], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/40], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/40], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/40], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/40], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/40], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/40], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/40], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/40], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/40], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/40], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/40], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/40], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/40], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/40], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/40], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/40], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/40], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/40], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/40], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/40], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/40], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/40], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/40], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/40], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/40], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/40], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/40], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/40], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/40], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/40], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/40], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/40], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/40], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/40], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/40], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/40], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/40], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/40], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/40], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/40], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/40], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/40], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/40], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/40], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/40], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/40], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/40], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/40], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/40], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 76.75521821631878 %\n",
      "Train accuracy of the network: 94.60951880094662 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/50], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/50], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/50], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/50], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/50], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/50], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/50], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/50], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/50], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/50], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/50], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/50], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/50], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/50], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/50], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/50], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/50], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/50], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/50], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/50], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/50], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/50], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/50], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/50], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/50], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/50], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/50], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/50], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/50], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/50], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/50], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/50], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/50], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/50], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/50], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/50], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/50], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/50], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/50], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/50], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/50], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/50], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/50], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/50], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/50], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/50], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/50], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/50], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/50], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/50], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/50], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/50], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/50], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/50], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/50], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/50], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/50], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/50], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/50], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/50], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/50], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/50], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/50], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/50], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/50], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/50], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/50], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/50], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/50], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/50], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/50], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/50], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/50], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/50], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/50], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/50], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/50], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/50], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/50], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/50], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/50], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/50], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/50], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/50], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/50], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/50], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/50], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/50], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/50], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/50], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/50], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/50], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/50], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/50], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/50], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/50], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/50], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/50], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/50], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/50], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/50], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/50], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/50], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/50], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/50], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/50], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/50], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/50], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/50], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/50], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/50], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/50], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/50], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/50], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/50], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/50], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/50], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/50], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/50], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/50], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/50], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/50], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/50], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/50], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/50], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/50], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/50], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/50], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/50], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/50], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/50], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/50], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/50], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/50], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/50], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/50], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/50], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/50], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/50], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/50], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/50], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/50], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/50], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/50], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/50], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/50], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/50], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/50], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/50], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/50], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/50], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/50], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/50], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/50], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/50], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/50], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/50], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 75.80645161290323 %\n",
      "Train accuracy of the network: 95.03023928477518 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/60], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/60], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/60], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/60], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/60], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/60], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/60], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/60], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/60], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/60], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/60], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/60], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/60], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/60], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/60], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/60], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/60], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/60], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/60], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/60], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/60], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/60], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/60], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/60], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/60], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/60], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/60], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/60], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/60], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/60], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/60], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/60], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/60], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/60], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/60], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/60], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/60], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/60], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/60], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/60], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/60], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/60], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/60], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/60], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/60], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/60], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/60], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/60], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/60], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/60], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/60], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/60], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/60], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/60], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/60], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/60], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/60], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/60], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/60], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/60], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/60], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/60], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/60], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/60], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/60], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/60], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/60], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/60], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/60], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/60], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/60], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/60], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/60], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/60], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/60], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/60], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/60], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/60], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/60], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/60], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/60], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/60], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/60], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/60], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/60], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/60], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/60], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/60], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/60], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/60], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/60], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/60], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/60], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/60], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/60], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/60], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/60], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/60], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/60], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/60], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/60], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/60], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/60], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/60], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/60], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/60], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/60], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/60], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/60], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/60], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/60], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/60], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/60], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/60], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/60], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/60], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/60], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/60], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/60], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/60], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/60], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/60], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/60], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/60], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/60], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/60], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/60], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/60], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/60], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/60], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/60], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/60], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/60], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/60], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/60], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/60], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/60], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/60], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/60], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/60], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/60], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/60], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/60], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/60], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/60], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/60], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/60], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/60], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/60], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/60], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/60], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/60], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/60], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/60], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/60], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/60], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 74.76280834914611 %\n",
      "Train accuracy of the network: 95.66789376807783 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/75], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/75], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/75], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/75], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/75], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/75], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/75], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/75], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/75], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/75], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/75], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/75], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/75], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/75], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/75], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/75], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/75], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/75], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/75], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/75], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/75], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/75], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/75], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/75], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/75], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/75], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/75], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/75], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/75], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/75], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/75], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/75], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/75], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/75], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/75], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/75], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/75], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/75], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/75], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/75], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/75], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/75], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/75], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/75], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/75], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/75], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/75], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/75], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/75], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/75], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/75], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/75], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/75], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/75], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/75], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/75], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/75], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/75], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/75], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/75], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/75], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/75], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/75], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/75], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/75], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/75], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/75], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/75], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/75], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/75], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/75], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/75], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/75], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/75], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/75], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/75], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/75], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/75], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/75], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/75], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/75], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/75], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/75], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/75], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/75], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/75], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/75], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/75], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/75], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/75], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/75], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/75], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/75], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/75], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/75], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/75], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/75], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/75], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/75], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/75], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/75], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/75], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/75], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/75], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/75], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/75], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/75], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/75], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/75], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/75], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/75], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/75], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/75], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/75], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/75], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/75], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/75], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/75], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/75], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/75], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/75], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/75], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/75], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/75], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/75], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/75], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/75], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/75], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/75], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/75], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/75], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/75], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/75], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/75], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/75], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/75], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/75], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/75], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/75], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/75], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/75], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/75], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/75], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/75], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/75], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/75], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/75], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/75], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/75], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/75], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/75], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/75], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [61/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [61/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [61/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [61/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [61/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [61/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [62/75], Step [200/951], Loss: 0.6338\n",
      "Epoch [62/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [62/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [62/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [62/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [62/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [63/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [63/75], Step [200/951], Loss: 0.6336\n",
      "Epoch [63/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [63/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [63/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [63/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [63/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [63/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [63/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [64/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [64/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [64/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [64/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [64/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [64/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [65/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [65/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [65/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [65/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [65/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [65/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [66/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [66/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [66/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [66/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [66/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [66/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [67/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [67/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [67/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [67/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [67/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [67/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [67/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [67/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [67/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [68/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [68/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [68/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [68/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [68/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [68/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [69/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [69/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [69/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [69/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [69/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [69/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [70/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [70/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [70/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [70/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [70/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [70/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [71/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [71/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [71/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [71/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [71/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [72/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [72/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [72/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [72/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [72/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [72/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [72/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [72/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [72/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [73/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [73/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [73/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [73/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [73/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [73/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [73/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [73/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [73/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [74/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [74/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [74/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [74/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [74/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [74/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [74/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [74/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [74/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [75/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [75/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [75/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [75/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [75/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [75/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [75/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [75/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [75/75], Step [900/951], Loss: 0.6328\n",
      "Test accuracy of the network: 74.573055028463 %\n",
      "Train accuracy of the network: 96.1872206153037 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/100], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/100], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/100], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/100], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/100], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/100], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/100], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/100], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/100], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/100], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/100], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/100], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/100], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/100], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/100], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/100], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/100], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/100], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/100], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/100], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/100], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/100], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/100], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/100], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/100], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/100], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/100], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/100], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/100], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/100], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/100], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/100], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/100], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/100], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/100], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/100], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/100], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/100], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/100], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/100], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/100], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/100], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/100], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/100], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/100], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/100], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/100], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/100], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/100], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/100], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/100], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/100], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/100], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/100], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/100], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/100], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/100], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/100], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/100], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/100], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/100], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/100], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/100], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/100], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/100], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/100], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/100], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/100], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/100], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/100], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/100], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/100], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/100], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/100], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/100], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/100], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/100], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/100], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/100], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/100], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/100], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/100], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/100], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/100], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/100], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/100], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/100], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/100], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/100], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/100], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/100], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/100], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/100], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/100], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/100], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/100], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/100], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/100], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/100], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/100], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/100], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/100], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/100], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/100], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/100], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/100], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/100], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/100], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/100], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/100], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/100], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/100], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/100], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/100], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/100], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/100], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/100], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/100], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/100], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/100], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/100], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/100], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/100], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/100], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/100], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/100], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/100], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/100], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/100], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/100], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/100], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/100], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/100], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/100], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/100], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/100], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/100], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/100], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/100], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/100], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/100], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/100], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/100], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/100], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/100], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/100], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/100], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/100], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/100], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/100], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/100], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/100], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [61/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [61/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [61/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [61/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [61/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [61/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [62/100], Step [200/951], Loss: 0.6338\n",
      "Epoch [62/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [62/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [62/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [62/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [62/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [63/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [63/100], Step [200/951], Loss: 0.6336\n",
      "Epoch [63/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [63/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [63/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [63/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [63/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [63/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [63/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [64/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [64/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [64/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [64/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [64/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [64/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [65/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [65/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [65/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [65/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [65/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [65/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [66/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [66/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [66/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [66/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [66/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [66/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [67/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [67/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [67/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [67/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [67/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [67/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [67/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [67/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [67/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [68/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [68/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [68/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [68/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [68/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [68/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [69/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [69/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [69/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [69/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [69/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [69/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [70/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [70/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [70/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [70/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [70/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [70/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [71/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [71/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [71/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [71/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [71/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [72/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [72/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [72/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [72/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [72/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [72/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [72/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [72/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [72/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [73/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [73/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [73/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [73/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [73/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [73/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [73/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [73/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [73/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [74/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [74/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [74/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [74/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [74/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [74/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [74/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [74/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [74/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [75/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [75/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [75/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [75/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [75/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [75/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [75/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [75/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [75/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [76/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [76/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [76/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [76/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [76/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [76/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [77/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [77/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [77/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [77/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [77/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [77/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [77/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [77/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [77/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [78/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [78/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [78/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [78/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [78/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [78/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [78/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [78/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [78/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [79/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [79/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [79/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [79/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [79/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [79/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [79/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [79/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [79/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [80/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [80/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [80/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [80/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [80/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [80/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [81/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [81/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [81/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [81/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [81/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [81/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [81/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [81/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [81/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [82/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [82/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [82/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [82/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [82/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [82/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [82/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [82/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [82/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [83/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [83/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [83/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [83/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [83/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [83/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [84/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [84/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [84/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [84/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [84/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [84/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [85/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [85/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [85/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [85/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [85/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [85/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [85/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [85/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [85/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [86/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [86/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [86/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [86/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [86/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [86/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [86/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [86/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [86/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [87/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [87/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [87/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [87/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [87/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [87/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [88/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [88/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [88/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [88/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [88/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [88/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [88/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [88/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [88/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [89/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [89/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [89/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [89/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [89/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [89/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [90/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [90/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [90/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [90/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [90/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [90/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [90/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [90/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [90/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [91/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [91/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [91/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [91/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [91/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [91/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [92/100], Step [100/951], Loss: 0.6331\n",
      "Epoch [92/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [92/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [92/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [92/100], Step [500/951], Loss: 0.6389\n",
      "Epoch [92/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [92/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [92/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [92/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [93/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [93/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [93/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [93/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [93/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [93/100], Step [600/951], Loss: 0.6338\n",
      "Epoch [93/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [93/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [93/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [94/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [94/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [94/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [94/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [94/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [94/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [94/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [94/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [94/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [95/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [95/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [95/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [95/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [95/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [95/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [95/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [95/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [95/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [96/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [96/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [96/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [96/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [96/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [96/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [96/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [96/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [96/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [97/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [97/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [97/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [97/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [97/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [97/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [97/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [97/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [97/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [200/951], Loss: 0.6334\n",
      "Epoch [98/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [98/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [98/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [98/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [98/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [98/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [99/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [99/100], Step [200/951], Loss: 0.6336\n",
      "Epoch [99/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [99/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [99/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [99/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [99/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [99/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [99/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [100/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [100/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [100/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [100/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [100/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [100/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [100/100], Step [700/951], Loss: 0.6337\n",
      "Epoch [100/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [100/100], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 68.21631878557875 %\n",
      "Train accuracy of the network: 96.73941625032869 %\n"
     ]
    }
   ],
   "source": [
    "fnn_test_accuracies = []\n",
    "fnn_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('fnn', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    fnn_test_accuracies.append(test_accuracy)\n",
    "    fnn_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, fnn_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, fnn_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
