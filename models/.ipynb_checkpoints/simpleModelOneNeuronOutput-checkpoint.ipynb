{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText\n",
    "# from fnnCovidCombinedPreprocess import getFNNCoronaVocabulary, getFNNCoronaText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.oupt = nn.Linear(hidden_size, 1)  \n",
    "#         self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.hidden1(x))\n",
    "#         out = self.relu(out)\n",
    "#         out = self.oupt(out)\n",
    "        out = torch.sigmoid(self.oupt(out))\n",
    "#         out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    elif dataset == 'combined':\n",
    "        X,Y = getFNNCoronaText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNCoronaVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train)).float()\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    train_loader_batch_size_1 = torch.utils.data.DataLoader(train_data,batch_size=1, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=1, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "#             loss = criterion(y_pred, labels.long())\n",
    "            loss = loss_fn(y_pred, labels.reshape(-1, 1))\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, label in test_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             print('INPUTS', inputs)\n",
    "#             print('LABELS', labels)\n",
    "#             print('OUTPUTS.DATA', outputs.data)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('_', _)\n",
    "#             print('PREDICTED', predicted)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            output = model(inputs)\n",
    "            total += 1\n",
    "            if label >= 0.5 and output >= 0.5:\n",
    "                correct += 1\n",
    "            elif label < 0.5 and output < 0.5:\n",
    "                correct += 1\n",
    "            \n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, label in train_loader_batch_size_1:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            output = model(inputs)\n",
    "            total += 1\n",
    "            if label >= 0.5 and output >= 0.5:\n",
    "                correct += 1\n",
    "            elif label < 0.5 and output < 0.5:\n",
    "                correct += 1\n",
    "                \n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_used = [1, 5, 10, 20, 30, 40, 50, 60, 75, 100]\n",
    "corona_test_accuracies = []\n",
    "corona_train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/1], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/1], Step [15/19], Loss: 0.2169\n",
      "Test accuracy of the network: 86.71248568155785 %\n",
      "Train accuracy of the network: 97.2508591065292 %\n",
      "Epoch [1/5], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/5], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/5], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/5], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/5], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/5], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/5], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/5], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/5], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/5], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/5], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/5], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/5], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/5], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/5], Step [15/19], Loss: 0.0092\n",
      "Test accuracy of the network: 88.77434135166094 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/10], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/10], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/10], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/10], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/10], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/10], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/10], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/10], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/10], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/10], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/10], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/10], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/10], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/10], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/10], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/10], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/10], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/10], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/10], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/10], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/10], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/10], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/10], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/10], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/10], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/10], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/10], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/10], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/10], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/10], Step [15/19], Loss: 0.0018\n",
      "Test accuracy of the network: 89.57617411225658 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/20], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/20], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/20], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/20], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/20], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/20], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/20], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/20], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/20], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/20], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/20], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/20], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/20], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/20], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/20], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/20], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/20], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/20], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/20], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/20], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/20], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/20], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/20], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/20], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/20], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/20], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/20], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/20], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/20], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/20], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/20], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/20], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/20], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/20], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/20], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/20], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/20], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/20], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/20], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/20], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/20], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/20], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/20], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/20], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/20], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/20], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/20], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/20], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/20], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/20], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/20], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/20], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/20], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/20], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/20], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/20], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/20], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/20], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/20], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/20], Step [15/19], Loss: 0.0004\n",
      "Test accuracy of the network: 89.91981672394043 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/30], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/30], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/30], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/30], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/30], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/30], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/30], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/30], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/30], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/30], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/30], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/30], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/30], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/30], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/30], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/30], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/30], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/30], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/30], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/30], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/30], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/30], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/30], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/30], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/30], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/30], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/30], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/30], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/30], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/30], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/30], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/30], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/30], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/30], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/30], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/30], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/30], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/30], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/30], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/30], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/30], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/30], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/30], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/30], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/30], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/30], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/30], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/30], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/30], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/30], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/30], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/30], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/30], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/30], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/30], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/30], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/30], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/30], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/30], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/30], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/30], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/30], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/30], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/30], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/30], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/30], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/30], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/30], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/30], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/30], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/30], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/30], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/30], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/30], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/30], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/30], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/30], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/30], Step [15/19], Loss: 0.0002\n",
      "Test accuracy of the network: 89.69072164948453 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/40], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/40], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/40], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/40], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/40], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/40], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/40], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/40], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/40], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/40], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/40], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/40], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/40], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/40], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/40], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/40], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/40], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/40], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/40], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/40], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/40], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/40], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/40], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/40], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/40], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/40], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/40], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/40], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/40], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/40], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/40], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/40], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/40], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/40], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/40], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/40], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/40], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/40], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/40], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/40], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/40], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/40], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/40], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/40], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/40], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/40], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/40], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/40], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/40], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/40], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/40], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/40], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/40], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/40], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/40], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/40], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/40], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/40], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/40], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/40], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/40], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/40], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/40], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/40], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/40], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/40], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/40], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/40], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/40], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/40], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/40], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/40], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/40], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [31/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [31/40], Step [10/19], Loss: 0.0004\n",
      "Epoch [31/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [32/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [32/40], Step [10/19], Loss: 0.0004\n",
      "Epoch [32/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [33/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [33/40], Step [10/19], Loss: 0.0004\n",
      "Epoch [33/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [34/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [34/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [34/40], Step [15/19], Loss: 0.0002\n",
      "Epoch [35/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [35/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [35/40], Step [15/19], Loss: 0.0001\n",
      "Epoch [36/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [36/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [36/40], Step [15/19], Loss: 0.0001\n",
      "Epoch [37/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [37/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [37/40], Step [15/19], Loss: 0.0001\n",
      "Epoch [38/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [38/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [38/40], Step [15/19], Loss: 0.0001\n",
      "Epoch [39/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [39/40], Step [10/19], Loss: 0.0003\n",
      "Epoch [39/40], Step [15/19], Loss: 0.0001\n",
      "Epoch [40/40], Step [5/19], Loss: 0.0000\n",
      "Epoch [40/40], Step [10/19], Loss: 0.0002\n",
      "Epoch [40/40], Step [15/19], Loss: 0.0001\n",
      "Test accuracy of the network: 90.03436426116839 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/50], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/50], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/50], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/50], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/50], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/50], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/50], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/50], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/50], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/50], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/50], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/50], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/50], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/50], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/50], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/50], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/50], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/50], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/50], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/50], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/50], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/50], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/50], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/50], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/50], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/50], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/50], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/50], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/50], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/50], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/50], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/50], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/50], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/50], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/50], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/50], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/50], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/50], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/50], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/50], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/50], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/50], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/50], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/50], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/50], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/50], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/50], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/50], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/50], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/50], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/50], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/50], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/50], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/50], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/50], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/50], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/50], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/50], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/50], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/50], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/50], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/50], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/50], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/50], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/50], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/50], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/50], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/50], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/50], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/50], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/50], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/50], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/50], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [31/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [31/50], Step [10/19], Loss: 0.0004\n",
      "Epoch [31/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [32/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [32/50], Step [10/19], Loss: 0.0004\n",
      "Epoch [32/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [33/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [33/50], Step [10/19], Loss: 0.0004\n",
      "Epoch [33/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [34/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [34/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [34/50], Step [15/19], Loss: 0.0002\n",
      "Epoch [35/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [35/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [35/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [36/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [36/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [36/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [37/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [37/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [37/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [38/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [38/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [38/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [39/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [39/50], Step [10/19], Loss: 0.0003\n",
      "Epoch [39/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [40/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [40/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [40/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [41/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [41/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [41/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [42/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [42/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [42/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [43/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [43/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [43/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [44/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [44/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [44/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [45/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [45/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [45/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [46/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [46/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [46/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [47/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [47/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [47/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [48/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [48/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [48/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [49/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [49/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [49/50], Step [15/19], Loss: 0.0001\n",
      "Epoch [50/50], Step [5/19], Loss: 0.0000\n",
      "Epoch [50/50], Step [10/19], Loss: 0.0002\n",
      "Epoch [50/50], Step [15/19], Loss: 0.0001\n",
      "Test accuracy of the network: 90.14891179839634 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/60], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/60], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/60], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/60], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/60], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/60], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/60], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/60], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/60], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/60], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/60], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/60], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/60], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/60], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/60], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/60], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/60], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/60], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/60], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/60], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/60], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/60], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/60], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/60], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/60], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/60], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/60], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/60], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/60], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/60], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/60], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/60], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/60], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/60], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/60], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/60], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/60], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/60], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/60], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/60], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/60], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/60], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/60], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/60], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/60], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/60], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/60], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/60], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/60], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/60], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/60], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/60], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/60], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/60], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/60], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/60], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/60], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/60], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/60], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/60], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/60], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/60], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/60], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/60], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/60], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/60], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/60], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/60], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/60], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/60], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/60], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/60], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/60], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [31/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [31/60], Step [10/19], Loss: 0.0004\n",
      "Epoch [31/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [32/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [32/60], Step [10/19], Loss: 0.0004\n",
      "Epoch [32/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [33/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [33/60], Step [10/19], Loss: 0.0004\n",
      "Epoch [33/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [34/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [34/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [34/60], Step [15/19], Loss: 0.0002\n",
      "Epoch [35/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [35/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [35/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [36/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [36/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [36/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [37/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [37/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [37/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [38/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [38/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [38/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [39/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [39/60], Step [10/19], Loss: 0.0003\n",
      "Epoch [39/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [40/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [40/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [40/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [41/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [41/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [41/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [42/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [42/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [42/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [43/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [43/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [43/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [44/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [44/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [44/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [45/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [45/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [45/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [46/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [46/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [46/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [47/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [47/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [47/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [48/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [48/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [48/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [49/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [49/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [49/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [50/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [50/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [50/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [51/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [51/60], Step [10/19], Loss: 0.0002\n",
      "Epoch [51/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [52/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [52/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [52/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [53/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [53/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [53/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [54/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [54/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [54/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [55/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [55/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [55/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [56/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [56/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [56/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [57/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [57/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [57/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [58/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [58/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [58/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [59/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [59/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [59/60], Step [15/19], Loss: 0.0001\n",
      "Epoch [60/60], Step [5/19], Loss: 0.0000\n",
      "Epoch [60/60], Step [10/19], Loss: 0.0001\n",
      "Epoch [60/60], Step [15/19], Loss: 0.0001\n",
      "Test accuracy of the network: 90.03436426116839 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/75], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/75], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/75], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/75], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/75], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/75], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/75], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/75], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/75], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/75], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/75], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/75], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/75], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/75], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/75], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/75], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/75], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/75], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/75], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/75], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/75], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/75], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/75], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/75], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/75], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/75], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/75], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/75], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/75], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/75], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/75], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/75], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/75], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/75], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/75], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/75], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/75], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/75], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/75], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/75], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/75], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/75], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/75], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/75], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/75], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/75], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/75], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/75], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/75], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/75], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/75], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/75], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/75], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/75], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/75], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/75], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/75], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/75], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/75], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/75], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/75], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/75], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/75], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/75], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/75], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/75], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/75], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/75], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/75], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/75], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/75], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/75], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/75], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [31/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [31/75], Step [10/19], Loss: 0.0004\n",
      "Epoch [31/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [32/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [32/75], Step [10/19], Loss: 0.0004\n",
      "Epoch [32/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [33/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [33/75], Step [10/19], Loss: 0.0004\n",
      "Epoch [33/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [34/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [34/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [34/75], Step [15/19], Loss: 0.0002\n",
      "Epoch [35/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [35/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [35/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [36/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [36/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [36/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [37/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [37/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [37/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [38/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [38/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [38/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [39/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [39/75], Step [10/19], Loss: 0.0003\n",
      "Epoch [39/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [40/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [40/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [40/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [41/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [41/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [41/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [42/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [42/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [42/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [43/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [43/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [43/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [44/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [44/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [44/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [45/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [45/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [45/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [46/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [46/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [46/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [47/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [47/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [47/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [48/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [48/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [48/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [49/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [49/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [49/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [50/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [50/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [50/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [51/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [51/75], Step [10/19], Loss: 0.0002\n",
      "Epoch [51/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [52/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [52/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [52/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [53/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [53/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [53/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [54/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [54/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [54/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [55/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [55/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [55/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [56/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [56/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [56/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [57/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [57/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [57/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [58/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [58/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [58/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [59/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [59/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [59/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [60/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [60/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [60/75], Step [15/19], Loss: 0.0001\n",
      "Epoch [61/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [61/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [61/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [62/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [62/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [62/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [63/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [63/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [63/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [64/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [64/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [64/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [65/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [65/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [65/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [66/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [66/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [66/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [67/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [67/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [67/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [68/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [68/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [68/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [69/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [69/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [69/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [70/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [70/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [70/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [71/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [71/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [71/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [72/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [72/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [72/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [73/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [73/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [73/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [74/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [74/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [74/75], Step [15/19], Loss: 0.0000\n",
      "Epoch [75/75], Step [5/19], Loss: 0.0000\n",
      "Epoch [75/75], Step [10/19], Loss: 0.0001\n",
      "Epoch [75/75], Step [15/19], Loss: 0.0000\n",
      "Test accuracy of the network: 89.91981672394043 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/100], Step [5/19], Loss: 0.3543\n",
      "Epoch [1/100], Step [10/19], Loss: 0.5111\n",
      "Epoch [1/100], Step [15/19], Loss: 0.2169\n",
      "Epoch [2/100], Step [5/19], Loss: 0.0395\n",
      "Epoch [2/100], Step [10/19], Loss: 0.1832\n",
      "Epoch [2/100], Step [15/19], Loss: 0.0935\n",
      "Epoch [3/100], Step [5/19], Loss: 0.0094\n",
      "Epoch [3/100], Step [10/19], Loss: 0.0684\n",
      "Epoch [3/100], Step [15/19], Loss: 0.0311\n",
      "Epoch [4/100], Step [5/19], Loss: 0.0038\n",
      "Epoch [4/100], Step [10/19], Loss: 0.0362\n",
      "Epoch [4/100], Step [15/19], Loss: 0.0157\n",
      "Epoch [5/100], Step [5/19], Loss: 0.0021\n",
      "Epoch [5/100], Step [10/19], Loss: 0.0217\n",
      "Epoch [5/100], Step [15/19], Loss: 0.0092\n",
      "Epoch [6/100], Step [5/19], Loss: 0.0013\n",
      "Epoch [6/100], Step [10/19], Loss: 0.0140\n",
      "Epoch [6/100], Step [15/19], Loss: 0.0059\n",
      "Epoch [7/100], Step [5/19], Loss: 0.0009\n",
      "Epoch [7/100], Step [10/19], Loss: 0.0097\n",
      "Epoch [7/100], Step [15/19], Loss: 0.0041\n",
      "Epoch [8/100], Step [5/19], Loss: 0.0007\n",
      "Epoch [8/100], Step [10/19], Loss: 0.0071\n",
      "Epoch [8/100], Step [15/19], Loss: 0.0030\n",
      "Epoch [9/100], Step [5/19], Loss: 0.0005\n",
      "Epoch [9/100], Step [10/19], Loss: 0.0054\n",
      "Epoch [9/100], Step [15/19], Loss: 0.0023\n",
      "Epoch [10/100], Step [5/19], Loss: 0.0004\n",
      "Epoch [10/100], Step [10/19], Loss: 0.0042\n",
      "Epoch [10/100], Step [15/19], Loss: 0.0018\n",
      "Epoch [11/100], Step [5/19], Loss: 0.0003\n",
      "Epoch [11/100], Step [10/19], Loss: 0.0034\n",
      "Epoch [11/100], Step [15/19], Loss: 0.0015\n",
      "Epoch [12/100], Step [5/19], Loss: 0.0003\n",
      "Epoch [12/100], Step [10/19], Loss: 0.0028\n",
      "Epoch [12/100], Step [15/19], Loss: 0.0012\n",
      "Epoch [13/100], Step [5/19], Loss: 0.0002\n",
      "Epoch [13/100], Step [10/19], Loss: 0.0024\n",
      "Epoch [13/100], Step [15/19], Loss: 0.0010\n",
      "Epoch [14/100], Step [5/19], Loss: 0.0002\n",
      "Epoch [14/100], Step [10/19], Loss: 0.0020\n",
      "Epoch [14/100], Step [15/19], Loss: 0.0009\n",
      "Epoch [15/100], Step [5/19], Loss: 0.0002\n",
      "Epoch [15/100], Step [10/19], Loss: 0.0017\n",
      "Epoch [15/100], Step [15/19], Loss: 0.0008\n",
      "Epoch [16/100], Step [5/19], Loss: 0.0002\n",
      "Epoch [16/100], Step [10/19], Loss: 0.0015\n",
      "Epoch [16/100], Step [15/19], Loss: 0.0007\n",
      "Epoch [17/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [17/100], Step [10/19], Loss: 0.0013\n",
      "Epoch [17/100], Step [15/19], Loss: 0.0006\n",
      "Epoch [18/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [18/100], Step [10/19], Loss: 0.0012\n",
      "Epoch [18/100], Step [15/19], Loss: 0.0005\n",
      "Epoch [19/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [19/100], Step [10/19], Loss: 0.0011\n",
      "Epoch [19/100], Step [15/19], Loss: 0.0005\n",
      "Epoch [20/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [20/100], Step [10/19], Loss: 0.0010\n",
      "Epoch [20/100], Step [15/19], Loss: 0.0004\n",
      "Epoch [21/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [21/100], Step [10/19], Loss: 0.0009\n",
      "Epoch [21/100], Step [15/19], Loss: 0.0004\n",
      "Epoch [22/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [22/100], Step [10/19], Loss: 0.0008\n",
      "Epoch [22/100], Step [15/19], Loss: 0.0004\n",
      "Epoch [23/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [23/100], Step [10/19], Loss: 0.0007\n",
      "Epoch [23/100], Step [15/19], Loss: 0.0003\n",
      "Epoch [24/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [24/100], Step [10/19], Loss: 0.0007\n",
      "Epoch [24/100], Step [15/19], Loss: 0.0003\n",
      "Epoch [25/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [25/100], Step [10/19], Loss: 0.0006\n",
      "Epoch [25/100], Step [15/19], Loss: 0.0003\n",
      "Epoch [26/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [26/100], Step [10/19], Loss: 0.0006\n",
      "Epoch [26/100], Step [15/19], Loss: 0.0003\n",
      "Epoch [27/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [27/100], Step [10/19], Loss: 0.0005\n",
      "Epoch [27/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [28/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [28/100], Step [10/19], Loss: 0.0005\n",
      "Epoch [28/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [29/100], Step [5/19], Loss: 0.0001\n",
      "Epoch [29/100], Step [10/19], Loss: 0.0005\n",
      "Epoch [29/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [30/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [30/100], Step [10/19], Loss: 0.0004\n",
      "Epoch [30/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [31/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [31/100], Step [10/19], Loss: 0.0004\n",
      "Epoch [31/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [32/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [32/100], Step [10/19], Loss: 0.0004\n",
      "Epoch [32/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [33/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [33/100], Step [10/19], Loss: 0.0004\n",
      "Epoch [33/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [34/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [34/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [34/100], Step [15/19], Loss: 0.0002\n",
      "Epoch [35/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [35/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [35/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [36/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [36/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [36/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [37/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [37/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [37/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [38/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [38/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [38/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [39/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [39/100], Step [10/19], Loss: 0.0003\n",
      "Epoch [39/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [40/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [40/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [40/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [41/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [41/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [41/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [42/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [42/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [42/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [43/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [43/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [43/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [44/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [44/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [44/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [45/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [45/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [45/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [46/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [46/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [46/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [47/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [47/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [47/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [48/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [48/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [48/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [49/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [49/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [49/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [50/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [50/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [50/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [51/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [51/100], Step [10/19], Loss: 0.0002\n",
      "Epoch [51/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [52/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [52/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [52/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [53/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [53/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [53/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [54/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [54/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [54/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [55/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [55/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [55/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [56/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [56/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [56/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [57/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [57/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [57/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [58/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [58/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [58/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [59/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [59/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [59/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [60/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [60/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [60/100], Step [15/19], Loss: 0.0001\n",
      "Epoch [61/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [61/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [61/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [62/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [62/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [62/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [63/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [63/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [63/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [64/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [64/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [64/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [65/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [65/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [65/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [66/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [66/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [66/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [67/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [67/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [67/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [68/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [68/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [68/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [69/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [69/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [69/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [70/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [70/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [70/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [71/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [71/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [71/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [72/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [72/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [72/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [73/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [73/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [73/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [74/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [74/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [74/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [75/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [75/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [75/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [76/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [76/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [76/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [77/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [77/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [77/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [78/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [78/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [78/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [79/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [79/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [79/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [80/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [80/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [80/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [81/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [81/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [81/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [82/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [82/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [82/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [83/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [83/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [83/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [84/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [84/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [84/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [85/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [85/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [85/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [86/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [86/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [86/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [87/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [87/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [87/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [88/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [88/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [88/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [89/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [89/100], Step [10/19], Loss: 0.0001\n",
      "Epoch [89/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [90/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [90/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [90/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [91/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [91/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [91/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [92/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [92/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [92/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [93/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [93/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [93/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [94/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [94/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [94/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [95/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [95/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [95/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [96/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [96/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [96/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [97/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [97/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [97/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [98/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [98/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [98/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [99/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [99/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [99/100], Step [15/19], Loss: 0.0000\n",
      "Epoch [100/100], Step [5/19], Loss: 0.0000\n",
      "Epoch [100/100], Step [10/19], Loss: 0.0000\n",
      "Epoch [100/100], Step [15/19], Loss: 0.0000\n",
      "Test accuracy of the network: 89.69072164948453 %\n",
      "Train accuracy of the network: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=num_epoch)\n",
    "    corona_test_accuracies.append(test_accuracy)\n",
    "    corona_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, corona_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, corona_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/1], Step [500/941], Loss: 0.4891\n",
      "Test accuracy of the network: 70.06319115323855 %\n",
      "Train accuracy of the network: 81.27159181504119 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/5], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/5], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/5], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/5], Step [500/941], Loss: 0.0551\n",
      "Test accuracy of the network: 67.9304897314376 %\n",
      "Train accuracy of the network: 94.7315971299495 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/10], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/10], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/10], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/10], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/10], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/10], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/10], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/10], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/10], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/10], Step [500/941], Loss: 0.0031\n",
      "Test accuracy of the network: 70.22116903633491 %\n",
      "Train accuracy of the network: 98.61812383736381 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/20], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/20], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/20], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/20], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/20], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/20], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/20], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/20], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/20], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/20], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/20], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/20], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/20], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/20], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/20], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/20], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/20], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/20], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/20], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/20], Step [500/941], Loss: 0.0001\n",
      "Test accuracy of the network: 68.64139020537125 %\n",
      "Train accuracy of the network: 99.95349455221897 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/30], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/30], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/30], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/30], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/30], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/30], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/30], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/30], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/30], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/30], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/30], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/30], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/30], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/30], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/30], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/30], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/30], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/30], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/30], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/30], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/30], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/30], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 69.74723538704582 %\n",
      "Train accuracy of the network: 99.96013818761627 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/40], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/40], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/40], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/40], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/40], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/40], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/40], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/40], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/40], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/40], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/40], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/40], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/40], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/40], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/40], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/40], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/40], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/40], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/40], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/40], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [31/40], Step [500/941], Loss: 0.0095\n",
      "Epoch [32/40], Step [500/941], Loss: 0.0250\n",
      "Epoch [33/40], Step [500/941], Loss: 0.0004\n",
      "Epoch [34/40], Step [500/941], Loss: 0.0003\n",
      "Epoch [35/40], Step [500/941], Loss: 0.0002\n",
      "Epoch [36/40], Step [500/941], Loss: 0.0001\n",
      "Epoch [37/40], Step [500/941], Loss: 0.0001\n",
      "Epoch [38/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [39/40], Step [500/941], Loss: 0.0000\n",
      "Epoch [40/40], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 69.27330173775671 %\n",
      "Train accuracy of the network: 99.96678182301355 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/50], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/50], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/50], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/50], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/50], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/50], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/50], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/50], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/50], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/50], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/50], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/50], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/50], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/50], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/50], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/50], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/50], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/50], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/50], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/50], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [31/50], Step [500/941], Loss: 0.0095\n",
      "Epoch [32/50], Step [500/941], Loss: 0.0250\n",
      "Epoch [33/50], Step [500/941], Loss: 0.0004\n",
      "Epoch [34/50], Step [500/941], Loss: 0.0003\n",
      "Epoch [35/50], Step [500/941], Loss: 0.0002\n",
      "Epoch [36/50], Step [500/941], Loss: 0.0001\n",
      "Epoch [37/50], Step [500/941], Loss: 0.0001\n",
      "Epoch [38/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [39/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [40/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [41/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [42/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [43/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [44/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [45/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [46/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [47/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [48/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [49/50], Step [500/941], Loss: 0.0000\n",
      "Epoch [50/50], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 69.3522906793049 %\n",
      "Train accuracy of the network: 99.98006909380813 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/60], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/60], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/60], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/60], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/60], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/60], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/60], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/60], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/60], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/60], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/60], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/60], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/60], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/60], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/60], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/60], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/60], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/60], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [31/60], Step [500/941], Loss: 0.0095\n",
      "Epoch [32/60], Step [500/941], Loss: 0.0250\n",
      "Epoch [33/60], Step [500/941], Loss: 0.0004\n",
      "Epoch [34/60], Step [500/941], Loss: 0.0003\n",
      "Epoch [35/60], Step [500/941], Loss: 0.0002\n",
      "Epoch [36/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [37/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [38/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [39/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [40/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [41/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [42/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [43/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [44/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [45/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [46/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [47/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [48/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [49/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [50/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [51/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [52/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [53/60], Step [500/941], Loss: 0.0193\n",
      "Epoch [54/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [55/60], Step [500/941], Loss: 0.0001\n",
      "Epoch [56/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [57/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [58/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [59/60], Step [500/941], Loss: 0.0000\n",
      "Epoch [60/60], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 68.79936808846762 %\n",
      "Train accuracy of the network: 99.98006909380813 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/75], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/75], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/75], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/75], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/75], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/75], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/75], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/75], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/75], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/75], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/75], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/75], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/75], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/75], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/75], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/75], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/75], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/75], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [31/75], Step [500/941], Loss: 0.0095\n",
      "Epoch [32/75], Step [500/941], Loss: 0.0250\n",
      "Epoch [33/75], Step [500/941], Loss: 0.0004\n",
      "Epoch [34/75], Step [500/941], Loss: 0.0003\n",
      "Epoch [35/75], Step [500/941], Loss: 0.0002\n",
      "Epoch [36/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [37/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [38/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [39/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [40/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [41/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [42/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [43/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [44/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [45/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [46/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [47/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [48/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [49/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [50/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [51/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [52/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [53/75], Step [500/941], Loss: 0.0193\n",
      "Epoch [54/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [55/75], Step [500/941], Loss: 0.0001\n",
      "Epoch [56/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [57/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [58/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [59/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [60/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [61/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [62/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [63/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [64/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [65/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [66/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [67/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [68/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [69/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [70/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [71/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [72/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [73/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [74/75], Step [500/941], Loss: 0.0000\n",
      "Epoch [75/75], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 69.66824644549763 %\n",
      "Train accuracy of the network: 99.98671272920542 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/100], Step [500/941], Loss: 0.4891\n",
      "Epoch [2/100], Step [500/941], Loss: 0.4241\n",
      "Epoch [3/100], Step [500/941], Loss: 0.3268\n",
      "Epoch [4/100], Step [500/941], Loss: 0.1347\n",
      "Epoch [5/100], Step [500/941], Loss: 0.0551\n",
      "Epoch [6/100], Step [500/941], Loss: 0.0201\n",
      "Epoch [7/100], Step [500/941], Loss: 0.0199\n",
      "Epoch [8/100], Step [500/941], Loss: 0.0088\n",
      "Epoch [9/100], Step [500/941], Loss: 0.0036\n",
      "Epoch [10/100], Step [500/941], Loss: 0.0031\n",
      "Epoch [11/100], Step [500/941], Loss: 0.0043\n",
      "Epoch [12/100], Step [500/941], Loss: 0.0037\n",
      "Epoch [13/100], Step [500/941], Loss: 0.0086\n",
      "Epoch [14/100], Step [500/941], Loss: 0.0017\n",
      "Epoch [15/100], Step [500/941], Loss: 0.0020\n",
      "Epoch [16/100], Step [500/941], Loss: 0.0028\n",
      "Epoch [17/100], Step [500/941], Loss: 0.0019\n",
      "Epoch [18/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [19/100], Step [500/941], Loss: 0.0002\n",
      "Epoch [20/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [21/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [22/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [23/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [24/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [25/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [26/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [27/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [28/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [29/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [30/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [31/100], Step [500/941], Loss: 0.0095\n",
      "Epoch [32/100], Step [500/941], Loss: 0.0250\n",
      "Epoch [33/100], Step [500/941], Loss: 0.0004\n",
      "Epoch [34/100], Step [500/941], Loss: 0.0003\n",
      "Epoch [35/100], Step [500/941], Loss: 0.0002\n",
      "Epoch [36/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [37/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [38/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [39/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [40/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [41/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [42/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [43/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [44/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [45/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [46/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [47/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [48/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [49/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [50/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [51/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [52/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [53/100], Step [500/941], Loss: 0.0193\n",
      "Epoch [54/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [55/100], Step [500/941], Loss: 0.0001\n",
      "Epoch [56/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [57/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [58/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [59/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [60/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [61/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [62/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [63/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [64/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [65/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [66/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [67/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [68/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [69/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [70/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [71/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [72/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [73/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [74/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [75/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [76/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [77/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [78/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [79/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [80/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [81/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [82/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [83/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [84/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [85/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [86/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [87/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [88/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [89/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [90/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [91/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [92/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [93/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [94/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [95/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [96/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [97/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [98/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [99/100], Step [500/941], Loss: 0.0000\n",
      "Epoch [100/100], Step [500/941], Loss: 0.0000\n",
      "Test accuracy of the network: 69.66824644549763 %\n",
      "Train accuracy of the network: 99.98006909380813 %\n"
     ]
    }
   ],
   "source": [
    "liar_test_accuracies = []\n",
    "liar_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('liar', num_epochs=num_epoch, print_epoch_mod=500)\n",
    "    liar_test_accuracies.append(test_accuracy)\n",
    "    liar_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, liar_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, liar_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/1], Step [500/951], Loss: 0.6513\n",
      "Test accuracy of the network: 80.74003795066413 %\n",
      "Train accuracy of the network: 77.62950302392848 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/5], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/5], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/5], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/5], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/5], Step [500/951], Loss: 0.0589\n",
      "Test accuracy of the network: 79.88614800759014 %\n",
      "Train accuracy of the network: 92.93978438075204 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/10], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/10], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/10], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/10], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/10], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/10], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/10], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/10], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/10], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/10], Step [500/951], Loss: 0.0077\n",
      "Test accuracy of the network: 78.17836812144212 %\n",
      "Train accuracy of the network: 98.27767551932685 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/20], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/20], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/20], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/20], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/20], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/20], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/20], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/20], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/20], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/20], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/20], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/20], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/20], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/20], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/20], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/20], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/20], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/20], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/20], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/20], Step [500/951], Loss: 0.0004\n",
      "Test accuracy of the network: 83.01707779886148 %\n",
      "Train accuracy of the network: 99.9671312122009 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/30], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/30], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/30], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/30], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/30], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/30], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/30], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/30], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/30], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/30], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/30], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/30], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/30], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/30], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/30], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/30], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/30], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/30], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/30], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/30], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/30], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/30], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/30], Step [500/951], Loss: 0.0000\n",
      "Test accuracy of the network: 80.92979127134726 %\n",
      "Train accuracy of the network: 99.99342624244018 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/40], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/40], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/40], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/40], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/40], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/40], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/40], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/40], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/40], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/40], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/40], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/40], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/40], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/40], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/40], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/40], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/40], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/40], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/40], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/40], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/40], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [31/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [32/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [33/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [34/40], Step [500/951], Loss: 0.0059\n",
      "Epoch [35/40], Step [500/951], Loss: 0.0002\n",
      "Epoch [36/40], Step [500/951], Loss: 0.0002\n",
      "Epoch [37/40], Step [500/951], Loss: 0.0001\n",
      "Epoch [38/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [39/40], Step [500/951], Loss: 0.0000\n",
      "Epoch [40/40], Step [500/951], Loss: 0.0000\n",
      "Test accuracy of the network: 80.92979127134726 %\n",
      "Train accuracy of the network: 99.99342624244018 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/50], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/50], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/50], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/50], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/50], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/50], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/50], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/50], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/50], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/50], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/50], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/50], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/50], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/50], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/50], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/50], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/50], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/50], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/50], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/50], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/50], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [31/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [32/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [33/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [34/50], Step [500/951], Loss: 0.0059\n",
      "Epoch [35/50], Step [500/951], Loss: 0.0002\n",
      "Epoch [36/50], Step [500/951], Loss: 0.0002\n",
      "Epoch [37/50], Step [500/951], Loss: 0.0001\n",
      "Epoch [38/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [39/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [40/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [41/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [42/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [43/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [44/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [45/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [46/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [47/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [48/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [49/50], Step [500/951], Loss: 0.0000\n",
      "Epoch [50/50], Step [500/951], Loss: 0.0000\n",
      "Test accuracy of the network: 81.49905123339659 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/60], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/60], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/60], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/60], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/60], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/60], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/60], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/60], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/60], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/60], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/60], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/60], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/60], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/60], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/60], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/60], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/60], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/60], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/60], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/60], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/60], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [31/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [32/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [33/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [34/60], Step [500/951], Loss: 0.0059\n",
      "Epoch [35/60], Step [500/951], Loss: 0.0002\n",
      "Epoch [36/60], Step [500/951], Loss: 0.0002\n",
      "Epoch [37/60], Step [500/951], Loss: 0.0001\n",
      "Epoch [38/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [39/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [40/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [41/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [42/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [43/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [44/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [45/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [46/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [47/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [48/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [49/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [50/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [51/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [52/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [53/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [54/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [55/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [56/60], Step [500/951], Loss: 0.0000\n",
      "Epoch [57/60], Step [500/951], Loss: 0.0025\n",
      "Epoch [58/60], Step [500/951], Loss: 0.0012\n",
      "Epoch [59/60], Step [500/951], Loss: 0.0001\n",
      "Epoch [60/60], Step [500/951], Loss: 0.0001\n",
      "Test accuracy of the network: 79.12713472485768 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/75], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/75], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/75], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/75], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/75], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/75], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/75], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/75], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/75], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/75], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/75], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/75], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/75], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/75], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/75], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/75], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/75], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/75], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/75], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/75], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/75], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [31/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [32/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [33/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [34/75], Step [500/951], Loss: 0.0059\n",
      "Epoch [35/75], Step [500/951], Loss: 0.0002\n",
      "Epoch [36/75], Step [500/951], Loss: 0.0002\n",
      "Epoch [37/75], Step [500/951], Loss: 0.0001\n",
      "Epoch [38/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [39/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [40/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [41/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [42/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [43/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [44/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [45/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [46/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [47/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [48/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [49/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [50/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [51/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [52/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [53/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [54/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [55/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [56/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [57/75], Step [500/951], Loss: 0.0025\n",
      "Epoch [58/75], Step [500/951], Loss: 0.0012\n",
      "Epoch [59/75], Step [500/951], Loss: 0.0001\n",
      "Epoch [60/75], Step [500/951], Loss: 0.0001\n",
      "Epoch [61/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [62/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [63/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [64/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [65/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [66/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [67/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [68/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [69/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [70/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [71/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [72/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [73/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [74/75], Step [500/951], Loss: 0.0000\n",
      "Epoch [75/75], Step [500/951], Loss: 0.0000\n",
      "Test accuracy of the network: 79.41176470588235 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/100], Step [500/951], Loss: 0.6513\n",
      "Epoch [2/100], Step [500/951], Loss: 0.5977\n",
      "Epoch [3/100], Step [500/951], Loss: 0.3836\n",
      "Epoch [4/100], Step [500/951], Loss: 0.2220\n",
      "Epoch [5/100], Step [500/951], Loss: 0.0589\n",
      "Epoch [6/100], Step [500/951], Loss: 0.0261\n",
      "Epoch [7/100], Step [500/951], Loss: 0.0139\n",
      "Epoch [8/100], Step [500/951], Loss: 0.0210\n",
      "Epoch [9/100], Step [500/951], Loss: 0.0070\n",
      "Epoch [10/100], Step [500/951], Loss: 0.0077\n",
      "Epoch [11/100], Step [500/951], Loss: 0.0067\n",
      "Epoch [12/100], Step [500/951], Loss: 0.0013\n",
      "Epoch [13/100], Step [500/951], Loss: 0.0026\n",
      "Epoch [14/100], Step [500/951], Loss: 0.0028\n",
      "Epoch [15/100], Step [500/951], Loss: 0.0065\n",
      "Epoch [16/100], Step [500/951], Loss: 0.0011\n",
      "Epoch [17/100], Step [500/951], Loss: 0.0004\n",
      "Epoch [18/100], Step [500/951], Loss: 0.0003\n",
      "Epoch [19/100], Step [500/951], Loss: 0.0001\n",
      "Epoch [20/100], Step [500/951], Loss: 0.0004\n",
      "Epoch [21/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [22/100], Step [500/951], Loss: 0.0001\n",
      "Epoch [23/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [24/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [25/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [26/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [27/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [28/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [29/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [30/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [31/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [32/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [33/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [34/100], Step [500/951], Loss: 0.0059\n",
      "Epoch [35/100], Step [500/951], Loss: 0.0002\n",
      "Epoch [36/100], Step [500/951], Loss: 0.0002\n",
      "Epoch [37/100], Step [500/951], Loss: 0.0001\n",
      "Epoch [38/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [39/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [40/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [41/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [42/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [43/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [44/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [45/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [46/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [47/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [48/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [49/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [50/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [51/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [52/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [53/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [54/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [55/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [56/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [57/100], Step [500/951], Loss: 0.0025\n",
      "Epoch [58/100], Step [500/951], Loss: 0.0012\n",
      "Epoch [59/100], Step [500/951], Loss: 0.0001\n",
      "Epoch [60/100], Step [500/951], Loss: 0.0001\n",
      "Epoch [61/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [62/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [63/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [64/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [65/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [66/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [67/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [68/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [69/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [70/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [71/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [72/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [73/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [74/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [75/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [76/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [77/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [78/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [79/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [80/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [81/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [82/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [83/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [84/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [85/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [86/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [87/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [88/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [89/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [90/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [91/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [92/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [93/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [94/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [95/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [96/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [97/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [98/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [99/100], Step [500/951], Loss: 0.0000\n",
      "Epoch [100/100], Step [500/951], Loss: 0.0000\n",
      "Test accuracy of the network: 77.41935483870968 %\n",
      "Train accuracy of the network: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "fnn_test_accuracies = []\n",
    "fnn_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('fnn', num_epochs=num_epoch, print_epoch_mod=500)\n",
    "    liar_test_accuracies.append(test_accuracy)\n",
    "    liar_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, fnn_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, fnn_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHiddenLayerNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TwoHiddenLayerNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()   \n",
    "        self.oupt = nn.Linear(hidden_size, 1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.hidden1(x))\n",
    "#         out = self.relu(out)\n",
    "        out = torch.tanh(self.hidden2(out))\n",
    "#         out = self.relu(out)\n",
    "        out = torch.sigmoid(self.oupt(out))\n",
    "#         out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestTwoHiddenLayerModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    elif dataset == 'combined':\n",
    "        X,Y = getFNNCoronaText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNCoronaVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train)).float()\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    train_loader_batch_size_1 = torch.utils.data.DataLoader(train_data,batch_size=1, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=1, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = TwoHiddenLayerNeuralNet(vocabsize, 200).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "#             loss = criterion(y_pred, labels.long())\n",
    "            loss = loss_fn(y_pred, labels.reshape(-1, 1))\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, label in test_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             print('INPUTS', inputs)\n",
    "#             print('LABELS', labels)\n",
    "#             print('OUTPUTS.DATA', outputs.data)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('_', _)\n",
    "#             print('PREDICTED', predicted)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            output = model(inputs)\n",
    "            total += 1\n",
    "            if label >= 0.5 and output >= 0.5:\n",
    "                correct += 1\n",
    "            elif label < 0.5 and output < 0.5:\n",
    "                correct += 1\n",
    "            \n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, label in train_loader_batch_size_1:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            output = model(inputs)\n",
    "            total += 1\n",
    "            if label >= 0.5 and output >= 0.5:\n",
    "                correct += 1\n",
    "            elif label < 0.5 and output < 0.5:\n",
    "                correct += 1\n",
    "                \n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
