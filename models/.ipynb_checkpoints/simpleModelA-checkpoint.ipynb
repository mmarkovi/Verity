{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/5], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6444\n",
      "Test accuracy of the network: 65.75028636884306 %\n",
      "Train accuracy of the network: 76.97594501718213 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=5)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/10], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/10], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/10], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/10], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/10], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/10], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/10], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/10], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/10], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/10], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/10], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/10], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/10], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/10], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/10], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/10], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/10], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/10], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/10], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/10], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/10], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/10], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/10], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/10], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/10], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/10], Step [15/19], Loss: 0.6455\n",
      "Test accuracy of the network: 67.46849942726232 %\n",
      "Train accuracy of the network: 74.5704467353952 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=10)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/20], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/20], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/20], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/20], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/20], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/20], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/20], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/20], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/20], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/20], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/20], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/20], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/20], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/20], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/20], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/20], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/20], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/20], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/20], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/20], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/20], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/20], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/20], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/20], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/20], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/20], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/20], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/20], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/20], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/20], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/20], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/20], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/20], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/20], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/20], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/20], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/20], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/20], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/20], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/20], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/20], Step [15/19], Loss: 0.6403\n",
      "Test accuracy of the network: 78.35051546391753 %\n",
      "Train accuracy of the network: 88.65979381443299 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=20)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/30], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/30], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/30], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/30], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/30], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/30], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/30], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/30], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/30], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/30], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/30], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/30], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/30], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/30], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/30], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/30], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/30], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/30], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/30], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/30], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/30], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/30], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/30], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/30], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/30], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/30], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/30], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/30], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/30], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/30], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/30], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/30], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/30], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/30], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/30], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/30], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/30], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/30], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/30], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/30], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/30], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/30], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/30], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/30], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/30], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/30], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/30], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/30], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/30], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/30], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/30], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/30], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/30], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/30], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/30], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/30], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/30], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/30], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/30], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/30], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/30], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/30], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/30], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/30], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/30], Step [15/19], Loss: 0.6351\n",
      "Test accuracy of the network: 86.5979381443299 %\n",
      "Train accuracy of the network: 98.62542955326461 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=30)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6330\n",
      "Test accuracy of the network: 88.31615120274914 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=40)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=50)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=60)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=75)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=100)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_used = [5, 10, 20, 30, 40, 50, 60, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/5], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/5], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/5], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/5], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/5], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/5], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/5], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/5], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/5], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/5], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/5], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/5], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/5], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/5], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/5], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/5], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/5], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/5], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/5], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/5], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/5], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/5], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/5], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/5], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/5], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/5], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/5], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/5], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/5], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/5], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/5], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/5], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/5], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/5], Step [900/941], Loss: 0.6396\n",
      "Test accuracy of the network: 68.48341232227489 %\n",
      "Train accuracy of the network: 77.77039596066967 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/10], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/10], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/10], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/10], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/10], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/10], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/10], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/10], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/10], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/10], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/10], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/10], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/10], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/10], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/10], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/10], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/10], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/10], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/10], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/10], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/10], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/10], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/10], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/10], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/10], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/10], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/10], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/10], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/10], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/10], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/10], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/10], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/10], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/10], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/10], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/10], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/10], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/10], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/10], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/10], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/10], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/10], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/10], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/10], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/10], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/10], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [900/941], Loss: 0.6391\n",
      "Test accuracy of the network: 68.00947867298578 %\n",
      "Train accuracy of the network: 78.22880680308265 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/20], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/20], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/20], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/20], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/20], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/20], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/20], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/20], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/20], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/20], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/20], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/20], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/20], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/20], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/20], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/20], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/20], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/20], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/20], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/20], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/20], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/20], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/20], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/20], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/20], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/20], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/20], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/20], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/20], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/20], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/20], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/20], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/20], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/20], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/20], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/20], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/20], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/20], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/20], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/20], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/20], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/20], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/20], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/20], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/20], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/20], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/20], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/20], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/20], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/20], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/20], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/20], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/20], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/20], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/20], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/20], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/20], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/20], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/20], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/20], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/20], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/20], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/20], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/20], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/20], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/20], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/20], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/20], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/20], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/20], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/20], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [900/941], Loss: 0.6399\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 83.23810789263885 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/30], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/30], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/30], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/30], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/30], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/30], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/30], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/30], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/30], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/30], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/30], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/30], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/30], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/30], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/30], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/30], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/30], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/30], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/30], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/30], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/30], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/30], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/30], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/30], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/30], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/30], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/30], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/30], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/30], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/30], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/30], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/30], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/30], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/30], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/30], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/30], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/30], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/30], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/30], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/30], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/30], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/30], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/30], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/30], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/30], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/30], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/30], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/30], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/30], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/30], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/30], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/30], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/30], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/30], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/30], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/30], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/30], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/30], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/30], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/30], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/30], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/30], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/30], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/30], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/30], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/30], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/30], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/30], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/30], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/30], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/30], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/30], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/30], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/30], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/30], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/30], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/30], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/30], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/30], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/30], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/30], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/30], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/30], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/30], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/30], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/30], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/30], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/30], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/30], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/30], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/30], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/30], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/30], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/30], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/30], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/30], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/30], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/30], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/30], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/30], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/30], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/30], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/30], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/30], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/30], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/30], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/30], Step [900/941], Loss: 0.6356\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 90.10762689343609 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/40], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/40], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/40], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/40], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/40], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/40], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/40], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/40], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/40], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/40], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/40], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/40], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/40], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/40], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/40], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/40], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/40], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/40], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/40], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/40], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/40], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/40], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/40], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/40], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/40], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/40], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/40], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/40], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/40], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/40], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/40], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/40], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/40], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/40], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/40], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/40], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/40], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/40], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/40], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/40], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/40], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/40], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/40], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/40], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/40], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/40], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/40], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/40], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/40], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/40], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/40], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/40], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/40], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/40], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/40], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/40], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/40], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/40], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/40], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/40], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/40], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/40], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/40], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/40], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/40], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/40], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/40], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/40], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/40], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/40], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/40], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/40], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/40], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/40], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/40], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/40], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/40], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/40], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/40], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/40], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/40], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/40], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/40], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/40], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/40], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/40], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/40], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/40], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/40], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/40], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/40], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/40], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/40], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/40], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/40], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/40], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/40], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/40], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/40], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/40], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/40], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/40], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/40], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/40], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/40], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/40], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/40], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/40], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/40], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/40], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/40], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/40], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/40], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/40], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/40], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/40], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/40], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/40], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/40], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/40], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/40], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/40], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/40], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/40], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/40], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 67.53554502369668 %\n",
      "Train accuracy of the network: 92.33988838692532 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/50], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/50], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/50], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/50], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/50], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/50], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/50], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/50], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/50], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/50], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/50], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/50], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/50], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/50], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/50], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/50], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/50], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/50], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/50], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/50], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/50], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/50], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/50], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/50], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/50], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/50], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/50], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/50], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/50], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/50], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/50], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/50], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/50], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/50], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/50], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/50], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/50], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/50], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/50], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/50], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/50], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/50], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/50], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/50], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/50], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/50], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/50], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/50], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/50], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/50], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/50], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/50], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/50], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/50], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/50], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/50], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/50], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/50], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/50], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/50], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/50], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/50], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/50], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/50], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/50], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/50], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/50], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/50], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/50], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/50], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/50], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/50], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/50], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/50], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/50], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/50], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/50], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/50], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/50], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/50], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/50], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/50], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/50], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/50], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/50], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/50], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/50], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/50], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/50], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/50], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/50], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/50], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/50], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/50], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/50], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/50], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/50], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/50], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/50], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/50], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/50], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/50], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/50], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/50], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/50], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/50], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/50], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/50], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/50], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/50], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/50], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/50], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/50], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/50], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/50], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/50], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/50], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 66.66666666666667 %\n",
      "Train accuracy of the network: 93.58224820621844 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/60], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/60], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/60], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/60], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/60], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/60], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/60], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/60], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/60], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/60], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/60], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/60], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/60], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/60], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/60], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/60], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/60], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/60], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/60], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/60], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/60], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/60], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/60], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/60], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/60], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/60], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/60], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/60], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/60], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/60], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/60], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/60], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/60], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/60], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/60], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/60], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/60], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/60], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/60], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/60], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/60], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/60], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/60], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/60], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/60], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/60], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/60], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/60], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/60], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/60], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/60], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/60], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/60], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/60], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/60], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/60], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/60], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/60], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/60], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/60], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/60], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/60], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/60], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/60], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/60], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/60], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/60], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/60], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/60], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/60], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/60], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/60], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/60], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/60], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/60], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/60], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/60], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/60], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/60], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/60], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/60], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/60], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/60], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/60], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/60], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/60], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/60], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/60], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/60], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/60], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/60], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/60], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/60], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/60], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/60], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/60], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/60], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/60], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/60], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/60], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/60], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/60], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/60], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/60], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/60], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/60], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/60], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/60], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/60], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/60], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/60], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/60], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/60], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/60], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/60], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/60], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/60], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 66.5086887835703 %\n",
      "Train accuracy of the network: 94.41270263087962 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/75], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/75], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/75], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/75], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/75], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/75], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/75], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/75], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/75], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/75], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/75], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/75], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/75], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/75], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/75], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/75], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/75], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/75], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/75], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/75], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/75], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/75], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/75], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/75], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/75], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/75], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/75], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/75], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/75], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/75], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/75], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/75], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/75], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/75], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/75], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/75], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/75], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/75], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/75], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/75], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/75], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/75], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/75], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/75], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/75], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/75], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/75], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/75], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/75], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/75], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/75], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/75], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/75], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/75], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/75], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/75], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/75], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/75], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/75], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/75], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/75], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/75], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/75], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/75], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/75], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/75], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/75], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/75], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/75], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/75], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/75], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/75], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/75], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/75], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/75], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/75], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/75], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/75], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/75], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/75], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/75], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/75], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/75], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/75], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/75], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/75], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/75], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/75], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/75], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/75], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/75], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/75], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/75], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/75], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/75], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/75], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/75], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/75], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/75], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/75], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/75], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/75], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/75], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/75], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/75], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/75], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/75], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/75], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/75], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/75], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/75], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/75], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/75], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/75], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/75], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/75], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/75], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/75], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/75], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 64.77093206951027 %\n",
      "Train accuracy of the network: 94.93090619186819 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/100], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/100], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/100], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/100], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/100], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/100], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/100], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/100], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/100], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/100], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/100], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/100], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/100], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/100], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/100], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/100], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/100], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/100], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/100], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/100], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/100], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/100], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/100], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/100], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/100], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/100], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/100], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/100], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/100], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/100], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/100], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/100], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/100], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/100], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/100], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/100], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/100], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/100], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/100], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/100], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/100], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/100], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/100], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/100], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/100], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/100], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/100], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/100], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/100], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/100], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/100], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/100], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/100], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/100], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/100], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/100], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/100], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/100], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/100], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/100], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/100], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/100], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/100], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/100], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/100], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/100], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/100], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/100], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/100], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/100], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/100], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/100], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/100], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/100], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/100], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/100], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/100], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/100], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/100], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/100], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/100], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/100], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/100], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/100], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/100], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/100], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/100], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/100], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/100], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/100], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/100], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/100], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/100], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/100], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/100], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/100], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/100], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/100], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/100], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/100], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/100], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/100], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/100], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/100], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/100], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/100], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/100], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/100], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/100], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/100], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/100], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/100], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/100], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/100], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/100], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/100], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [76/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [76/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [76/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [76/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [76/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [76/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [77/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [77/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [77/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [77/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [77/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [77/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [77/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [78/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [78/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [78/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [78/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [78/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [78/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [78/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [200/941], Loss: 0.6343\n",
      "Epoch [79/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [79/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [79/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [79/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [79/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [79/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [80/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [80/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [80/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [80/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [80/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [80/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [81/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [81/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [81/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [81/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [81/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [81/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [82/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [82/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [82/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [82/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [82/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [82/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [83/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [83/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [83/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [83/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [83/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [83/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [84/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [84/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [84/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [84/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [84/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [84/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [84/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [85/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [85/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [85/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [85/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [85/100], Step [900/941], Loss: 0.6348\n",
      "Epoch [86/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [86/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [86/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [86/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [86/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [86/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [86/100], Step [900/941], Loss: 0.6349\n",
      "Epoch [87/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [87/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [87/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [87/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [87/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [87/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [87/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [88/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [88/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [88/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [88/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [88/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [88/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [88/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [89/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [89/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [89/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [89/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [89/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [89/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [89/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [90/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [90/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [90/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [90/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [90/100], Step [900/941], Loss: 0.6347\n",
      "Epoch [91/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [200/941], Loss: 0.6349\n",
      "Epoch [91/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [91/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [91/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [91/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [91/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [91/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [92/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [92/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [92/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [92/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [92/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [92/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [92/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [93/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [93/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [93/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [93/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [93/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [93/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [93/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [94/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [94/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [94/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [94/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [94/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [94/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [94/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [95/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [95/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [95/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [95/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [95/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [95/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [95/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [95/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [95/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [96/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [96/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [96/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [96/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [96/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [96/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [96/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [97/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [97/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [97/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [97/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [97/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [97/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [97/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [98/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [98/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [98/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [98/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [98/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [98/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [98/100], Step [900/941], Loss: 0.6379\n",
      "Epoch [99/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [99/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [99/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [99/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [99/100], Step [500/941], Loss: 0.6334\n",
      "Epoch [99/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [99/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [99/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [99/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [100/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [100/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [100/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [100/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [100/100], Step [700/941], Loss: 0.6353\n",
      "Epoch [100/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [100/100], Step [900/941], Loss: 0.6339\n",
      "Test accuracy of the network: 61.45339652448657 %\n",
      "Train accuracy of the network: 95.31623704491098 %\n"
     ]
    }
   ],
   "source": [
    "liar_test_accuracies = []\n",
    "liar_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('liar', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    liar_test_accuracies.append(test_accuracy)\n",
    "    liar_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, liar_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, liar_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/5], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/5], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/5], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/5], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/5], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/5], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/5], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/5], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/5], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/5], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/5], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/5], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/5], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/5], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/5], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/5], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/5], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/5], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/5], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/5], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/5], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/5], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/5], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/5], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/5], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/5], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/5], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/5], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/5], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/5], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/5], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/5], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/5], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/5], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/5], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/5], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/5], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/5], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/5], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/5], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/5], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/5], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/5], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/5], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/5], Step [900/951], Loss: 0.6357\n",
      "Test accuracy of the network: 82.73244781783681 %\n",
      "Train accuracy of the network: 72.35077570339206 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/10], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/10], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/10], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/10], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/10], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/10], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/10], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/10], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/10], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/10], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/10], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/10], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/10], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/10], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/10], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/10], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/10], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/10], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/10], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/10], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/10], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/10], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/10], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/10], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/10], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/10], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/10], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/10], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/10], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/10], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/10], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/10], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/10], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/10], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/10], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/10], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/10], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/10], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/10], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/10], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/10], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/10], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/10], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/10], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/10], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/10], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/10], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/10], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/10], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/10], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/10], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/10], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/10], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/10], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/10], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/10], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/10], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/10], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/10], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/10], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/10], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/10], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/10], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/10], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/10], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/10], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/10], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/10], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/10], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/10], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/10], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/10], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/10], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/10], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/10], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/10], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/10], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/10], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/10], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/10], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/10], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/10], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/10], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/10], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/10], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/10], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/10], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/10], Step [900/951], Loss: 0.6337\n",
      "Test accuracy of the network: 78.17836812144212 %\n",
      "Train accuracy of the network: 79.27294241388378 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/20], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/20], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/20], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/20], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/20], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/20], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/20], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/20], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/20], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/20], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/20], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/20], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/20], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/20], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/20], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/20], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/20], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/20], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/20], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/20], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/20], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/20], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/20], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/20], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/20], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/20], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/20], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/20], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/20], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/20], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/20], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/20], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/20], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/20], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/20], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/20], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/20], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/20], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/20], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/20], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/20], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/20], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/20], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/20], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/20], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/20], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/20], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/20], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/20], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/20], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/20], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/20], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/20], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/20], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/20], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/20], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/20], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/20], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/20], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/20], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/20], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/20], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/20], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/20], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/20], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/20], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/20], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/20], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/20], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/20], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/20], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/20], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/20], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/20], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/20], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/20], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/20], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/20], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/20], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/20], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/20], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/20], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/20], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/20], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/20], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/20], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/20], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/20], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/20], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/20], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/20], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/20], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/20], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/20], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/20], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/20], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/20], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/20], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/20], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/20], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/20], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/20], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/20], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/20], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/20], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/20], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/20], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/20], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/20], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/20], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/20], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/20], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/20], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/20], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/20], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/20], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/20], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/20], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/20], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/20], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/20], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/20], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/20], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/20], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/20], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/20], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/20], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/20], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/20], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/20], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/20], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/20], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/20], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/20], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/20], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/20], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/20], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/20], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/20], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/20], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/20], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/20], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/20], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/20], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/20], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/20], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/20], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/20], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/20], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/20], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/20], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/20], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/20], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/20], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/20], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/20], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/20], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/20], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/20], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/20], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/20], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/20], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/20], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/20], Step [900/951], Loss: 0.6331\n",
      "Test accuracy of the network: 79.60151802656546 %\n",
      "Train accuracy of the network: 88.48935051275309 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/30], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/30], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/30], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/30], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/30], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/30], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/30], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/30], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/30], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/30], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/30], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/30], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/30], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/30], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/30], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/30], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/30], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/30], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/30], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/30], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/30], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/30], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/30], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/30], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/30], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/30], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/30], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/30], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/30], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/30], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/30], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/30], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/30], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/30], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/30], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/30], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/30], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/30], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/30], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/30], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/30], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/30], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/30], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/30], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/30], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/30], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/30], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/30], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/30], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/30], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/30], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/30], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/30], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/30], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/30], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/30], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/30], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/30], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/30], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/30], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/30], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/30], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/30], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/30], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/30], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/30], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/30], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/30], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/30], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/30], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/30], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/30], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/30], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/30], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/30], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/30], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/30], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/30], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/30], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/30], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/30], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/30], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/30], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/30], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/30], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/30], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/30], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/30], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/30], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/30], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/30], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/30], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/30], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/30], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/30], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/30], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/30], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/30], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/30], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/30], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/30], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/30], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/30], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/30], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/30], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/30], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/30], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/30], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/30], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/30], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/30], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/30], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/30], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/30], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/30], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/30], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/30], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/30], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/30], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/30], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/30], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/30], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/30], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/30], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/30], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/30], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/30], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/30], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/30], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/30], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/30], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/30], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/30], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/30], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/30], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/30], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/30], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/30], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/30], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/30], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/30], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/30], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/30], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/30], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/30], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/30], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/30], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/30], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/30], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/30], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/30], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/30], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/30], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/30], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/30], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/30], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/30], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/30], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/30], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/30], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/30], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/30], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/30], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/30], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/30], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/30], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/30], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/30], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/30], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/30], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/30], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/30], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/30], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/30], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/30], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/30], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/30], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/30], Step [900/951], Loss: 0.6332\n",
      "Test accuracy of the network: 79.03225806451613 %\n",
      "Train accuracy of the network: 93.09098080462793 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/40], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/40], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/40], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/40], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/40], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/40], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/40], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/40], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/40], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/40], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/40], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/40], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/40], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/40], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/40], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/40], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/40], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/40], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/40], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/40], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/40], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/40], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/40], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/40], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/40], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/40], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/40], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/40], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/40], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/40], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/40], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/40], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/40], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/40], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/40], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/40], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/40], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/40], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/40], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/40], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/40], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/40], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/40], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/40], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/40], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/40], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/40], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/40], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/40], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/40], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/40], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/40], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/40], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/40], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/40], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/40], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/40], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/40], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/40], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/40], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/40], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/40], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/40], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/40], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/40], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/40], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/40], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/40], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/40], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/40], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/40], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/40], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/40], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/40], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/40], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/40], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/40], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/40], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/40], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/40], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/40], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/40], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/40], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/40], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/40], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/40], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/40], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/40], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/40], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/40], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/40], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/40], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/40], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/40], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/40], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/40], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/40], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/40], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/40], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/40], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/40], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/40], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/40], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/40], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/40], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/40], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/40], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/40], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/40], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/40], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/40], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/40], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/40], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/40], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/40], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/40], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/40], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/40], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/40], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/40], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/40], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/40], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/40], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/40], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/40], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/40], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/40], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/40], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/40], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/40], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/40], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/40], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/40], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/40], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/40], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/40], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/40], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/40], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/40], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/40], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/40], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/40], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/40], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/40], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/40], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/40], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/40], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/40], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/40], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/40], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/40], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/40], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/40], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/40], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/40], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/40], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/40], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/40], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/40], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/40], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/40], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/40], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/40], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/40], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/40], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/40], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/40], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/40], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/40], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/40], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/40], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/40], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/40], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/40], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/40], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/40], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/40], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/40], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/40], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/40], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/40], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/40], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/40], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/40], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/40], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/40], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/40], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/40], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/40], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/40], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 76.75521821631878 %\n",
      "Train accuracy of the network: 94.60951880094662 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/50], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/50], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/50], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/50], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/50], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/50], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/50], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/50], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/50], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/50], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/50], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/50], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/50], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/50], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/50], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/50], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/50], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/50], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/50], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/50], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/50], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/50], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/50], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/50], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/50], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/50], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/50], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/50], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/50], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/50], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/50], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/50], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/50], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/50], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/50], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/50], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/50], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/50], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/50], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/50], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/50], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/50], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/50], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/50], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/50], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/50], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/50], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/50], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/50], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/50], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/50], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/50], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/50], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/50], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/50], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/50], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/50], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/50], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/50], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/50], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/50], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/50], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/50], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/50], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/50], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/50], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/50], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/50], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/50], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/50], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/50], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/50], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/50], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/50], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/50], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/50], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/50], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/50], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/50], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/50], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/50], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/50], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/50], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/50], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/50], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/50], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/50], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/50], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/50], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/50], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/50], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/50], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/50], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/50], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/50], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/50], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/50], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/50], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/50], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/50], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/50], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/50], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/50], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/50], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/50], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/50], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/50], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/50], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/50], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/50], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/50], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/50], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/50], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/50], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/50], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/50], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/50], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/50], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/50], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/50], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/50], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/50], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/50], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/50], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/50], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/50], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/50], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/50], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/50], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/50], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/50], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/50], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/50], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/50], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/50], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/50], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/50], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/50], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/50], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/50], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/50], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/50], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/50], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/50], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/50], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/50], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/50], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/50], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/50], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/50], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/50], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/50], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/50], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/50], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/50], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/50], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/50], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/50], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/50], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/50], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/50], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/50], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/50], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/50], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/50], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/50], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/50], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/50], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/50], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/50], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/50], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/50], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/50], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/50], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/50], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/50], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/50], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/50], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/50], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/50], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/50], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/50], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/50], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/50], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/50], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/50], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/50], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/50], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/50], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/50], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 75.80645161290323 %\n",
      "Train accuracy of the network: 95.03023928477518 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/60], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/60], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/60], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/60], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/60], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/60], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/60], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/60], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/60], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/60], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/60], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/60], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/60], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/60], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/60], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/60], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/60], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/60], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/60], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/60], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/60], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/60], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/60], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/60], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/60], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/60], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/60], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/60], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/60], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/60], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/60], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/60], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/60], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/60], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/60], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/60], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/60], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/60], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/60], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/60], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/60], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/60], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/60], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/60], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/60], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/60], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/60], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/60], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/60], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/60], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/60], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/60], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/60], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/60], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/60], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/60], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/60], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/60], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/60], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/60], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/60], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/60], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/60], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/60], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/60], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/60], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/60], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/60], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/60], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/60], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/60], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/60], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/60], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/60], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/60], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/60], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/60], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/60], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/60], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/60], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/60], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/60], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/60], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/60], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/60], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/60], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/60], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/60], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/60], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/60], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/60], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/60], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/60], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/60], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/60], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/60], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/60], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/60], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/60], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/60], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/60], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/60], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/60], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/60], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/60], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/60], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/60], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/60], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/60], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/60], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/60], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/60], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/60], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/60], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/60], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/60], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/60], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/60], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/60], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/60], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/60], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/60], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/60], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/60], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/60], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/60], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/60], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/60], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/60], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/60], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/60], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/60], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/60], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/60], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/60], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/60], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/60], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/60], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/60], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/60], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/60], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/60], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/60], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/60], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/60], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/60], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/60], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/60], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/60], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/60], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/60], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/60], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/60], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/60], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/60], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/60], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/60], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/60], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/60], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/60], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/60], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/60], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/60], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/60], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/60], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/60], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/60], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/60], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/60], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/60], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/60], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/60], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/60], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/60], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/60], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/60], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/60], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/60], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/60], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/60], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/60], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/60], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/60], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/60], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/60], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/60], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/60], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/60], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/60], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/60], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 74.76280834914611 %\n",
      "Train accuracy of the network: 95.66789376807783 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/75], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/75], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/75], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/75], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/75], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/75], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/75], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/75], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/75], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/75], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/75], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/75], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/75], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/75], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/75], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/75], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/75], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/75], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/75], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/75], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/75], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/75], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/75], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/75], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/75], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/75], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/75], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/75], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/75], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/75], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/75], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/75], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/75], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/75], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/75], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/75], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/75], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/75], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/75], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/75], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/75], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/75], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/75], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/75], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/75], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/75], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/75], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/75], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/75], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/75], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/75], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/75], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/75], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/75], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/75], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/75], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/75], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/75], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/75], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/75], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/75], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/75], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/75], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/75], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/75], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/75], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/75], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/75], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/75], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/75], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/75], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/75], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/75], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/75], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/75], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/75], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/75], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/75], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/75], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/75], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/75], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/75], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/75], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/75], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/75], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/75], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/75], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/75], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/75], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/75], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/75], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/75], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/75], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/75], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/75], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/75], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/75], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/75], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/75], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/75], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/75], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/75], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/75], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/75], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/75], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/75], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/75], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/75], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/75], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/75], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/75], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/75], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/75], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/75], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/75], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/75], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/75], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/75], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/75], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/75], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/75], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/75], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/75], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/75], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/75], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/75], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/75], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/75], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/75], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/75], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/75], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/75], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/75], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/75], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/75], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/75], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/75], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/75], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/75], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/75], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/75], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/75], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/75], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/75], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/75], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/75], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/75], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/75], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/75], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/75], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/75], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/75], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/75], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/75], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/75], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/75], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/75], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/75], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/75], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/75], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/75], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/75], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/75], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/75], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/75], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/75], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/75], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/75], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/75], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/75], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/75], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/75], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/75], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/75], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/75], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [61/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [61/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [61/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [61/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [61/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [61/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [61/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [62/75], Step [200/951], Loss: 0.6338\n",
      "Epoch [62/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [62/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [62/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [62/75], Step [700/951], Loss: 0.6333\n",
      "Epoch [62/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [62/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [63/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [63/75], Step [200/951], Loss: 0.6336\n",
      "Epoch [63/75], Step [300/951], Loss: 0.6332\n",
      "Epoch [63/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [63/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [63/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [63/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [63/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [63/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [64/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [64/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [64/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [64/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [64/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [64/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [64/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [65/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [65/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [65/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [65/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [65/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [65/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [65/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [66/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [66/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [66/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [66/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [66/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [66/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [66/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [67/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [67/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [67/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [67/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [67/75], Step [500/951], Loss: 0.6330\n",
      "Epoch [67/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [67/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [67/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [67/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [68/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [68/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [68/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [68/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [68/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [68/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [68/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [69/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [69/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [69/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [69/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [69/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [69/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [69/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [70/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [70/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [400/951], Loss: 0.6327\n",
      "Epoch [70/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [70/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [70/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [70/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [70/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [71/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [71/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [400/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [500/951], Loss: 0.6329\n",
      "Epoch [71/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [71/75], Step [700/951], Loss: 0.6331\n",
      "Epoch [71/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [71/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [72/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [72/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [72/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [72/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [72/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [72/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [72/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [72/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [72/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [73/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [73/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [73/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [73/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [73/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [73/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [73/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [73/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [73/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [74/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [74/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [74/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [74/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [74/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [74/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [74/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [74/75], Step [800/951], Loss: 0.6326\n",
      "Epoch [74/75], Step [900/951], Loss: 0.6329\n",
      "Epoch [75/75], Step [100/951], Loss: 0.6328\n",
      "Epoch [75/75], Step [200/951], Loss: 0.6333\n",
      "Epoch [75/75], Step [300/951], Loss: 0.6329\n",
      "Epoch [75/75], Step [400/951], Loss: 0.6326\n",
      "Epoch [75/75], Step [500/951], Loss: 0.6328\n",
      "Epoch [75/75], Step [600/951], Loss: 0.6333\n",
      "Epoch [75/75], Step [700/951], Loss: 0.6330\n",
      "Epoch [75/75], Step [800/951], Loss: 0.6327\n",
      "Epoch [75/75], Step [900/951], Loss: 0.6328\n",
      "Test accuracy of the network: 74.573055028463 %\n",
      "Train accuracy of the network: 96.1872206153037 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/100], Step [100/951], Loss: 0.6441\n",
      "Epoch [1/100], Step [200/951], Loss: 0.7013\n",
      "Epoch [1/100], Step [300/951], Loss: 0.6404\n",
      "Epoch [1/100], Step [400/951], Loss: 0.6629\n",
      "Epoch [1/100], Step [500/951], Loss: 0.6409\n",
      "Epoch [1/100], Step [600/951], Loss: 0.7022\n",
      "Epoch [1/100], Step [700/951], Loss: 0.7062\n",
      "Epoch [1/100], Step [800/951], Loss: 0.6384\n",
      "Epoch [1/100], Step [900/951], Loss: 0.6465\n",
      "Epoch [2/100], Step [100/951], Loss: 0.6731\n",
      "Epoch [2/100], Step [200/951], Loss: 0.6813\n",
      "Epoch [2/100], Step [300/951], Loss: 0.6395\n",
      "Epoch [2/100], Step [400/951], Loss: 0.6453\n",
      "Epoch [2/100], Step [500/951], Loss: 0.6659\n",
      "Epoch [2/100], Step [600/951], Loss: 0.7005\n",
      "Epoch [2/100], Step [700/951], Loss: 0.6431\n",
      "Epoch [2/100], Step [800/951], Loss: 0.6393\n",
      "Epoch [2/100], Step [900/951], Loss: 0.6400\n",
      "Epoch [3/100], Step [100/951], Loss: 0.6426\n",
      "Epoch [3/100], Step [200/951], Loss: 0.6468\n",
      "Epoch [3/100], Step [300/951], Loss: 0.6413\n",
      "Epoch [3/100], Step [400/951], Loss: 0.6450\n",
      "Epoch [3/100], Step [500/951], Loss: 0.6451\n",
      "Epoch [3/100], Step [600/951], Loss: 0.6859\n",
      "Epoch [3/100], Step [700/951], Loss: 0.6414\n",
      "Epoch [3/100], Step [800/951], Loss: 0.6349\n",
      "Epoch [3/100], Step [900/951], Loss: 0.6361\n",
      "Epoch [4/100], Step [100/951], Loss: 0.6424\n",
      "Epoch [4/100], Step [200/951], Loss: 0.6422\n",
      "Epoch [4/100], Step [300/951], Loss: 0.6381\n",
      "Epoch [4/100], Step [400/951], Loss: 0.6420\n",
      "Epoch [4/100], Step [500/951], Loss: 0.6419\n",
      "Epoch [4/100], Step [600/951], Loss: 0.6404\n",
      "Epoch [4/100], Step [700/951], Loss: 0.6421\n",
      "Epoch [4/100], Step [800/951], Loss: 0.6375\n",
      "Epoch [4/100], Step [900/951], Loss: 0.6371\n",
      "Epoch [5/100], Step [100/951], Loss: 0.6381\n",
      "Epoch [5/100], Step [200/951], Loss: 0.6407\n",
      "Epoch [5/100], Step [300/951], Loss: 0.6364\n",
      "Epoch [5/100], Step [400/951], Loss: 0.6397\n",
      "Epoch [5/100], Step [500/951], Loss: 0.6415\n",
      "Epoch [5/100], Step [600/951], Loss: 0.6423\n",
      "Epoch [5/100], Step [700/951], Loss: 0.6421\n",
      "Epoch [5/100], Step [800/951], Loss: 0.6349\n",
      "Epoch [5/100], Step [900/951], Loss: 0.6357\n",
      "Epoch [6/100], Step [100/951], Loss: 0.6395\n",
      "Epoch [6/100], Step [200/951], Loss: 0.6414\n",
      "Epoch [6/100], Step [300/951], Loss: 0.6367\n",
      "Epoch [6/100], Step [400/951], Loss: 0.6388\n",
      "Epoch [6/100], Step [500/951], Loss: 0.6404\n",
      "Epoch [6/100], Step [600/951], Loss: 0.6409\n",
      "Epoch [6/100], Step [700/951], Loss: 0.6425\n",
      "Epoch [6/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [6/100], Step [900/951], Loss: 0.6351\n",
      "Epoch [7/100], Step [100/951], Loss: 0.6368\n",
      "Epoch [7/100], Step [200/951], Loss: 0.6402\n",
      "Epoch [7/100], Step [300/951], Loss: 0.6355\n",
      "Epoch [7/100], Step [400/951], Loss: 0.6371\n",
      "Epoch [7/100], Step [500/951], Loss: 0.6408\n",
      "Epoch [7/100], Step [600/951], Loss: 0.6391\n",
      "Epoch [7/100], Step [700/951], Loss: 0.6505\n",
      "Epoch [7/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [7/100], Step [900/951], Loss: 0.6353\n",
      "Epoch [8/100], Step [100/951], Loss: 0.6361\n",
      "Epoch [8/100], Step [200/951], Loss: 0.6408\n",
      "Epoch [8/100], Step [300/951], Loss: 0.6353\n",
      "Epoch [8/100], Step [400/951], Loss: 0.6366\n",
      "Epoch [8/100], Step [500/951], Loss: 0.6406\n",
      "Epoch [8/100], Step [600/951], Loss: 0.6375\n",
      "Epoch [8/100], Step [700/951], Loss: 0.6381\n",
      "Epoch [8/100], Step [800/951], Loss: 0.6338\n",
      "Epoch [8/100], Step [900/951], Loss: 0.6343\n",
      "Epoch [9/100], Step [100/951], Loss: 0.6350\n",
      "Epoch [9/100], Step [200/951], Loss: 0.6404\n",
      "Epoch [9/100], Step [300/951], Loss: 0.6350\n",
      "Epoch [9/100], Step [400/951], Loss: 0.6354\n",
      "Epoch [9/100], Step [500/951], Loss: 0.6372\n",
      "Epoch [9/100], Step [600/951], Loss: 0.6359\n",
      "Epoch [9/100], Step [700/951], Loss: 0.6373\n",
      "Epoch [9/100], Step [800/951], Loss: 0.6335\n",
      "Epoch [9/100], Step [900/951], Loss: 0.6338\n",
      "Epoch [10/100], Step [100/951], Loss: 0.6348\n",
      "Epoch [10/100], Step [200/951], Loss: 0.6413\n",
      "Epoch [10/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [10/100], Step [400/951], Loss: 0.6354\n",
      "Epoch [10/100], Step [500/951], Loss: 0.6364\n",
      "Epoch [10/100], Step [600/951], Loss: 0.6354\n",
      "Epoch [10/100], Step [700/951], Loss: 0.6368\n",
      "Epoch [10/100], Step [800/951], Loss: 0.6334\n",
      "Epoch [10/100], Step [900/951], Loss: 0.6337\n",
      "Epoch [11/100], Step [100/951], Loss: 0.6347\n",
      "Epoch [11/100], Step [200/951], Loss: 0.6397\n",
      "Epoch [11/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [11/100], Step [400/951], Loss: 0.6344\n",
      "Epoch [11/100], Step [500/951], Loss: 0.6361\n",
      "Epoch [11/100], Step [600/951], Loss: 0.6351\n",
      "Epoch [11/100], Step [700/951], Loss: 0.6355\n",
      "Epoch [11/100], Step [800/951], Loss: 0.6332\n",
      "Epoch [11/100], Step [900/951], Loss: 0.6347\n",
      "Epoch [12/100], Step [100/951], Loss: 0.6345\n",
      "Epoch [12/100], Step [200/951], Loss: 0.6399\n",
      "Epoch [12/100], Step [300/951], Loss: 0.6342\n",
      "Epoch [12/100], Step [400/951], Loss: 0.6344\n",
      "Epoch [12/100], Step [500/951], Loss: 0.6362\n",
      "Epoch [12/100], Step [600/951], Loss: 0.6347\n",
      "Epoch [12/100], Step [700/951], Loss: 0.6355\n",
      "Epoch [12/100], Step [800/951], Loss: 0.6331\n",
      "Epoch [12/100], Step [900/951], Loss: 0.6334\n",
      "Epoch [13/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [13/100], Step [200/951], Loss: 0.6399\n",
      "Epoch [13/100], Step [300/951], Loss: 0.6341\n",
      "Epoch [13/100], Step [400/951], Loss: 0.6340\n",
      "Epoch [13/100], Step [500/951], Loss: 0.6361\n",
      "Epoch [13/100], Step [600/951], Loss: 0.6343\n",
      "Epoch [13/100], Step [700/951], Loss: 0.6347\n",
      "Epoch [13/100], Step [800/951], Loss: 0.6331\n",
      "Epoch [13/100], Step [900/951], Loss: 0.6334\n",
      "Epoch [14/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [14/100], Step [200/951], Loss: 0.6395\n",
      "Epoch [14/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [14/100], Step [400/951], Loss: 0.6337\n",
      "Epoch [14/100], Step [500/951], Loss: 0.6353\n",
      "Epoch [14/100], Step [600/951], Loss: 0.6344\n",
      "Epoch [14/100], Step [700/951], Loss: 0.6344\n",
      "Epoch [14/100], Step [800/951], Loss: 0.6330\n",
      "Epoch [14/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [15/100], Step [100/951], Loss: 0.6336\n",
      "Epoch [15/100], Step [200/951], Loss: 0.6392\n",
      "Epoch [15/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [15/100], Step [400/951], Loss: 0.6334\n",
      "Epoch [15/100], Step [500/951], Loss: 0.6344\n",
      "Epoch [15/100], Step [600/951], Loss: 0.6342\n",
      "Epoch [15/100], Step [700/951], Loss: 0.6343\n",
      "Epoch [15/100], Step [800/951], Loss: 0.6329\n",
      "Epoch [15/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [16/100], Step [100/951], Loss: 0.6336\n",
      "Epoch [16/100], Step [200/951], Loss: 0.6391\n",
      "Epoch [16/100], Step [300/951], Loss: 0.6339\n",
      "Epoch [16/100], Step [400/951], Loss: 0.6336\n",
      "Epoch [16/100], Step [500/951], Loss: 0.6346\n",
      "Epoch [16/100], Step [600/951], Loss: 0.6342\n",
      "Epoch [16/100], Step [700/951], Loss: 0.6342\n",
      "Epoch [16/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [16/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [17/100], Step [100/951], Loss: 0.6337\n",
      "Epoch [17/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [17/100], Step [300/951], Loss: 0.6334\n",
      "Epoch [17/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [17/100], Step [500/951], Loss: 0.6338\n",
      "Epoch [17/100], Step [600/951], Loss: 0.6337\n",
      "Epoch [17/100], Step [700/951], Loss: 0.6338\n",
      "Epoch [17/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [17/100], Step [900/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [100/951], Loss: 0.6334\n",
      "Epoch [18/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [18/100], Step [300/951], Loss: 0.6335\n",
      "Epoch [18/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [500/951], Loss: 0.6333\n",
      "Epoch [18/100], Step [600/951], Loss: 0.6338\n",
      "Epoch [18/100], Step [700/951], Loss: 0.6338\n",
      "Epoch [18/100], Step [800/951], Loss: 0.6328\n",
      "Epoch [18/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [19/100], Step [100/951], Loss: 0.6333\n",
      "Epoch [19/100], Step [200/951], Loss: 0.6390\n",
      "Epoch [19/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [19/100], Step [400/951], Loss: 0.6333\n",
      "Epoch [19/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [19/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [19/100], Step [700/951], Loss: 0.6334\n",
      "Epoch [19/100], Step [800/951], Loss: 0.6330\n",
      "Epoch [19/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [20/100], Step [100/951], Loss: 0.6332\n",
      "Epoch [20/100], Step [200/951], Loss: 0.6377\n",
      "Epoch [20/100], Step [300/951], Loss: 0.6333\n",
      "Epoch [20/100], Step [400/951], Loss: 0.6332\n",
      "Epoch [20/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [20/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [20/100], Step [700/951], Loss: 0.6336\n",
      "Epoch [20/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [20/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [21/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [21/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [21/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [400/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [21/100], Step [600/951], Loss: 0.6337\n",
      "Epoch [21/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [21/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [21/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [22/100], Step [200/951], Loss: 0.6370\n",
      "Epoch [22/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [22/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [22/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [22/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [22/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [22/100], Step [900/951], Loss: 0.6333\n",
      "Epoch [23/100], Step [100/951], Loss: 0.6332\n",
      "Epoch [23/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [23/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [400/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [23/100], Step [600/951], Loss: 0.6339\n",
      "Epoch [23/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [23/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [23/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [24/100], Step [200/951], Loss: 0.6355\n",
      "Epoch [24/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [24/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [24/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [24/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [24/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [24/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [25/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [25/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [25/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [25/100], Step [600/951], Loss: 0.6336\n",
      "Epoch [25/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [25/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [25/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [26/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [26/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [400/951], Loss: 0.6330\n",
      "Epoch [26/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [26/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [26/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [26/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [26/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [100/951], Loss: 0.6333\n",
      "Epoch [27/100], Step [200/951], Loss: 0.6357\n",
      "Epoch [27/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [27/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [27/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [27/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [27/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [27/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [28/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [28/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [28/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [28/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [28/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [28/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [29/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [29/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [400/951], Loss: 0.6328\n",
      "Epoch [29/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [29/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [29/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [29/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [30/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [30/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [30/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [30/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [30/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [30/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [30/100], Step [900/951], Loss: 0.6332\n",
      "Epoch [31/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [31/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [31/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [31/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [31/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [31/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [31/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [31/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [31/100], Step [900/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [32/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [32/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [32/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [32/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [32/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [32/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [32/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [33/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [33/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [33/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [33/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [33/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [33/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [33/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [34/100], Step [100/951], Loss: 0.6330\n",
      "Epoch [34/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [34/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [34/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [34/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [34/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [34/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [34/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [34/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [35/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [35/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [35/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [35/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [35/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [35/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [35/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [35/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [35/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [36/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [36/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [36/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [36/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [36/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [36/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [36/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [37/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [37/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [37/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [37/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [37/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [37/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [37/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [37/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [37/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [38/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [38/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [38/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [38/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [38/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [38/100], Step [600/951], Loss: 0.6335\n",
      "Epoch [38/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [38/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [38/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [39/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [39/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [39/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [39/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [39/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [39/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [39/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [39/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [39/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [40/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [40/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [40/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [40/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [40/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [40/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [40/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [40/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [40/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [41/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [41/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [41/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [41/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [41/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [41/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [41/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [41/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [41/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [42/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [42/100], Step [200/951], Loss: 0.6353\n",
      "Epoch [42/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [42/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [42/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [42/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [42/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [42/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [42/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [43/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [43/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [43/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [43/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [43/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [43/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [43/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [43/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [43/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [44/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [44/100], Step [200/951], Loss: 0.6352\n",
      "Epoch [44/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [44/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [44/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [44/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [44/100], Step [700/951], Loss: 0.6332\n",
      "Epoch [44/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [44/100], Step [900/951], Loss: 0.6330\n",
      "Epoch [45/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [45/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [45/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [45/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [45/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [45/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [45/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [45/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [45/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [46/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [46/100], Step [200/951], Loss: 0.6341\n",
      "Epoch [46/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [46/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [46/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [46/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [46/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [46/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [46/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [47/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [47/100], Step [200/951], Loss: 0.6339\n",
      "Epoch [47/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [47/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [47/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [47/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [47/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [47/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [47/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [48/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [48/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [48/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [48/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [48/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [48/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [49/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [49/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [49/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [49/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [49/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [49/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [49/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [50/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [50/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [50/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [50/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [50/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [50/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [50/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [51/100], Step [200/951], Loss: 0.6340\n",
      "Epoch [51/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [51/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [51/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [51/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [51/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [51/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [52/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [52/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [52/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [52/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [52/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [52/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [52/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [53/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [53/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [53/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [53/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [53/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [53/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [53/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [53/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [53/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [54/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [54/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [54/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [54/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [54/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [54/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [54/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [54/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [54/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [55/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [55/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [55/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [55/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [55/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [55/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [55/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [56/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [56/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [56/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [56/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [56/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [56/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [56/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [57/100], Step [300/951], Loss: 0.6330\n",
      "Epoch [57/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [57/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [57/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [57/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [57/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [57/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [58/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [58/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [58/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [58/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [58/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [58/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [59/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [59/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [59/100], Step [300/951], Loss: 0.6331\n",
      "Epoch [59/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [59/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [59/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [59/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [59/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [59/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [60/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [60/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [60/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [60/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [60/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [60/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [60/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [61/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [61/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [61/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [61/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [61/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [61/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [61/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [62/100], Step [200/951], Loss: 0.6338\n",
      "Epoch [62/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [62/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [62/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [62/100], Step [700/951], Loss: 0.6333\n",
      "Epoch [62/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [62/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [63/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [63/100], Step [200/951], Loss: 0.6336\n",
      "Epoch [63/100], Step [300/951], Loss: 0.6332\n",
      "Epoch [63/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [63/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [63/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [63/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [63/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [63/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [64/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [64/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [64/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [64/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [64/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [64/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [64/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [65/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [65/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [65/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [65/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [65/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [65/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [65/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [66/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [66/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [66/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [66/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [66/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [66/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [66/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [67/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [67/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [67/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [67/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [67/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [67/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [67/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [67/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [67/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [68/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [68/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [68/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [68/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [68/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [68/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [68/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [69/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [69/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [69/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [69/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [69/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [69/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [69/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [70/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [70/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [70/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [70/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [70/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [70/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [70/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [71/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [71/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [400/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [71/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [71/100], Step [700/951], Loss: 0.6331\n",
      "Epoch [71/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [71/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [72/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [72/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [72/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [72/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [72/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [72/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [72/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [72/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [72/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [73/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [73/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [73/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [73/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [73/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [73/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [73/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [73/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [73/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [74/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [74/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [74/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [74/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [74/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [74/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [74/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [74/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [74/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [75/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [75/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [75/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [75/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [75/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [75/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [75/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [75/100], Step [800/951], Loss: 0.6327\n",
      "Epoch [75/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [76/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [76/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [76/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [76/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [76/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [76/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [76/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [77/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [77/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [77/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [77/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [77/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [77/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [77/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [77/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [77/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [78/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [78/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [78/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [78/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [78/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [78/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [78/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [78/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [78/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [79/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [79/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [79/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [79/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [79/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [79/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [79/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [79/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [79/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [80/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [80/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [80/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [80/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [80/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [80/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [80/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [81/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [81/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [81/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [81/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [81/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [81/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [81/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [81/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [81/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [82/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [82/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [82/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [82/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [82/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [82/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [82/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [82/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [82/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [83/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [83/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [83/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [83/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [83/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [83/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [83/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [84/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [84/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [84/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [84/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [84/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [84/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [84/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [85/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [85/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [85/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [85/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [85/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [85/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [85/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [85/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [85/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [86/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [86/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [86/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [86/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [86/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [86/100], Step [600/951], Loss: 0.6334\n",
      "Epoch [86/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [86/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [86/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [87/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [87/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [87/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [87/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [87/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [87/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [87/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [88/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [88/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [88/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [88/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [88/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [88/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [88/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [88/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [88/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [89/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [89/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [89/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [89/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [89/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [89/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [89/100], Step [900/951], Loss: 0.6329\n",
      "Epoch [90/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [90/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [90/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [90/100], Step [400/951], Loss: 0.6327\n",
      "Epoch [90/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [90/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [90/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [90/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [90/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [91/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [91/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [91/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [91/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [91/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [91/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [91/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [92/100], Step [100/951], Loss: 0.6331\n",
      "Epoch [92/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [92/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [92/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [92/100], Step [500/951], Loss: 0.6389\n",
      "Epoch [92/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [92/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [92/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [92/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [93/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [93/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [93/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [93/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [93/100], Step [500/951], Loss: 0.6329\n",
      "Epoch [93/100], Step [600/951], Loss: 0.6338\n",
      "Epoch [93/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [93/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [93/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [94/100], Step [100/951], Loss: 0.6329\n",
      "Epoch [94/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [94/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [94/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [94/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [94/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [94/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [94/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [94/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [95/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [95/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [95/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [95/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [95/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [95/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [95/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [95/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [95/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [96/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [96/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [96/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [96/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [96/100], Step [500/951], Loss: 0.6331\n",
      "Epoch [96/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [96/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [96/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [96/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [97/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [97/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [97/100], Step [300/951], Loss: 0.6328\n",
      "Epoch [97/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [97/100], Step [500/951], Loss: 0.6330\n",
      "Epoch [97/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [97/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [97/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [97/100], Step [900/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [200/951], Loss: 0.6334\n",
      "Epoch [98/100], Step [300/951], Loss: 0.6329\n",
      "Epoch [98/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [98/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [98/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [98/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [98/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [98/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [99/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [99/100], Step [200/951], Loss: 0.6336\n",
      "Epoch [99/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [99/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [99/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [99/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [99/100], Step [700/951], Loss: 0.6330\n",
      "Epoch [99/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [99/100], Step [900/951], Loss: 0.6327\n",
      "Epoch [100/100], Step [100/951], Loss: 0.6328\n",
      "Epoch [100/100], Step [200/951], Loss: 0.6333\n",
      "Epoch [100/100], Step [300/951], Loss: 0.6327\n",
      "Epoch [100/100], Step [400/951], Loss: 0.6326\n",
      "Epoch [100/100], Step [500/951], Loss: 0.6328\n",
      "Epoch [100/100], Step [600/951], Loss: 0.6333\n",
      "Epoch [100/100], Step [700/951], Loss: 0.6337\n",
      "Epoch [100/100], Step [800/951], Loss: 0.6326\n",
      "Epoch [100/100], Step [900/951], Loss: 0.6329\n",
      "Test accuracy of the network: 68.21631878557875 %\n",
      "Train accuracy of the network: 96.73941625032869 %\n"
     ]
    }
   ],
   "source": [
    "fnn_test_accuracies = []\n",
    "fnn_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('fnn', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    fnn_test_accuracies.append(test_accuracy)\n",
    "    fnn_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, fnn_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, fnn_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHiddenLayerNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoHiddenLayerNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestTwoHiddenLayerModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = TwoHiddenLayerNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6434\n",
      "Test accuracy of the network: 72.85223367697594 %\n",
      "Train accuracy of the network: 78.69415807560138 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72.85223367697594, 78.69415807560138)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAndTestTwoHiddenLayerModel('corona', num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6434\n",
      "Test accuracy of the network: 72.85223367697594 %\n",
      "Train accuracy of the network: 78.69415807560138 %\n",
      "Epoch [1/10], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/10], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/10], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/10], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/10], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/10], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/10], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/10], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/10], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/10], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/10], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/10], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/10], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/10], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/10], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/10], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/10], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/10], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/10], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/10], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/10], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/10], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/10], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/10], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/10], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/10], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/10], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/10], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/10], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/10], Step [15/19], Loss: 0.6349\n",
      "Test accuracy of the network: 82.13058419243987 %\n",
      "Train accuracy of the network: 94.84536082474227 %\n",
      "Epoch [1/20], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/20], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/20], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/20], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/20], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/20], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/20], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/20], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/20], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/20], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/20], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/20], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/20], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/20], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/20], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/20], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/20], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/20], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/20], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/20], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/20], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/20], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/20], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/20], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/20], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/20], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/20], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/20], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/20], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/20], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/20], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/20], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/20], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/20], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/20], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/20], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/20], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/20], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/20], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/20], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/20], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/20], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/20], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/20], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/20], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/20], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/20], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/20], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/20], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/20], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/20], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/20], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/20], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/20], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/20], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/20], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/20], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/20], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/20], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/20], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.65979381443299 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/30], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/30], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/30], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/30], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/30], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/30], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/30], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/30], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/30], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/30], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/30], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/30], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/30], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/30], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/30], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/30], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/30], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/30], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/30], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/30], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/30], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/30], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/30], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/30], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/30], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/30], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/30], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/30], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/30], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/30], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/30], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/30], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/30], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/30], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/30], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/30], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/30], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/30], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/30], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/30], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/30], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/30], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/30], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/30], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/30], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/30], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/30], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/30], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/30], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/30], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/30], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/30], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/30], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/30], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/30], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/30], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/30], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/30], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/30], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/30], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/30], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/30], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.43069873997709 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6327\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6328\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6327\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6351\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6326\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 89.23253150057273 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6327\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6327\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6326\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 89.23253150057273 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6326\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 89.57617411225658 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6355\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6331\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6379\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6356\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6327\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6326\n",
      "Test accuracy of the network: 88.88888888888889 %\n",
      "Train accuracy of the network: 100.0 %\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6901\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6822\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6431\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6457\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6433\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6388\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6450\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6380\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6457\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6411\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6451\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6388\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6361\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6359\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6430\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6356\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6410\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6355\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6346\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6402\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6340\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6349\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6380\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6338\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6370\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6346\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6359\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6355\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6331\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6379\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6356\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6327\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6368\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6333\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6357\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6335\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6335\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6357\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6331\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 87.97250859106529 %\n",
      "Train accuracy of the network: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "corona_2hl_test_accuracies = []\n",
    "corona_2hl_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestTwoHiddenLayerModel('corona', num_epochs=num_epoch)\n",
    "    corona_2hl_test_accuracies.append(test_accuracy)\n",
    "    corona_2hl_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, corona_2hl_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, corona_2hl_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of epochs: 60\n",
      "89.57617411225658\n"
     ]
    }
   ],
   "source": [
    "print('Best number of epochs:', num_epochs_used[corona_2hl_test_accuracies.index(max(corona_2hl_test_accuracies))])\n",
    "print(max(corona_2hl_test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_2hl_test_accuracies = []\n",
    "liar_2hl_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestTwoHiddenLayerModel('liar', num_epochs=num_epoch)\n",
    "    liar_2hl_test_accuracies.append(test_accuracy)\n",
    "    liar_2hl_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, liar_2hl_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, liar_2hl_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liar Train Accuracies: [72.27610948711134, 74.01009832580388, 74.45522189742226, 73.97023651342015, 74.06989104437949, 73.73770927451501, 73.44538931703428, 73.64469837895297, 72.56842944459208]\n",
      "Liar Test Accuracies: [64.53396524486571, 66.35071090047393, 66.90363349131121, 66.9826224328594, 66.19273301737756, 66.82464454976304, 66.90363349131121, 66.5086887835703, 66.19273301737756]\n"
     ]
    }
   ],
   "source": [
    "print('Liar Train Accuracies:', liar_2hl_train_accuracies)\n",
    "print('Liar Test Accuracies:', liar_2hl_test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/5], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/5], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/5], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/5], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/5], Step [500/951], Loss: 0.7021\n",
      "Test accuracy of the network: 78.84250474383302 %\n",
      "Train accuracy of the network: 60.143307914804105 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/10], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/10], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/10], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/10], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/10], Step [500/951], Loss: 0.7021\n",
      "Epoch [6/10], Step [500/951], Loss: 0.6418\n",
      "Epoch [7/10], Step [500/951], Loss: 0.6408\n",
      "Epoch [8/10], Step [500/951], Loss: 0.6413\n",
      "Epoch [9/10], Step [500/951], Loss: 0.6402\n",
      "Epoch [10/10], Step [500/951], Loss: 0.6398\n",
      "Test accuracy of the network: 71.5370018975332 %\n",
      "Train accuracy of the network: 59.558243491980015 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/20], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/20], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/20], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/20], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/20], Step [500/951], Loss: 0.7021\n",
      "Epoch [6/20], Step [500/951], Loss: 0.6418\n",
      "Epoch [7/20], Step [500/951], Loss: 0.6408\n",
      "Epoch [8/20], Step [500/951], Loss: 0.6413\n",
      "Epoch [9/20], Step [500/951], Loss: 0.6402\n",
      "Epoch [10/20], Step [500/951], Loss: 0.6398\n",
      "Epoch [11/20], Step [500/951], Loss: 0.6411\n",
      "Epoch [12/20], Step [500/951], Loss: 0.6396\n",
      "Epoch [13/20], Step [500/951], Loss: 0.6393\n",
      "Epoch [14/20], Step [500/951], Loss: 0.6398\n",
      "Epoch [15/20], Step [500/951], Loss: 0.6400\n",
      "Epoch [16/20], Step [500/951], Loss: 0.6401\n",
      "Epoch [17/20], Step [500/951], Loss: 0.6400\n",
      "Epoch [18/20], Step [500/951], Loss: 0.6395\n",
      "Epoch [19/20], Step [500/951], Loss: 0.6398\n",
      "Epoch [20/20], Step [500/951], Loss: 0.6393\n",
      "Test accuracy of the network: 80.74003795066413 %\n",
      "Train accuracy of the network: 63.66026820930844 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/30], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/30], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/30], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/30], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/30], Step [500/951], Loss: 0.7021\n",
      "Epoch [6/30], Step [500/951], Loss: 0.6418\n",
      "Epoch [7/30], Step [500/951], Loss: 0.6408\n",
      "Epoch [8/30], Step [500/951], Loss: 0.6413\n",
      "Epoch [9/30], Step [500/951], Loss: 0.6402\n",
      "Epoch [10/30], Step [500/951], Loss: 0.6398\n",
      "Epoch [11/30], Step [500/951], Loss: 0.6411\n",
      "Epoch [12/30], Step [500/951], Loss: 0.6396\n",
      "Epoch [13/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [14/30], Step [500/951], Loss: 0.6398\n",
      "Epoch [15/30], Step [500/951], Loss: 0.6400\n",
      "Epoch [16/30], Step [500/951], Loss: 0.6401\n",
      "Epoch [17/30], Step [500/951], Loss: 0.6400\n",
      "Epoch [18/30], Step [500/951], Loss: 0.6395\n",
      "Epoch [19/30], Step [500/951], Loss: 0.6398\n",
      "Epoch [20/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [21/30], Step [500/951], Loss: 0.6394\n",
      "Epoch [22/30], Step [500/951], Loss: 0.6394\n",
      "Epoch [23/30], Step [500/951], Loss: 0.6399\n",
      "Epoch [24/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [25/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [26/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [27/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [28/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [29/30], Step [500/951], Loss: 0.6393\n",
      "Epoch [30/30], Step [500/951], Loss: 0.6397\n",
      "Test accuracy of the network: 83.20683111954459 %\n",
      "Train accuracy of the network: 64.96187220615303 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/40], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/40], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/40], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/40], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/40], Step [500/951], Loss: 0.7021\n",
      "Epoch [6/40], Step [500/951], Loss: 0.6418\n",
      "Epoch [7/40], Step [500/951], Loss: 0.6408\n",
      "Epoch [8/40], Step [500/951], Loss: 0.6413\n",
      "Epoch [9/40], Step [500/951], Loss: 0.6402\n",
      "Epoch [10/40], Step [500/951], Loss: 0.6398\n",
      "Epoch [11/40], Step [500/951], Loss: 0.6411\n",
      "Epoch [12/40], Step [500/951], Loss: 0.6396\n",
      "Epoch [13/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [14/40], Step [500/951], Loss: 0.6398\n",
      "Epoch [15/40], Step [500/951], Loss: 0.6400\n",
      "Epoch [16/40], Step [500/951], Loss: 0.6401\n",
      "Epoch [17/40], Step [500/951], Loss: 0.6400\n",
      "Epoch [18/40], Step [500/951], Loss: 0.6395\n",
      "Epoch [19/40], Step [500/951], Loss: 0.6398\n",
      "Epoch [20/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [21/40], Step [500/951], Loss: 0.6394\n",
      "Epoch [22/40], Step [500/951], Loss: 0.6394\n",
      "Epoch [23/40], Step [500/951], Loss: 0.6399\n",
      "Epoch [24/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [25/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [26/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [27/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [28/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [29/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [30/40], Step [500/951], Loss: 0.6397\n",
      "Epoch [31/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [32/40], Step [500/951], Loss: 0.6394\n",
      "Epoch [33/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [34/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [35/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [36/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [37/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [38/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [39/40], Step [500/951], Loss: 0.6393\n",
      "Epoch [40/40], Step [500/951], Loss: 0.6393\n",
      "Test accuracy of the network: 82.63757115749526 %\n",
      "Train accuracy of the network: 66.09913226400211 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15212, 4975)\n",
      "Epoch [1/50], Step [500/951], Loss: 0.6435\n",
      "Epoch [2/50], Step [500/951], Loss: 0.6420\n",
      "Epoch [3/50], Step [500/951], Loss: 0.6421\n",
      "Epoch [4/50], Step [500/951], Loss: 0.6422\n",
      "Epoch [5/50], Step [500/951], Loss: 0.7021\n",
      "Epoch [6/50], Step [500/951], Loss: 0.6418\n",
      "Epoch [7/50], Step [500/951], Loss: 0.6408\n",
      "Epoch [8/50], Step [500/951], Loss: 0.6413\n",
      "Epoch [9/50], Step [500/951], Loss: 0.6402\n",
      "Epoch [10/50], Step [500/951], Loss: 0.6398\n",
      "Epoch [11/50], Step [500/951], Loss: 0.6411\n",
      "Epoch [12/50], Step [500/951], Loss: 0.6396\n",
      "Epoch [13/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [14/50], Step [500/951], Loss: 0.6398\n",
      "Epoch [15/50], Step [500/951], Loss: 0.6400\n",
      "Epoch [16/50], Step [500/951], Loss: 0.6401\n",
      "Epoch [17/50], Step [500/951], Loss: 0.6400\n",
      "Epoch [18/50], Step [500/951], Loss: 0.6395\n",
      "Epoch [19/50], Step [500/951], Loss: 0.6398\n",
      "Epoch [20/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [21/50], Step [500/951], Loss: 0.6394\n",
      "Epoch [22/50], Step [500/951], Loss: 0.6394\n",
      "Epoch [23/50], Step [500/951], Loss: 0.6399\n",
      "Epoch [24/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [25/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [26/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [27/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [28/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [29/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [30/50], Step [500/951], Loss: 0.6397\n",
      "Epoch [31/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [32/50], Step [500/951], Loss: 0.6394\n",
      "Epoch [33/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [34/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [35/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [36/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [37/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [38/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [39/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [40/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [41/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [42/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [43/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [44/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [45/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [46/50], Step [500/951], Loss: 0.6396\n",
      "Epoch [47/50], Step [500/951], Loss: 0.6393\n",
      "Epoch [48/50], Step [500/951], Loss: 0.6393\n"
     ]
    }
   ],
   "source": [
    "fnn_2hl_test_accuracies = []\n",
    "fnn_2hl_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestTwoHiddenLayerModel('fnn', num_epochs=num_epoch, print_epoch_mod=500)\n",
    "    fnn_2hl_test_accuracies.append(test_accuracy)\n",
    "    fnn_2hl_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, fnn_2hl_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, fnn_2hl_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
