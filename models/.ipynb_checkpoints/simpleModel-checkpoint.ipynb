{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/5], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6444\n",
      "Test accuracy of the network: 65.75028636884306 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/10], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/10], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/10], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/10], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/10], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/10], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/10], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/10], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/10], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/10], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/10], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/10], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/10], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/10], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/10], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/10], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/10], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/10], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/10], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/10], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/10], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/10], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/10], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/10], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/10], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/10], Step [15/19], Loss: 0.6455\n",
      "Accuracy of the network on the 10000 test images: 67.46849942726232 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/20], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/20], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/20], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/20], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/20], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/20], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/20], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/20], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/20], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/20], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/20], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/20], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/20], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/20], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/20], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/20], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/20], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/20], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/20], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/20], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/20], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/20], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/20], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/20], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/20], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/20], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/20], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/20], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/20], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/20], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/20], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/20], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/20], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/20], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/20], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/20], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/20], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/20], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/20], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/20], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/20], Step [15/19], Loss: 0.6403\n",
      "Accuracy of the network on the 10000 test images: 78.35051546391753 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/30], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/30], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/30], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/30], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/30], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/30], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/30], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/30], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/30], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/30], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/30], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/30], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/30], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/30], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/30], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/30], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/30], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/30], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/30], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/30], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/30], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/30], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/30], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/30], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/30], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/30], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/30], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/30], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/30], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/30], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/30], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/30], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/30], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/30], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/30], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/30], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/30], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/30], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/30], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/30], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/30], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/30], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/30], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/30], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/30], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/30], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/30], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/30], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/30], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/30], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/30], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/30], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/30], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/30], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/30], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/30], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/30], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/30], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/30], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/30], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/30], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/30], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/30], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/30], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/30], Step [15/19], Loss: 0.6351\n",
      "Accuracy of the network on the 10000 test images: 86.5979381443299 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6330\n",
      "Accuracy of the network on the 10000 test images: 88.31615120274914 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.20160366552119 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.54524627720504 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.20160366552119 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Accuracy of the network on the 10000 test images: 88.54524627720504 %\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('corona', num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/5], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/5], Step [300/941], Loss: 0.6436\n"
     ]
    }
   ],
   "source": [
    "trainAndTestSimpleModel('liar', num_epochs=5, print_epoch_mod=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
