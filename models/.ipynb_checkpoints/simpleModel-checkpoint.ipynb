{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../preprocessing/') #need this in order to get to the other file in other directory\n",
    "\n",
    "#can comment out the ones you aren't using to save a little bit of time\n",
    "from covidPreprocess import getCoronaVocabulary, getCoronaText\n",
    "from liarPreprocess import getLiarVocabulary, getLiarText\n",
    "from fnnPreprocess import getFNNVocabulary, getFNNText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        #Written based off of the tutorial at\n",
    "        #https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()   \n",
    "        self.hOutput1 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.hOutput1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestSimpleModel(dataset: str, num_epochs = 5, learning_rate = 0.001, print_epoch_mod = 5):\n",
    "    '''\n",
    "    gets around 63-71% for corona and Liar datasets, around 80-83% on FNN\n",
    "    \n",
    "    used this article for help in writing the tensor parts of code so it works with the model\n",
    "    https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8\n",
    "    '''\n",
    "    torch.manual_seed(1)\n",
    "    if dataset == 'corona':\n",
    "        X,Y = getCoronaText() #this function will give us the text array (not document term matrix) and Y\n",
    "        X_train,Y_train, vectorizer_train = getCoronaVocabulary(True)\n",
    "    elif dataset == 'liar':\n",
    "        X,Y = getLiarText()\n",
    "        X_train,Y_train, vectorizer_train = getLiarVocabulary(True)\n",
    "    elif dataset == 'fnn':\n",
    "        X,Y = getFNNText()\n",
    "        X_train,Y_train, vectorizer_train = getFNNVocabulary(True)\n",
    "    \n",
    "    #transform our testing dataset to match the vocabulary for the training dataset\n",
    "    #transform will return the document-term matrix for X based on training dataset\n",
    "    x_test = vectorizer_train.transform(X)\n",
    "    \n",
    "    #sample test on logistic classifier\n",
    "    '''classifier = LogisticRegression()\n",
    "    classifier.fit(X_train,Y_train)\n",
    "    score = classifier.score(x_test,Y)\n",
    "    print(score)'''\n",
    "    \n",
    "    vocabsize = X_train.shape[1]\n",
    "    \n",
    "    \n",
    "    #transform our training and test data into tensors for the classifier to learn off of\n",
    "    X_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "    Y_tensor = torch.from_numpy(np.array(Y_train))\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(x_test.todense()).float()\n",
    "    Y_test_tensor = torch.from_numpy(np.array(Y))\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    #use TensorDataset to be able to use our DataLoader\n",
    "    train_data = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    test_data = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "#     test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=16, shuffle=False)\n",
    "    \n",
    "    #initialize our model\n",
    "    model = SimpleNeuralNet(vocabsize, 200, 2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, labels) in enumerate(train_loader):\n",
    "    \n",
    "            # Forward pass\n",
    "            # The forward process computes the loss of each iteration on each sample\n",
    "            model.train()\n",
    "            y_pred = model(x_batch)\n",
    "            #need to transform labels to long datatype using .long() or it complains it's an int\n",
    "            loss = criterion(y_pred, labels.long())\n",
    "    \n",
    "            # Backward pass, using the optimizer to update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    #compute gradients\n",
    "            optimizer.step()   #initiate gradient descent\n",
    "    \n",
    "     \n",
    "            # Below, an epoch corresponds to one pass through all of the samples.\n",
    "            # Each training step corresponds to a parameter update using \n",
    "            # a gradient computed on a minibatch of 100 samples \n",
    "            if (i + 1) % print_epoch_mod == 0: \n",
    "                #leaving it on 5 for corona dataset, probably want to change to % 50 or % 100\n",
    "                # for the other datasets so don't get spammed \n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "    # Test the model\n",
    "    # In the test phase, we don't need to compute gradients (the model has already been learned)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        \n",
    "    # Print out training accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Train accuracy of the network: {} %'.format(100 * correct / total))\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/5], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/5], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/5], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/5], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/5], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/5], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/5], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/5], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/5], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/5], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/5], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/5], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/5], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/5], Step [15/19], Loss: 0.6444\n",
      "Test accuracy of the network: 65.75028636884306 %\n",
      "Train accuracy of the network: 76.97594501718213 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=5)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/10], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/10], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/10], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/10], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/10], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/10], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/10], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/10], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/10], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/10], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/10], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/10], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/10], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/10], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/10], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/10], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/10], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/10], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/10], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/10], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/10], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/10], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/10], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/10], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/10], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/10], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/10], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/10], Step [15/19], Loss: 0.6455\n",
      "Test accuracy of the network: 67.46849942726232 %\n",
      "Train accuracy of the network: 74.5704467353952 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=10)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/20], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/20], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/20], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/20], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/20], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/20], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/20], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/20], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/20], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/20], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/20], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/20], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/20], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/20], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/20], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/20], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/20], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/20], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/20], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/20], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/20], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/20], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/20], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/20], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/20], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/20], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/20], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/20], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/20], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/20], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/20], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/20], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/20], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/20], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/20], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/20], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/20], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/20], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/20], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/20], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/20], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/20], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/20], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/20], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/20], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/20], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/20], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/20], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/20], Step [15/19], Loss: 0.6403\n",
      "Test accuracy of the network: 78.35051546391753 %\n",
      "Train accuracy of the network: 88.65979381443299 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=20)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/30], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/30], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/30], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/30], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/30], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/30], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/30], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/30], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/30], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/30], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/30], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/30], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/30], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/30], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/30], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/30], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/30], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/30], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/30], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/30], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/30], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/30], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/30], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/30], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/30], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/30], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/30], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/30], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/30], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/30], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/30], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/30], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/30], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/30], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/30], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/30], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/30], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/30], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/30], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/30], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/30], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/30], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/30], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/30], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/30], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/30], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/30], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/30], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/30], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/30], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/30], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/30], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/30], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/30], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/30], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/30], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/30], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/30], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/30], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/30], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/30], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/30], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/30], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/30], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/30], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/30], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/30], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/30], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/30], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/30], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/30], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/30], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/30], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/30], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/30], Step [15/19], Loss: 0.6351\n",
      "Test accuracy of the network: 86.5979381443299 %\n",
      "Train accuracy of the network: 98.62542955326461 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=30)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/40], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/40], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/40], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/40], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/40], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/40], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/40], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/40], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/40], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/40], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/40], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/40], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/40], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/40], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/40], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/40], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/40], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/40], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/40], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/40], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/40], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/40], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/40], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/40], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/40], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/40], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/40], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/40], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/40], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/40], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/40], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/40], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/40], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/40], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/40], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/40], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/40], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/40], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/40], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/40], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/40], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/40], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/40], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/40], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/40], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/40], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/40], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/40], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/40], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/40], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/40], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/40], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/40], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/40], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/40], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/40], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/40], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/40], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/40], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/40], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/40], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/40], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/40], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/40], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/40], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/40], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/40], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/40], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/40], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/40], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/40], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/40], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/40], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/40], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/40], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/40], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/40], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/40], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/40], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/40], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/40], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/40], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/40], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/40], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/40], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/40], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/40], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/40], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/40], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/40], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/40], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/40], Step [15/19], Loss: 0.6330\n",
      "Test accuracy of the network: 88.31615120274914 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=40)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/50], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/50], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/50], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/50], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/50], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/50], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/50], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/50], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/50], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/50], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/50], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/50], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/50], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/50], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/50], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/50], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/50], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/50], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/50], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/50], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/50], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/50], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/50], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/50], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/50], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/50], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/50], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/50], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/50], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/50], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/50], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/50], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/50], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/50], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/50], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/50], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/50], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/50], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/50], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/50], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/50], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/50], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/50], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/50], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/50], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/50], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/50], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/50], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/50], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/50], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/50], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/50], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/50], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/50], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/50], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/50], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/50], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/50], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/50], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/50], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/50], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/50], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/50], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/50], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/50], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/50], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/50], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/50], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/50], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/50], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/50], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/50], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/50], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/50], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/50], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/50], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/50], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/50], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/50], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/50], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/50], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/50], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/50], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/50], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/50], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/50], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/50], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/50], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/50], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/50], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/50], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/50], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/50], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/50], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=50)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/60], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/60], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/60], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/60], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/60], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/60], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/60], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/60], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/60], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/60], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/60], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/60], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/60], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/60], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/60], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/60], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/60], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/60], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/60], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/60], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/60], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/60], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/60], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/60], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/60], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/60], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/60], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/60], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/60], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/60], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/60], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/60], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/60], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/60], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/60], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/60], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/60], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/60], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/60], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/60], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/60], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/60], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/60], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/60], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/60], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/60], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/60], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/60], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/60], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/60], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/60], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/60], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/60], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/60], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/60], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/60], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/60], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/60], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/60], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/60], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/60], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/60], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/60], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/60], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/60], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/60], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/60], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/60], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/60], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/60], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/60], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/60], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/60], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/60], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/60], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/60], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/60], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/60], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/60], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/60], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/60], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/60], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/60], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/60], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/60], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/60], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/60], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/60], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/60], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/60], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/60], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/60], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/60], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/60], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/60], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/60], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.3127147766323 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=60)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/75], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/75], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/75], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/75], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/75], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/75], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/75], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/75], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/75], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/75], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/75], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/75], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/75], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/75], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/75], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/75], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/75], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/75], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/75], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/75], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/75], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/75], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/75], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/75], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/75], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/75], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/75], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/75], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/75], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/75], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/75], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/75], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/75], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/75], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/75], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/75], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/75], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/75], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/75], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/75], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/75], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/75], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/75], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/75], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/75], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/75], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/75], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/75], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/75], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/75], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/75], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/75], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/75], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/75], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/75], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/75], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/75], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/75], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/75], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/75], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/75], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/75], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/75], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/75], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/75], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/75], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/75], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/75], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/75], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/75], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/75], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/75], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/75], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/75], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/75], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/75], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/75], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/75], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/75], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/75], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/75], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/75], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/75], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/75], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/75], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/75], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/75], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/75], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/75], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/75], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/75], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/75], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/75], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/75], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/75], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/75], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/75], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.20160366552119 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=75)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "there are 70 nan titles\n",
      "there are 9 nan text\n",
      "\n",
      "Extracting tokens....\n",
      "there are 12 nan titles\n",
      "there are 1 nan text\n",
      "Data shape for text:  (291, 7417)\n",
      "Epoch [1/100], Step [5/19], Loss: 0.6801\n",
      "Epoch [1/100], Step [10/19], Loss: 0.6651\n",
      "Epoch [1/100], Step [15/19], Loss: 0.6412\n",
      "Epoch [2/100], Step [5/19], Loss: 0.6423\n",
      "Epoch [2/100], Step [10/19], Loss: 0.6454\n",
      "Epoch [2/100], Step [15/19], Loss: 0.6420\n",
      "Epoch [3/100], Step [5/19], Loss: 0.6454\n",
      "Epoch [3/100], Step [10/19], Loss: 0.6450\n",
      "Epoch [3/100], Step [15/19], Loss: 0.6443\n",
      "Epoch [4/100], Step [5/19], Loss: 0.6441\n",
      "Epoch [4/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [4/100], Step [15/19], Loss: 0.6399\n",
      "Epoch [5/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [5/100], Step [10/19], Loss: 0.6418\n",
      "Epoch [5/100], Step [15/19], Loss: 0.6444\n",
      "Epoch [6/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [6/100], Step [10/19], Loss: 0.6404\n",
      "Epoch [6/100], Step [15/19], Loss: 0.6418\n",
      "Epoch [7/100], Step [5/19], Loss: 0.6445\n",
      "Epoch [7/100], Step [10/19], Loss: 0.6443\n",
      "Epoch [7/100], Step [15/19], Loss: 0.6434\n",
      "Epoch [8/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [8/100], Step [10/19], Loss: 0.6412\n",
      "Epoch [8/100], Step [15/19], Loss: 0.6427\n",
      "Epoch [9/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [9/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [9/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [10/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [10/100], Step [10/19], Loss: 0.6406\n",
      "Epoch [10/100], Step [15/19], Loss: 0.6455\n",
      "Epoch [11/100], Step [5/19], Loss: 0.6411\n",
      "Epoch [11/100], Step [10/19], Loss: 0.6403\n",
      "Epoch [11/100], Step [15/19], Loss: 0.6454\n",
      "Epoch [12/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [12/100], Step [10/19], Loss: 0.6408\n",
      "Epoch [12/100], Step [15/19], Loss: 0.6445\n",
      "Epoch [13/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [13/100], Step [10/19], Loss: 0.6410\n",
      "Epoch [13/100], Step [15/19], Loss: 0.6429\n",
      "Epoch [14/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [14/100], Step [10/19], Loss: 0.6397\n",
      "Epoch [14/100], Step [15/19], Loss: 0.6442\n",
      "Epoch [15/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [15/100], Step [10/19], Loss: 0.6399\n",
      "Epoch [15/100], Step [15/19], Loss: 0.6425\n",
      "Epoch [16/100], Step [5/19], Loss: 0.6408\n",
      "Epoch [16/100], Step [10/19], Loss: 0.6382\n",
      "Epoch [16/100], Step [15/19], Loss: 0.6440\n",
      "Epoch [17/100], Step [5/19], Loss: 0.6413\n",
      "Epoch [17/100], Step [10/19], Loss: 0.6373\n",
      "Epoch [17/100], Step [15/19], Loss: 0.6378\n",
      "Epoch [18/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [18/100], Step [10/19], Loss: 0.6401\n",
      "Epoch [18/100], Step [15/19], Loss: 0.6367\n",
      "Epoch [19/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [19/100], Step [10/19], Loss: 0.6392\n",
      "Epoch [19/100], Step [15/19], Loss: 0.6369\n",
      "Epoch [20/100], Step [5/19], Loss: 0.6420\n",
      "Epoch [20/100], Step [10/19], Loss: 0.6360\n",
      "Epoch [20/100], Step [15/19], Loss: 0.6403\n",
      "Epoch [21/100], Step [5/19], Loss: 0.6419\n",
      "Epoch [21/100], Step [10/19], Loss: 0.6368\n",
      "Epoch [21/100], Step [15/19], Loss: 0.6397\n",
      "Epoch [22/100], Step [5/19], Loss: 0.6415\n",
      "Epoch [22/100], Step [10/19], Loss: 0.6367\n",
      "Epoch [22/100], Step [15/19], Loss: 0.6392\n",
      "Epoch [23/100], Step [5/19], Loss: 0.6412\n",
      "Epoch [23/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [23/100], Step [15/19], Loss: 0.6344\n",
      "Epoch [24/100], Step [5/19], Loss: 0.6418\n",
      "Epoch [24/100], Step [10/19], Loss: 0.6389\n",
      "Epoch [24/100], Step [15/19], Loss: 0.6348\n",
      "Epoch [25/100], Step [5/19], Loss: 0.6417\n",
      "Epoch [25/100], Step [10/19], Loss: 0.6352\n",
      "Epoch [25/100], Step [15/19], Loss: 0.6355\n",
      "Epoch [26/100], Step [5/19], Loss: 0.6405\n",
      "Epoch [26/100], Step [10/19], Loss: 0.6346\n",
      "Epoch [26/100], Step [15/19], Loss: 0.6354\n",
      "Epoch [27/100], Step [5/19], Loss: 0.6374\n",
      "Epoch [27/100], Step [10/19], Loss: 0.6342\n",
      "Epoch [27/100], Step [15/19], Loss: 0.6363\n",
      "Epoch [28/100], Step [5/19], Loss: 0.6365\n",
      "Epoch [28/100], Step [10/19], Loss: 0.6339\n",
      "Epoch [28/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [29/100], Step [5/19], Loss: 0.6367\n",
      "Epoch [29/100], Step [10/19], Loss: 0.6336\n",
      "Epoch [29/100], Step [15/19], Loss: 0.6361\n",
      "Epoch [30/100], Step [5/19], Loss: 0.6360\n",
      "Epoch [30/100], Step [10/19], Loss: 0.6334\n",
      "Epoch [30/100], Step [15/19], Loss: 0.6351\n",
      "Epoch [31/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [31/100], Step [10/19], Loss: 0.6332\n",
      "Epoch [31/100], Step [15/19], Loss: 0.6347\n",
      "Epoch [32/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [32/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [32/100], Step [15/19], Loss: 0.6343\n",
      "Epoch [33/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [33/100], Step [10/19], Loss: 0.6331\n",
      "Epoch [33/100], Step [15/19], Loss: 0.6340\n",
      "Epoch [34/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [34/100], Step [10/19], Loss: 0.6330\n",
      "Epoch [34/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [35/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [35/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [35/100], Step [15/19], Loss: 0.6337\n",
      "Epoch [36/100], Step [5/19], Loss: 0.6354\n",
      "Epoch [36/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [36/100], Step [15/19], Loss: 0.6338\n",
      "Epoch [37/100], Step [5/19], Loss: 0.6369\n",
      "Epoch [37/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [37/100], Step [15/19], Loss: 0.6334\n",
      "Epoch [38/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [38/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [38/100], Step [15/19], Loss: 0.6332\n",
      "Epoch [39/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [39/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [39/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [40/100], Step [5/19], Loss: 0.6353\n",
      "Epoch [40/100], Step [10/19], Loss: 0.6329\n",
      "Epoch [40/100], Step [15/19], Loss: 0.6330\n",
      "Epoch [41/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [41/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [41/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [42/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [42/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [42/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [43/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [43/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [43/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [44/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [44/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [45/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [45/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [46/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [46/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [47/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [47/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [48/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [48/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [49/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [49/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [50/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [50/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [51/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [51/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [52/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [52/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [53/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [53/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [54/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [54/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [55/100], Step [10/19], Loss: 0.6328\n",
      "Epoch [55/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [56/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [56/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [56/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [57/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [57/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [57/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [58/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [58/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [58/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [59/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [59/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [59/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [60/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [60/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [60/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [61/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [61/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [61/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [62/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [62/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [62/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [63/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [63/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [63/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [64/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [64/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [64/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [65/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [65/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [65/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [66/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [66/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [66/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [67/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [67/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [67/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [68/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [68/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [68/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [69/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [69/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [69/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [70/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [70/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [70/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [71/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [71/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [71/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [72/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [72/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [72/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [73/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [73/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [73/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [74/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [74/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [74/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [75/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [75/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [75/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [76/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [76/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [76/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [77/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [77/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [77/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [78/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [78/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [78/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [79/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [79/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [79/100], Step [15/19], Loss: 0.6329\n",
      "Epoch [80/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [80/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [80/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [81/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [81/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [81/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [82/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [82/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [82/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [83/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [83/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [83/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [84/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [84/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [84/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [85/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [85/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [85/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [86/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [86/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [86/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [87/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [87/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [87/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [88/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [88/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [88/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [89/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [89/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [89/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [90/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [90/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [90/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [91/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [91/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [91/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [92/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [92/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [92/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [93/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [93/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [93/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [94/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [94/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [94/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [95/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [95/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [95/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [96/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [96/100], Step [10/19], Loss: 0.6327\n",
      "Epoch [96/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [97/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [97/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [97/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [98/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [98/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [98/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [99/100], Step [5/19], Loss: 0.6352\n",
      "Epoch [99/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [99/100], Step [15/19], Loss: 0.6328\n",
      "Epoch [100/100], Step [5/19], Loss: 0.6351\n",
      "Epoch [100/100], Step [10/19], Loss: 0.6326\n",
      "Epoch [100/100], Step [15/19], Loss: 0.6328\n",
      "Test accuracy of the network: 88.54524627720504 %\n",
      "Train accuracy of the network: 99.65635738831615 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, train_accuracy = trainAndTestSimpleModel('corona', num_epochs=100)\n",
    "test_accuracies.append(test_accuracy)\n",
    "train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_used = [5, 10, 20, 30, 40, 50, 60, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHSCAYAAAAubIVMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABFyElEQVR4nO3deXxU1d3H8e/JCiEBJEREFkFUFJEABqpoBUXEKsgiKq5gXau4A7VVq09bn8cK1rrvGkRFFAW0qAi40KqVRZYiKKAsgiwBWRKWkOU8f9wwM4EEQmaSc2fm83698so5d7YfuUq+3HPv7xprrQAAABC+BNcFAAAAxAqCFQAAQIQQrAAAACKEYAUAABAhBCsAAIAIIVgBAABESJLrAiSpcePGtlWrVq7LAAAAOKi5c+dustZmVfSYL4JVq1atNGfOHNdlAAAAHJQxZlVlj7EUCAAAECEEKwAAgAghWAEAAESIL86xqkhRUZHWrFmj3bt3uy4FVVCnTh01b95cycnJrksBAMAZ3warNWvWKCMjQ61atZIxxnU5OABrrTZv3qw1a9aodevWrssBAMAZ3y4F7t69W5mZmYSqKGCMUWZmJkcXAQBxz7fBShKhKoqwrwAA8Hmwcmnz5s3q2LGjOnbsqCOOOELNmjULzPfs2XPA186ZM0e33nrrIX/mvHnzZIzR1KlTq1s2AABwyLfnWLmWmZmp+fPnS5IeeOABpaena/jw4YHHi4uLlZRU8Y8vJydHOTk5h/yZ48aN0+mnn65x48apd+/e1aq7KkpKSpSYmFhj7w8AQLziiNUhGDp0qO68806deeaZ+v3vf69Zs2apW7du6tSpk7p166bvv/9ekvTZZ5+pT58+krxQ9tvf/lY9evTQ0Ucfrccff7zC97bWasKECcrNzdXHH39c7nylhx9+WCeddJKys7N19913S5KWL1+us88+W9nZ2ercubN++OGHcp8rScOGDVNubq4kr7v9n//8Z51++ul6++239cILL6hLly7Kzs7WhRdeqJ07d0qSNmzYoAEDBig7O1vZ2dn68ssvdd999+mxxx4LvO8999xT6Z8DAIB4dtAjVsaYlyX1kbTRWtu+bFsjSeMltZK0UtLF1totZY/9QdI1kkok3WqtDX9dqybP37H2kJ6+dOlSTZ8+XYmJidq+fbtmzpyppKQkTZ8+XX/84x/1zjvv7Pea7777Tp9++qny8/PVtm1b/e53v9uvLcEXX3yh1q1bq02bNurRo4c++OADDRw4UB9++KEmTZqkr7/+Wmlpafrll18kSZdffrnuvvtuDRgwQLt371Zpaal++umnA9Zep04d/fvf/5bkLXVed911kqR7771XL730km655Rbdeuut6t69uyZOnKiSkhIVFBToyCOP1MCBA3XbbbeptLRUb775pmbNmnVIPzcAAOJBVZYCcyU9KenVkG13S5phrX3IGHN32fz3xph2kgZLOlHSkZKmG2OOs9aWRLZsdy666KLAMtq2bds0ZMgQLVu2TMYYFRUVVfia888/X6mpqUpNTdXhhx+uDRs2qHnz5uWeM27cOA0ePFiSNHjwYI0dO1YDBw7U9OnTdfXVVystLU2S1KhRI+Xn52vt2rUaMGCAJC8wVcUll1wSGC9atEj33nuvtm7dqoKCgsDS4yeffKJXX/V2dWJioho0aKAGDRooMzNT8+bN04YNG9SpUydlZmZW9UcGAEDcOGiwstbONMa02mdzP0k9ysZjJH0m6fdl29+01hZKWmGMWS6pq6SvIlSvc/Xq1QuM77vvPp155pmaOHGiVq5cqR49elT4mtTU1MA4MTFRxcXF5R4vKSnRO++8o/fee08PPvhgoC9Ufn6+rLX7XXFnKznKlpSUpNLS0sB83/YHobUPHTpUkyZNUnZ2tnJzc/XZZ58d8M997bXXKjc3V+vXr9dvf/vbAz4XAIB4Vd1zrJpYa9dJUtn3w8u2N5MUuh61pmxbeKytua8wbNu2Tc2aeX+8vecyVcf06dOVnZ2tn376SStXrtSqVat04YUXatKkSTrnnHP08ssvB86B+uWXX1S/fn01b95ckyZNkiQVFhZq586dOuqoo7R48WIVFhZq27ZtmjFjRqWfmZ+fr6ZNm6qoqEivv/56YHvPnj31zDPPSPIC3/bt2yVJAwYM0EcffaTZs2fX6In1AABEs0ifvF7RyVAVphdjzPXGmDnGmDl5eXkRLqN2jBw5Un/4wx902mmnqaSk+qud48aNCyzr7XXhhRfqjTfe0LnnnqsLLrhAOTk56tixo0aPHi1JGjt2rB5//HF16NBB3bp10/r169WiRQtdfPHF6tChgy6//HJ16tSp0s/8y1/+ol/96lfq1auXjj/++MD2xx57TJ9++qlOOukknXzyyfr2228lSSkpKTrzzDN18cUXc0UhAACVMJUtK5V7krcU+M+Qk9e/l9TDWrvOGNNU0mfW2rZlJ67LWvt/Zc+bKukBa+0BlwJzcnLsnDlzym1bsmSJTjjhhGr8kVATSktL1blzZ7399ts69thjK3wO+wwAEA+MMXOttRX2VapuH6v3JA2R9FDZ98kh298wxvxd3snrx0ri8rEot3jxYvXp00cDBgyoNFQBwCErKpLy86WCAu/7vl8VbQ/dtnNn2Kd0IEbdfrs0dKiTj65Ku4Vx8k5Ub2yMWSPpfnmB6i1jzDWSVku6SJKstd8aY96StFhSsaSbY+mKwHjVrl07/fjjj67LAOBaUVH1Q1BF2woLXf+JEKs2bnT20VW5KvDSSh7qWcnzH5T0YDhFAQAioLg4ciGIIARUCbe0AQC/2DcIhXt0yO9BKCFBysjwvtLTg+PQr4q2791Wr573HsC+mjZ19tEEKyCabd0qff+9tHSp9/X999LKld4vaPhfSUn5YLRP7znfOVgQOtRtdevW7J01AAcIVoDfFRZKP/wQDE6hISpKW5WgluwNQpEIQQQhoEoIVpXYvHmzevb0TiNbv369EhMTlZWVJUmaNWuWUlJSDvj6zz77TCkpKerWrVulz+nXr582btyor76Kmcb0qK7SUmnt2vLhae/3lSu9xxH7EhIiF4IIQoATBKtKZGZmav78+ZKkBx54QOnp6Ro+fHiVX//ZZ58pPT290mC1detWffPNN0pPT9eKFSvUunXrSJS9n+LiYiUlsZt9o6Klu6VLpWXLvEvHD1VqqnTssVLbttJxx3nfjznG+4UK/9s3SBGEgKjHb9xDMHfuXN15550qKChQ48aNlZubq6ZNm+rxxx/Xs88+q6SkJLVr104PPfSQnn32WSUmJuq1117TE088oV//+tfl3uudd95R37591aRJE7355pv6wx/+IElavny5brzxRuXl5SkxMVFvv/222rRpo4cfflhjx45VQkKCfvOb3+ihhx5Sjx49NHr0aOXk5GjTpk3KycnRypUrlZubqylTpmj37t3asWOH3nvvPfXr109btmxRUVGR/vrXv6pfv36SpFdffVWjR4+WMUYdOnTQ008/rQ4dOmjp0qVKTk7W9u3b1aFDBy1btkzJycm1/jOPSpFeujNGOuooLzjtDU97v7dowcm7AOAjURGszP/U3L/g7P1Vay5nrdUtt9yiyZMnKysrS+PHj9c999yjl19+WQ899JBWrFih1NRUbd26VQ0bNtSNN954wKNc48aN0/33368mTZpo0KBBgWB1+eWX6+6779aAAQO0e/dulZaW6sMPP9SkSZP09ddfKy0tTb/88stB6/3qq6+0cOFCNWrUSMXFxZo4caLq16+vTZs26ZRTTtEFF1ygxYsX68EHH9QXX3yhxo0b65dfflFGRoZ69OihKVOmqH///nrzzTd14YUXEqr2VdHSXejJ49VZusvMrDg8tWnDESgAiBJREaz8oLCwUIsWLVKvXr0keTcoblp2Oefee/P1799f/fv3P+h7bdiwQcuXL9fpp58uY4ySkpK0aNEiHXXUUVq7dm3gvoF16tSR5N2k+eqrr1ZaWpokqVGjRgf9jF69egWeZ63VH//4R82cOVMJCQlau3atNmzYoE8++USDBg1S48aNy73vtddeq4cfflj9+/fXK6+8ohdeeOEQflIxZuvWis97itTSXWiIysyMePkAgNpFsKoia61OPPHECk80nzJlimbOnKn33ntPf/nLXwI3Lq7M+PHjtWXLlsB5Vdu3b9ebb76pkSNHVvrZpoLzLpKSklRadmRk9z6XaderVy8wfv3115WXl6e5c+cqOTlZrVq10u7duyt939NOO00rV67U559/rpKSErVv3/6Af56oV1go/fjj/uFp6dLqde81RmrZsuLw1LIlS3cAEMOiIlhVdbmuJqWmpiovL09fffWVTj31VBUVFWnp0qU64YQT9NNPP+nMM8/U6aefrjfeeEMFBQXKyMjQ9u3bK3yvcePG6aOPPtKpp54qSVqxYoV69eqlv/71r2revLkmTZqk/v37q7CwUCUlJTrnnHP05z//WZdddllgKbBRo0Zq1aqV5s6dq65du2rChAmV1r5t2zYdfvjhSk5O1qeffqpVq1ZJknr27KkBAwbojjvuUGZmZuB9Jemqq67SpZdeqvvuuy/CP0mfKCmRRoyQJk+u/tJdo0bll+z2hihOHgeAuBUVwcoPEhISNGHCBN16663atm2biouLdfvtt+u4447TFVdcoW3btslaqzvuuEMNGzZU3759NWjQIE2ePLncyesrV67U6tWrdcoppwTeu3Xr1qpfv76+/vprjR07VjfccIP+9Kc/KTk5WW+//bbOPfdczZ8/Xzk5OUpJSdF5552n//3f/9Xw4cN18cUXa+zYsTrrrLMqrf3yyy9X3759lZOTo44dO+r444+XJJ144om655571L17dyUmJqpTp07Kzc0NvObee+/VpZdWdkejKPfaa9Kjjx78eXuX7vY974mlOwBABYz1wZ3Bc3Jy7Jw5c8ptW7JkiU444QRHFWHChAmaPHmyxo4dW+XXRM0+Ky2VOnSQ9i7Z7l26qyg8tWghJSa6rRcA4CvGmLnW2pyKHuOIFfZzyy236MMPP9QHH3zgupSa8eGHwVCVni6tWCGVncAPAEA4CFbYzxNPPOG6hJr18MPB8Q03EKoAABHD5UmIL19/Lc2c6Y2TkqTbbnNbDwAgpvg6WPnh/C9UTdTsq1GjguPLLvPOoQIAIEJ8G6zq1KmjzZs3R88v7DhmrdXmzZsDDU19a9ky6d13g/NDuPcjAABV4dtzrJo3b641a9Yorzr3VkOtq1Onjpo3b+66jAP7+9+lvUH9N7+RTjrJbT0AgJjj22CVnJwc6EwOhG3DBumVV4LzSrrcAwAQDt8uBQIR9eST3q1rJCknR+re3W09AICYRLBC7CsokJ56KjgfOdJrCgoAQIQRrBD7Xn5Z2rLFGx99tDRwoNt6AAAxi2CF2FZc7J20vtddd3GLGgBAjSFYIba9/ba0apU3btxYGjrUaTkAgNhGsELssrb87WtuuUVKS3NXDwAg5hGsELtmzJDmz/fGdetKN93ktBwAQOwjWCF2hR6tuuYabrYMAKhxBCvEpnnzpGnTvHFCgnTnnW7rAQDEBYIVYtPo0cHxRRdJdPEHANQCghViz8qV0vjxwfmIEc5KAQDEF4IVYs+jj0olJd74rLOkk092Ww8AIG4QrBBbNm+WXnwxOOdmywCAWkSwQmx55hlp505v3KGDdM45busBAMQVghVix65d0uOPB+cjRnCzZQBArSJYIXa8+qqUl+eNW7SQLrnEbT0AgLhDsEJsKCkp32Lhzjul5GR39QAA4hLBCrFh0iRp+XJv3LChdO21LqsBAMQpghWi3743W77pJik93V09AIC4RbBC9PvXv6RZs7xxaqp0yy1u6wEAxC2CFaLfqFHB8VVXSUcc4a4WAEBcI1ghun37rfTPf3pjY6S77nJbDwAgrhGsEN1CrwTs319q29ZZKQAAEKwQvdaulV5/PTjnZssAAMcIVohejz0mFRV549NPl0491W09AIC4R7BCdNq2TXr22eCco1UAAB8gWCE6Pf+8lJ/vjY8/XurTx209AACIYIVoVFgo/eMfwfmIEVIC/ykDANzjtxGizxtvSD//7I2bNpUuv9xtPQAAlCFYIbqUlpZvsXDbbV63dQAAfCCsYGWMuc0Ys8gY860x5vaybQ8YY9YaY+aXfZ0XkUoBSfrgA2nxYm+ckSHdcIPbegAACJFU3RcaY9pLuk5SV0l7JH1kjJlS9vCj1trRlb4YqK7Qmy1ff73UsKGzUgAA2Fe1g5WkEyT9x1q7U5KMMZ9LGhCRqoCK/Oc/3g2XJSkpSbr9dqflAACwr3CWAhdJOsMYk2mMSZN0nqQWZY8NM8YsNMa8bIw5LOwqAan8zZYvv1xq3txdLQAAVKDawcpau0TS3yRNk/SRpAWSiiU9I6mNpI6S1kl6pKLXG2OuN8bMMcbMycvLq24ZiBfLlkkTJwbnw4e7qwUAgEqEdfK6tfYla21na+0Zkn6RtMxau8FaW2KtLZX0grxzsCp67fPW2hxrbU5WVlY4ZSAePPKIZK03Pu88qX17t/UAAFCBcK8KPLzse0tJAyWNM8Y0DXnKAHlLhkD1bdgg5eYG5yNHOisFAIADCefkdUl6xxiTKalI0s3W2i3GmLHGmI6SrKSVkrgeHuF58kmv27okdekinXGG23oAAKhEWMHKWvvrCrZdGc57AuUUFEhPPRWcjxwpGeOuHgAADoDO6/C3l16Stmzxxm3aSAPo6AEA8C+CFfyrqEj6+9+D87vukhIT3dUDAMBBEKzgX2+/La1e7Y2zsqShQ52WAwDAwRCs4E/Wlr99zbBhUt267uoBAKAKCFbwp+nTpQULvHFamnTzzW7rAQCgCghW8KfQo1XXXCNlZrqrBQCAKiJYwX+++cY7YiVJCQnSHXe4rQcAgCoiWMF/Ro8Oji++WGrd2l0tAAAcAoIV/GXlSumtt4LzESOclQIAwKEiWMFfHn1UKinxxj17Sp07u60HAIBDQLCCf2zeLL34YnDOzZYBAFGGYAX/ePppaedOb5ydLfXq5bYeAAAOEcEK/rBrl/TEE8H5iBHcbBkAEHUIVvCHMWOkvDxv3LKldzUgAABRhmAF90pKyrdYuOMOKTnZXT0AAFQTwQruTZok/fCDNz7sMOnaa52WAwBAdRGs4Ja10t/+FpzfdJOUnu6uHgAAwkCwglszZ0qzZ3vj1FTpllvc1gMAQBgIVnBr1KjgeMgQqUkTd7UAABAmghXcWbRImjLFGxsj3XWX23oAAAgTwQruhF4J2L+/dNxxzkoBACASCFZwY80a6Y03gnNuXwMAiAEEK7jx2GNSUZE3/vWvpVNOcVsPAAARQLBC7du2TXruueB8xAh3tQAAEEEEK9S+556T8vO98QknSOef77YeAAAihGCF2lVYKP3jH8H5iBFSAv8ZAgBiA7/RULveeENat84bN20qXXaZ23oAAIggghVqT2lp+Yagt9/udVsHACBGEKxQe6ZMkZYs8cYZGdINN7itBwCACCNYofaEHq264QapQQN3tQAAUAMIVqgdX30l/etf3jg5WbrtNrf1AABQAwhWqB2hR6suu0xq3txdLQAA1BCCFWre0qXSpEnB+fDhzkoBAKAmEaxQ8x55RLLWG59/vtS+vdt6AACoIQQr1KwNG6QxY4Jzbl8DAIhhBCvUrCee8LqtS1LXrtIZZ7itBwCAGkSwQs0pKJCefjo4HzlSMsZdPQAA1DCCFWrOSy9JW7Z442OOkfr3d1oOAAA1jWCFmlFUJP3978H5XXdJiYnu6gEAoBYQrFAz3npLWr3aG2dlSUOGuK0HAIBaQLBC5FlbviHoLbdIdeu6qwcAgFpCsELkTZsmLVjgjdPSpJtuclsPAAC1hGCFyHv44eD4mmukzEx3tQAAUIsIVoisb76RZszwxomJ0p13uq0HAIBaRLBCZIWeW3XxxVKrVs5KAQCgthGsEDkrVnhXA+7F7WsAAHGGYIXIefRRqbTUG599ttSpk9t6AACoZQQrRMamTdKLLwbnI0e6qwUAAEcIVoiMp5+Wdu3yxh07ekesAACIMwQrhG/XLumJJ4LzESO42TIAIC6FFayMMbcZYxYZY741xtxetq2RMWaaMWZZ2ffDIlIp/Cs311sKlKSjjpIuushpOQAAuFLtYGWMaS/pOkldJWVL6mOMOVbS3ZJmWGuPlTSjbI5YVVIiPfJIcH7HHVJysrt6AABwKJwjVidI+o+1dqe1tljS55IGSOonaUzZc8ZI6h9WhfC3iROlH37wxocd5nVaBwAgToUTrBZJOsMYk2mMSZN0nqQWkppYa9dJUtn3wyt6sTHmemPMHGPMnLy8vDDKgDPWlr99zc03S+np7uoBAMCxagcra+0SSX+TNE3SR5IWSCo+hNc/b63NsdbmZGVlVbcMuDRzpjR7tjdOTZWGDXNbDwAAjoV18rq19iVrbWdr7RmSfpG0TNIGY0xTSSr7vjH8MuFLoUerhg6VmjRxVgoAAH4Q7lWBh5d9bylpoKRxkt6TNKTsKUMkTQ7nM+BTixZJH3zgjY2R7rrLbT0AAPhAUpivf8cYkympSNLN1totxpiHJL1ljLlG0mpJXHsfi0aPDo4HDJCOPdZdLQAA+ERYwcpa++sKtm2W1DOc94XPrVkjvf56cM7tawAAkETndVTHP/4hFZddp/DrX0u/+pXTcgAA8AuCFQ7N1q3S888H5xytAgAggGCFQ/Pcc1J+vjdu10467zy39QAA4CMEK1RdYaG3DLjX8OFSAv8JAQCwF78VUXWvvy6tX++NjzxSuuwyt/UAAOAzBCtUTWmpNGpUcH777V63dQAAEECwQtVMmSJ99503zsiQrr/ebT0AAPgQwQpVE3r7mhtvlBo0cFcLAAA+RbDCwX31lfTvf3vj5GTpttvc1gMAgE+Fe0sbxIPHHw+OL79catbMXS1AFLPWqmBPgTbv2qzNOzerYE+B6qXUU3pKujJSMpSRmqH0lHQlGP7NC1Sk1JaqsLhQu4t3a3fxbhWWhIxDtrdt3FatGrZyUiPBCge2a5f0/vvB+e23OysF8JNSW6otu7YEQtKmnZsC48279p/v/b6nZM9B37teclnYSs0IBK6MlIxyASw0iO27LfBcghoiqLi0eL8AU1mwqWz7fo8d4uuLSouqVOujvR/V7afcXrM/kEoQrHBgU6dKO3Z447ZtpQ4d3NYD1IA9JXvKB6J9wlBFoWnLri2ysjVSz46iHdpRtEMbdmyIyPvVS65XaQhLT64kwBHUfMNaq6LSokMKIKHbD/iaQwg2JbbE9Y+iygqLC519NsEKB/bOO8HxhRdKxrirBQdUXFqsaT9M07Qfp6mktEQpiSlKTUr1vid63yvatnde1W1JCf79a8Naqx1FOw45JBXsKai1Gusk1VHjtMbKrJupjNQM7SzaqfzCfOXvyVd+Yb52FO2I+GfuDWqRsjeoHTCEVeHIWnpKuu+D2t6lp7CPzIQRbHYX73b9Y/CV1MRU1UmqozpJdZSaFDIO2d6yQUtn9fn3b0i4V1hYfhlw0CB3taBSS/KWKHd+rsYuHKt1Betq/PMSTMIhBbFy26oZ5lISU7S7eHe5kBQISPuEpKostUVKg9QGykzLVGbdTC8slY0z62YqMy0zEKAC29MylZacdsD3LLWl2rFnRyBo5e/JV8GegnLha79tZdsL9hSUe05NB7X1Wh+R96tuUKuXXK/ckZwqhZ6SQwtGtfnfk98lmIQKQ0xl4SY1KVV1EqvwnINsD30sJTFFxuf/wCdYoXIzZkjbtnnj1q2ljh2dloOgLbu26M1Fbyp3Qa5mrZ1Vq59daktj7l/RiSZRjeo2Kh+GQgJRudBU9r1R3UZKTkyOeC0JJsELEKkZUkb47xePQS0WJSckH3IIqU6IOdD2pIQk34caPyBYoXIsA/pKSWmJpv04TbnzczXpu0kqLNn/HIIm9Zro8pMuV4sGLbSnZI8Ki71/ce/9l3dgW2nwsdDHq7Kt1JY6+NNXXZ2kOod0BKlxWmPVT63v6+WocMRjUIu0cMJJJMJNamKqEhMSXf8YUEUEK1SsuFiaPDk4v/BCd7XEuSV5SzRmwRiNXThWP+f/vN/jyQnJ6nd8Pw3NHqrex/Su8XOgSkpL9g9qFYW3cLZVEPxSElMCIamiI0h7tx1sqQ3hqY2gVlkIC2wrm+8s2qmUxJRDWno61HATDUtP8BeCFSr2+efS5s3euFkzqWtXt/XEmS27tmj8t+OVOz9XX6/9usLnnNz0ZA3tOFSXtr9UmWmZtVZbYkKi0hLSCDCIiEgHNcA1ghUqtu8yYEJsLpP4SVWX+q7ocIWGZA/RSU1OclAlAOBACFbYX0mJ9O67wTnLgDWqKkt9F7S9QEM7DlXvNr1r5IRpAEBkEKywvy+/lDaUNSY8/HDptNPc1hOD/LzUBwCoPoIV9he6DDhggJTI1SiRUJWlvsPrHa4rO1zJUh8ARCmCFcqztvwyIE1Bw8ZSHwDED4IVyps9W/rpJ2/cqJHUvbvbeqIUS30AEJ8IVihvwoTguF8/KZmjJ1VVUlqi6T9O1yvzXzngUt8VJ12hIR2HqEMTbmgNALGGYIUga/dvs4CD+m7Tdxozf4xeXfhqpUt9fdv21dDsoTr3mHNZ6gOAGEawQtCCBdKPP3rj+vWls892W4+Pbd29VeMXjVfuglz9Z81/KnxO56adNTR7qC496VI1TmtcyxUCAFwgWCEo9GhV375Saqq7Wnxo71Jf7oJcTVwykaU+AMB+CFYIYhmwQiz1AQCqimAFz+LF0pIl3jgtTerd2209jrHUBwCoDoIVPKFHq847zwtXcYalPgBAuAhW8MTxMuDBlvqSEpLU97i+GtpxqH5zzG9Y6gMAVIpgBWn5cu+KQMk7Yf38893WU0ve+/49/d+//6/Spb5OR3QKNPDMqpdVy9UBAKIRwQrlj1b17i1lZLirpZZ8tPwj9Xuz337bs9KydEWHKzQke4iyj8h2UBkAIJoRrBB3y4DFpcW66+O7AnOW+gAAkUKwinerV3v3B5SkpCSvf1WMe3ney1qct1iSlJ6Srm9v+lYtG7R0XBUAIBYkuC4Ajr37bnDcs6d02GHuaqkF+YX5+tOnfwrM7z7tbkIVACBiCFbxLnQZcNAgd3XUklFfjtKGHRskSc0ymumOU+9wXBEAIJYQrOLZunXSF19444QEqd/+J3PHkrXb12r0l6MD8wfPelBpyfHXrwsAUHMIVvFs4kTJWm/cvbuUFdstBe779D7tKt4lSep4REdd0eEKxxUBAGINwSqexdHVgPPXz1fu/NzAfHSv0UpMSHRXEAAgJhGs4tWmTdLnn3tjY6QBA9zWU4OstRr+8XBZeUfnzj/2fPU8uqfjqgAAsYhgFa8mT5ZKSrxxt27SkUe6racGfbT8I81YMUOSlGAS9HCvhx1XBACIVQSreDVhQnAcw8uAxaXFGj5teGB+Xefr1C6rncOKAACxjGAVj7ZulWbMCM4HDnRWSk17Zd4r5ZqBPtDjAbcFAQBiGsEqHr3/vlRU5I1zcqSjjnJbTw0p2FOg+z69LzD//Wm/1xHpRzisCAAQ6whW8ShOmoKO+qJ8M9A7T73TcUUAgFhHsIo3+fnSRx8F5zF6ftXa7Ws16stRgTnNQAEAtYFgFW8++EAqLPTGHTpIxxzjtp4aQjNQAIALYQUrY8wdxphvjTGLjDHjjDF1jDEPGGPWGmPml32dF6liEQFx0BR0wfoFNAMFADiRVN0XGmOaSbpVUjtr7S5jzFuSBpc9/Ki1dnTlr4YTu3Z5R6z2isFgZa3V8GnBZqDnHXsezUABALUm3KXAJEl1jTFJktIk/Rx+SagxU6dKO3Z44+OPl9rFXj+nqT9M1fQfp0sqawZ6Ns1AAQC1p9rBylq7VtJoSaslrZO0zVr7cdnDw4wxC40xLxtjDotAnYiEfZuCGuOulhpQXFqs4R8Hm4Fe2+lanXj4iQ4rAgDEm2oHq7LA1E9Sa0lHSqpnjLlC0jOS2kjqKC9wPVLJ6683xswxxszJy8urbhmoqsJCr3/VXjG4DJg7P1ff5n0ryWsG+j9n/o/jigAA8SacpcCzJa2w1uZZa4skvSupm7V2g7W2xFpbKukFSV0rerG19nlrbY61NicrKyuMMlAlM2ZI27d749atpY4dnZYTaTQDBQD4QTjBarWkU4wxacYYI6mnpCXGmKYhzxkgaVE4BSJC9r0aMMaWAUd9MUrrC9ZLko7MOJJmoAAAJ6p9VaC19mtjzARJ30gqljRP0vOSXjTGdJRkJa2UdEP4ZSIsRUXSpEnBeYx1W6cZKADAL6odrCTJWnu/pPv32XxlOO+JGvD559Ivv3jj5s2lLl3c1hNhf/r0T4FmoNlNsnVlB/4TBAC4Qef1eBC6DDhwoJQQO7t94YaFemX+K4H56HNoBgoAcCd2fsOiYiUl0sSJwXmMXQ04YtqIQDPQ3xzzG5199NmOKwIAxDOCVaz78ktpwwZv3KSJdNppbuuJoI+Wf6SPf/BapyWYBI3qNeogrwAAoGYRrGJd6DLggAFSYmwsk9EMFADgRwSrWFZaGrM3XQ5tBlovuR7NQAEAvkCwimWzZ0tr1njjRo2k7t3d1hMhNAMFAPgVwSqWhR6t6tdPSk52V0sEjf5yNM1AAQC+RLCKVdaWD1Yx0hT05/yfyzUD/euZf1W9lHoOKwIAIIhgFavmz5d+/NEb168v9ezptJxI+dOnf9LOop2SpA5NOuiq7KscVwQAQBDBKlaFHq3q21dKTXVXS4Qs3LBQL897OTB/5JxHaAYKAPAVglWsisGrAWkGCgDwO4JVLFq8WPruO2+clib17u22ngiYunxquWagD/d62HFFAADsj2AVi0KPVp1/vheuolhJaYmGTws2A72m0zVqf3h7hxUBAFAxglUsmjAhOI6BZcDc+blatHGRpLJmoD1oBgoA8CeCVaxZvlxauNAbp6ZK553ntp4w7dsMdORpI9U0o6nDigAAqBzBKtaELgP27i1lZLirJQIe+fIRrStYJ8lrBnrXqXc5rggAgMoRrGJNDF0N+HP+z3r4y+BJ6jQDBQD4HcEqlqxa5d0fUPJuX9O3r9t6wkQzUABAtCFYxZJ33w2Oe/aUDjvMXS1h2rcZ6Oheo2kGCgDwPYJVLImhZcCR00YGmoGee8y56tWml+OKAAA4OIJVrFi3TvryS2+ckCD16+e2njBMXT5VU3+YKslrBjqq16iDvAIAAH8gWMWKiRMl6x3hUffuUlaW23qqqaS0RCOmjQjMf9vxtzQDBQBEDYJVrAhtCjpokLs6wjRmwRj9d+N/JXnNQP985p8dVwQAQNURrGJBXp70+efe2BhpwAC39VRTwZ4C3fvJvYE5zUABANGGYBULJk+WSku9cbduUtPoDCOhzUCbpjelGSgAIOoQrGJBDFwNuC5/XflmoGfRDBQAEH0IVtFuyxZpxozgPEqDVWgz0JMOP0lDsoc4rggAgENHsIp2778vFRV54y5dpJYt3dZTDf/d8F+9PD+kGeg5NAMFAEQnglW0i4FlwJHTR6rUeueI9W7TW+e0OcdxRQAAVA/BKprl50tTpwbnURisPv7hY320/CNJXjPQ0eeMdlwRAADVR7CKZh98IBUWeuMOHaRjjnFbzyEqKS3R8I+HB+Y0AwUARDuCVTQLXQaMwqagoc1A05LTaAYKAIh6BKtotXOnNGVKcB5ly4A79uwo3wy0G81AAQDRj2AVraZO9cKVJB1/vNSundt6DtEjX5VvBjq82/CDvAIAAP8jWEWrKL4acF3+Oj38RbAZ6F/O/AvNQAEAMYFgFY0KC73+VXtFWbC6/7P7taNohySvGejQjkPdFgQAQIQQrKLR9OnS9u3e+OijpY4dnZZzKBZtXKSX5r0UmNMMFAAQSwhW0WjfZUBj3NVyiEZMG0EzUABAzCJYRZuiImny5OA8ipYB920GOqrXKMcVAQAQWQSraPP559Ivv3jj5s29+wNGgZLSEo2YNiIwv7rj1TqpyUkOKwIAIPIIVtEmdBlw4EApITp24asLXtXCDQsl0QwUABC7ouO3MjwlJdK77wbnUdJtfceeHbr302Az0BHdRujIjCMdVgQAQM0gWEWTL76QNm70xk2aSN26ua2niv7+1d/1c/7PkrxmoCO6jTjIKwAAiE4Eq2gSugw4YICU6P82Bevy1+lvX/wtMKcZKAAglhGsokVpafllwCi5GjC0GWj7w9vTDBQAENMIVtFi9mxpzRpvnJkpde/utp4q2K8ZaC+agQIAYhvBKlpMmBAc9+snJSe7q6WKRk4bGWgGek6bc9T7mN6OKwIAoGYRrKKBtVF30+VpP0zTh8s/lCQZGZqBAgDiAsEqGsyfL61Y4Y3r15d69nRazsGUlJZo+LThgfnVHa9WhyYdHFYEAEDtCCtYGWPuMMZ8a4xZZIwZZ4ypY4xpZIyZZoxZVvb9sEgVG7dCj1b17SulprqrpQrGLhxbrhnoX876i+OKAACoHdUOVsaYZpJulZRjrW0vKVHSYEl3S5phrT1W0oyyOarL2vLnV/m8KeiOPTt0zyf3BOY0AwUAxJNwlwKTJNU1xiRJSpP0s6R+ksaUPT5GUv8wPyO+LV4sff+9N65XT+rt7xPAQ5uBHpF+hIZ3G36QVwAAEDuqHaystWsljZa0WtI6SdustR9LamKtXVf2nHWSDo9EoXErdBnwvPOkunXd1XIQ6wvW79cMND0l3WFFAADUrnCWAg+Td3SqtaQjJdUzxlxxCK+/3hgzxxgzJy8vr7plxL4ouhrw/k/LNwO9uuPVjisCAKB2hbMUeLakFdbaPGttkaR3JXWTtMEY01SSyr5vrOjF1trnrbU51tqcrKysMMqIYcuWSQu9k8CVmuodsfKpbzd+qxfnvRiYj+o1imagAIC4E06wWi3pFGNMmjHGSOopaYmk9yQNKXvOEEmTwysxjoUerTr3XCkjw10tBzFyevlmoOcec67jigAAqH1J1X2htfZrY8wESd9IKpY0T9LzktIlvWWMuUZe+LooEoXGpShZBpz+43R9sOwDSTQDBQDEt2oHK0my1t4v6f59NhfKO3qFcKxaJc2Z442Tk73+VT5UUlqi4R/TDBQAAInO6/717rvBcc+eUsOGzko5kLELx2rBhgWSvGagfz7zz44rAgDAHYKVX0XBMuDOop3lmoEOP3W4mtVv5rAiAADcIlj50c8/S1984Y0TE6X+/Z2WU5nQZqBN6jXRiNNGOK4IAAC3CFZ+NHFicNy9u9S4sbtaKrG+YL0e+vdDgTnNQAEAIFj5UxQsAz7w2QPlmoH+ttNvHVcEAIB7BCu/ycuTPv/cGxsjDRjgtp4KLM5brBe+eSEwpxkoAAAegpXfTJ4slXqNNnXaaVLTpm7rqcCIaSMCzUB7Hd1Lvdv4+8bQAADUFoKV30yYEBz7cBmwomagXuN9AABAsPKTLVukGTOC84ED3dVSgX2bgQ7tOFTZR2Q7rAgAAH8hWPnJ++9LxcXeuEsXqWVLt/Xs47WFr5VrBvqXM//iuCIAAPyFYOUnPr4akGagAAAcHMHKL/LzpalTg3OfBatHv3pUa/PXSqIZKAAAlSFY+cWUKVJhoTfOzpaOOcZtPSHWF6zXQ1/QDBQAgIMhWPmFj5cBH5z5oAr2FEiSTsw6UVd3utpxRQAA+BPByg927pQ++CA491Gw2rZ7m16Z/0pg/rez/6akhCSHFQEA4F8EKz+YOtULV5J0/PFSu3Zu6wkxZsGYwK1rTsw6Uecde57jigAA8C+ClR+ENgUdNMhdHfsotaV6avZTgfmwrsNoBgoAwAEQrFwrLJT++c/g3EfLgDN+nKGlm5dKkuqn1tcVHa5wXBEAAP5GsHJt+nRp+3ZvfPTR3hWBPvHk7CcD46HZQ7kSEACAgyBYubbv1YA+WWpbuXWl3v/+/cD8pi43OawGAIDoQLByqahImjw5OPfRMuCzc56VlZUkndPmHLVt3NZxRQAA+B/ByqXPPpN++cUbt2ghde3qtJy9dhXt0ovfvBiY39zlZofVAAAQPQhWLoUuAw4c6JtlwPHfjtfmXZslSUc1OErnH3u+44oAAIgOBCtXSkqkiRODc58sA1pr9eSs4EnrN3W5SYkJiQ4rAgAgehCsXPniC2njRm/cpInUrZvbesrMWjtLc9fNlSTVSaqjazpd47giAACiB8HKlX2XARP9cVQotMXC4PaDlZmW6bAaAACiC8HKhdJSX950eeOOjXrr27cC82FdhjmsBgCA6EOwcmHWLGntWm+cmSl17+62njIvfvOi9pTskSSd0vwUnXzkyY4rAgAgusRHsNqxQ+rfX5o503UlntCjVf36SUlJ7mopU1xarGfmPBOYc7QKAIBDF/vBats26dxzvUacffpIs2e7rcdaXy4Dvvf9e1qzfY0kKSstS4Pa+edm0AAARIvYD1abNknLlnnj/Hypd29p4UJ39cybJ61Y4Y0bNJB69nRXS4jQFgvXn3y9UpNSHVYDAEB0iv1g1aaNd6PjRo28+ZYtUq9e0tKlbuoJPVrVt6+U6j7ALM5brE9XfipJSjSJuuHkGxxXBABAdIr9YCVJ7dtLH38s1a/vzTdu9I4UrVxZu3X4dBnwqVlPBcb9j++vFg1aOKwGAIDoFR/BSpJOPlmaMkWqW9ebr1kjnX229PPPtVfD4sXS999743r1vGVJx7bt3qYxC8YE5twXEACA6oufYCVJp5/uncSekuLNf/jBWxbctKl2Pj/0aNV55wVDnkOvLnhVO4p2SJLaZbVTj1Y93BYEAEAUi69gJXlB6u23g53OFy+WzjlH2rq15j97woTgeJD7q+6stXpqdnAZcFiXYTI+uRE0AADRKP6ClSRdcIH02mvS3hAxb550/vlSQUHNfeayZdJ//+uN69Txjlg5NmPFDH2/2VuazEjJ0BUdrnBcEQAA0S0+g5UkDR4svfBCcP7ll16zzt27a+bzQpcBe/eW0tNr5nMOQWiLhaEdhyojNcNhNQAARL/4DVaSdM010j/+EZx/8ol00UVSUVHkP8tnVwOu2rpK7y99PzC/qctNDqsBACA2xHewkqTbbpMefDA4/+c/pSuukEpKIvcZq1ZJc+Z44+Rkr3+VY8/OeValtlSS1OvoXjq+8fGOKwIAIPoRrCTpj3+U7r47OH/rLem666TS0si8f+jRqrPPlho2jMz7VtPu4t164ZvgMigtFgAAiAyC1V7/+7/SsJAbD7/yinT77V5Tz3D5bBlw/KLx2rxrsySpZYOW6nNcH8cVAQAQGwhWexkjPfaYdPXVwW1PPCHdc0947/vzz96J8ZLX4qFfv/DeLwJCWyzclHOTEhMSHVYDAEDsIFiFSkjwrhS85JLgtv/7P+9oVnVNnBgcd+8uNW5c/feKgFlrZ2n2z7MlSamJqbqm8zVO6wEAIJYQrPaVmCiNHSv1CVkeu+ce6fHHq/d+oU1BfbAMGNpiYXD7wWqc5jboAQAQSwhWFUlO9rqzn3VWcNttt0kvvXRo75OXJ82c6Y2NkQYMiFyN1bBxx0aN/3Z8YD6s67ADPBsAABwqglVl6tTx7ivYrVtw23XXSW++WfX3mDQpeGXhaadJTZtGtMRD9dI3L2lPyR5J0q+a/Uo5R+Y4rQcAgFhDsDqQ9HRpyhSpUydvbq105ZXSe+9V7fU+uhqwuLRYz8x5JjDnaBUAAJFHsDqYhg2ljz+W2rXz5sXFXnf26dMP/LotW6QZM4LzgQNrrMSqeP/79/XT9p8kSVlpWbqo3UVO6wEAIBZVO1gZY9oaY+aHfG03xtxujHnAGLM2ZLv7uw2Hq3Fjado0qU0bb75nj9c24d//rvw1773nhTBJ6tpVatmy5us8gCdnB09av67zdUpNSnVYDQAAsanawcpa+721tqO1tqOkkyXtlLS3t8Cjex+z1n4QgTrdO/JI7whUixbefOdO6fzzpblzK36+j5YBl+Qt0ScrPpEkJZgE3Zhzo9N6AACIVZFaCuwp6Qdr7aoIvZ8/HXWUtwTYpIk3375dOuccadGi8s/Lz/eWD/dyHKxCG4L2a9tPLRq0cFgNAACxK1LBarCkcSHzYcaYhcaYl40xh0XoM/zhuOO8ZcHDyv5Yv/wi9eolLVsWfM6UKVJhoTfOzg4uITqwvXC7xiwYE5hz0joAADUn7GBljEmRdIGkt8s2PSOpjaSOktZJeqSS111vjJljjJmTl5cXbhm166STpKlTpYwMb75+vXdz5dWrvbmPlgHHLhirgj0FkqQTGp+gM1ud6bQeAABiWSSOWP1G0jfW2g2SZK3dYK0tsdaWSnpBUteKXmStfd5am2OtzcnKyopAGbWsSxfvyFTdut589WqpZ0/pxx+lD0JOKxs0yE19kqy15U5aH9Z1mIwxzuoBACDWRSJYXaqQZUBjTGgXzAGSFu33iljx61979wJMSfHmy5dLJ5/sndguSSec4H058smKT/Tdpu8kSRkpGbqyw5XOagEAIB6EFayMMWmSekl6N2Tzw8aY/xpjFko6U9Id4XyG7/XuLY0f791jUJK2bg0+5ngZMPRo1ZDsIcpIzXBYDQAAsS8pnBdba3dKytxnW/wdFunfX3r1VemKK7zu7Hs5DFart63We98HO8Tf3PVmZ7UAABAv6LweKZddJj33XHB+wgneFYGOPDvnWZVa7z6FZx99to5vfLyzWgAAiBdhHbHCPq67zutxNWWKdMstkqMTxXcX79YL37wQmN/chaNVAADUBoJVpF1wgffl0FvfvqVNOzdJklo2aKk+x/VxWg8AAPGCpcAYFNpp/Xc5v1NSAvkZAIDaQLCKMbPWztKstbMkSamJqbqm0zWOKwIAIH4QrGJM6NGqS9pfoqx6Udh8FQCAKEWwiiF5O/I0ftH4wHxYF+4LCABAbSJYxZCX5r2kwhLv5s9dm3VVl2ZdHFcEAEB8IVjFiOLSYj0z55nAnBYLAADUPoJVjPjn0n9q9bbVkqTGaY118YkXO64IAID4Q7CKEaEnrV/X+TrVSarjsBoAAOITwSoGLMlbouk/TpckJZgE3Zhzo+OKAACITwSrGPD07KcD4wvaXqCWDVo6rAYAgPhFsIpy+YX5GrNgTGBOiwUAANwhWEW5sQvHKn9PviTp+MbH66zWZzmuCACA+EWwimLWWj0568nAfFiXYTLGOKwIAID4RrCKYp+u/FRLNi2RJKWnpOvK7CsdVwQAQHwjWEWx0BYLQ7KHqH5qfYfVAAAAglWUWr1ttSZ9Nykwp9M6AADuEayi1HNznlOpLZUk9WzdUydkneC4IgAAQLCKQoXFhXrhmxcCc45WAQDgDwSrKPT24reVtzNPktSifgv1bdvXcUUAAEAiWEWl0BYLv8v5nZISkhxWAwAA9iJYRZnZa2fr67VfS5JSElN0bedrHVcEAAD2IlhFmdAWC5eceImy6mU5rAYAAIQiWEWRTTs36c1Fbwbmw7pyX0AAAPyEYBVFXvrmJRWWFEqSuhzZRV2bdXVcEQAACEWwihIlpSV6Zs4zgTktFgAA8B+CVZSYsmyKVm1bJUnKrJupS9pf4rgiAACwL4JVlAhtsXBd5+tUJ6mOw2oAAEBFCFZR4LtN32naj9MkSQkmQTfm3Oi4IgAAUBGCVRR4evbTgXHf4/rqqIZHOawGAABUhmDlc/mF+RqzYExgTosFAAD8i2Dlc68tfE3bC7dLktpmtlXP1j0dVwQAACpDsPIxa62enB08aX1Y12EyxjisCAAAHAjBysc+W/mZFuctliSlp6TrquyrHFcEAAAOhGDlY6H3Bbyqw1Wqn1rfYTUAAOBgCFY+9dO2nzTpu0mB+c1d6bQOAIDfEax86rm5z6nElkiSzmp9ltpltXNcEQAAOBiClQ8VFhfq+bnPB+bcFxAAgOhAsPKhCYsnKG9nniSpef3muqDtBY4rAgAAVUGw8qHQFgu/y/mdkhKSHFYDAACqimDlM3N/nqv/rPmPJCklMUXXdr7WcUUAAKCqCFY+E9pi4eITL9bh9Q53WA0AADgUBCsf2bxzs9747xuB+bAu3BcQAIBoQrDykZfmvaTCkkJJUs6ROerarKvjigAAwKEgWPlESWmJnp79dGB+c5ebuS8gAABRhmDlEx8s+0Crtq2SJGXWzdQlJ17iuCIAAHCoCFY+Edpi4drO16pucl2H1QAAgOogWPnA95u+18c/fCxJSjAJujHnRscVAQCA6qh2sDLGtDXGzA/52m6Mud0Y08gYM80Ys6zs+2GRLDgWhZ5b1ee4PmrVsJW7YgAAQLVVO1hZa7+31na01naUdLKknZImSrpb0gxr7bGSZpTNUYmCPQXKXZAbmNNiAQCA6BWppcCekn6w1q6S1E/SmLLtYyT1j9BnxKTXFr6m7YXbJUltM9uq59E9HVcEAACqK1LBarCkcWXjJtbadZJU9r3C1uHGmOuNMXOMMXPy8vIiVEZ0sdbqyVnBk9Zv7nKzEgynvQEAEK3C/i1ujEmRdIGktw/lddba5621OdbanKysrHDLiEozV83Ut3nfSpLqJdfTVdlXOa4IAACEIxKHR34j6Rtr7Yay+QZjTFNJKvu+MQKfEZNCWyxclX2VGtRp4LAaAAAQrkgEq0sVXAaUpPckDSkbD5E0OQKfEXPWbF+jiUsmBuY3d7nZYTUAACASwgpWxpg0Sb0kvRuy+SFJvYwxy8oeeyicz4hVz815TiW2RJLUo1UPnXj4iY4rAgAA4UoK58XW2p2SMvfZtlneVYKoRGFxoZ7/5vnAnBYLAADEBi5Bc+CdJe9o4w7v1LPm9Zur3/H9HFcEAAAigWDlQGiLhRtPvlFJCWEdOAQAAD5BsKpl36z7Rl+t+UqSlJyQrGs7X+u4IgAAECkEq1r21KynAuOLT7xYTdKbOKwGAABEEsGqFm3euVlvLHojMB/WlZPWAQCIJQSrWvTyvJe1u3i3JOnkpifrV81+5bgiAAAQSQSrWlJSWqJn5jwTmN/c5WYZYxxWBAAAIo1gVUs+XP6hVmxdIUlqVLeRBrcf7LgiAAAQaQSrWhLaYuHaTteqbnJdh9UAAICaQLCqBUs3L9XUH6ZKkoyMftfld44rAgAANYFgVQuemR08t6rPcX3UqmErd8UAAIAaQ7CqYQV7CvTK/FcCc1osAAAQuwhWNez1ha9rW+E2SdJxmcfp7KPPdlwRAACoKQSrGmSt1ZOzgyet35RzkxIMP3IAAGIVv+Vr0L9W/0uLNi6SJNVLrqchHYc4rggAANQkglUNCm2xcGWHK9WwTkN3xQAAgBpHsKoha7ev1btL3g3Mb+56s8NqAABAbSBY1ZDn5z6vElsiSep+VHe1P7y944oAAEBNI1jVgD0le/Tc3OcCc1osAAAQHwhWNeCdxe9ow44NkqRmGc3Ur20/xxUBAIDaQLCqAaEtFm7MuVHJickOqwEAALWFYBVh89bN05c/fSlJSk5I1nWdr3NcEQAAqC0Eqwh7avZTgfFFJ16kJulNHFYDAABqU5LrAmJFqS3V6C9HK3d+bmDbsC6ctA4AQDwhWEXApp2bNGTSEH2w7IPAtlObn6pTmp/isCoAAFDbCFZh+mL1Fxr8zmCt2b4msO3U5qfq7YveljHGYWUAAKC2cY5VNZXaUj38xcPqntu9XKga0W2EPh/6uZrVb+awOgAA4AJHrKqhoqW/RnUbaUz/MepzXB+HlQEAAJcIVoeosqW/Nwe9qZYNWjqsDAAAuMZSYBUdbOmPUAUAADhiVQWbd27WkElDNGXZlMA2lv4AAMC+CFYH8eVPX+qSCZew9AcAAA6KpcBKlNpSjfpilM545QyW/gAAQJVwxKoCFS39HVbnMI3pP0Z92/Z1WBkAAPAzgtU+Klr6O6X5KRo/aDxHqQAAwAGxFFimsqW/4acO18yhMwlVAADgoDhiJZb+AABAZMR9sPrqp690yYRL9NP2nwLbWPoDAADVEbdLgaW2VKO/HK0zcs8oF6pY+gMAANUVl0esNu/crKGTh+qfS/8Z2MbSHwAACFfcBauKlv5+1exXGj9ovI5qeJTDygAAQLSLm6XAypb+7jr1Ls28eiahCgAAhC0ujlhVtvSX2z9XF7S9wGFlAAAglsR8sFq6eanOfvVslv4AAECNi/mlwJYNWiozLTMwZ+kPAADUlJgPVnWS6uitQW/pqAZHafLgyRp9zmilJKa4LgsAAMSgmF8KlKRjM4/VsluWKTkx2XUpAAAghsX8Eau9CFUAAKCmxU2wAgAAqGlhBStjTENjzARjzHfGmCXGmFONMQ8YY9YaY+aXfZ0XqWIBAAD8LNxzrB6T9JG1dpAxJkVSmqTekh611o4OuzoAAIAoUu1gZYypL+kMSUMlyVq7R9IeY0xkKgMAAIgy4SwFHi0pT9Irxph5xpgXjTH1yh4bZoxZaIx52RhzWPhlAgAA+F84wSpJUmdJz1hrO0naIeluSc9IaiOpo6R1kh6p6MXGmOuNMXOMMXPy8vLCKAMAAMAfwglWayStsdZ+XTafIKmztXaDtbbEWlsq6QVJXSt6sbX2eWttjrU2JysrK4wyAAAA/KHawcpau17ST8aYtmWbekpabIxpGvK0AZIWhVEfAABA1Aj3qsBbJL1edkXgj5KulvS4MaajJCtppaQbwvwMAACAqBBWsLLWzpeUs8/mK8N5TwAAgGhF53UAAIAIIVgBAABECMEKAAAgQghWAAAAEUKwAgAAiBCCFQAAQIQYa63rGmSMyZO0ynUdOKjGkja5LgJVwr6KDuyn6MG+ih61sa+OstZWeNsYXwQrRAdjzBxr7b59y+BD7KvowH6KHuyr6OF6X7EUCAAAECEEKwAAgAghWOFQPO+6AFQZ+yo6sJ+iB/sqejjdV5xjBQAAECEcsQIAAIgQghX2Y4xpYYz51BizxBjzrTHmtrLtjYwx04wxy8q+H+a6VniMMYnGmHnGmH+WzdlXPmSMaWiMmWCM+a7s/69T2Vf+Y4y5o+zvvkXGmHHGmDrsJ/8wxrxsjNlojFkUsq3S/WOM+YMxZrkx5ntjTO+aro9ghYoUS7rLWnuCpFMk3WyMaSfpbkkzrLXHSppRNoc/3CZpScicfeVPj0n6yFp7vKRsefuMfeUjxphmkm6VlGOtbS8pUdJgsZ/8JFfSuftsq3D/lP3uGizpxLLXPG2MSazJ4ghW2I+1dp219puycb68v/ybSeonaUzZ08ZI6u+kQJRjjGku6XxJL4ZsZl/5jDGmvqQzJL0kSdbaPdbarWJf+VGSpLrGmCRJaZJ+FvvJN6y1MyX9ss/myvZPP0lvWmsLrbUrJC2X1LUm6yNY4YCMMa0kdZL0taQm1tp1khe+JB3usDQE/UPSSEmlIdvYV/5ztKQ8Sa+ULdu+aIypJ/aVr1hr10oaLWm1pHWStllrPxb7ye8q2z/NJP0U8rw1ZdtqDMEKlTLGpEt6R9Lt1trtruvB/owxfSRttNbOdV0LDipJUmdJz1hrO0naIZaTfKfs3Jx+klpLOlJSPWPMFW6rQhhMBdtqtB0CwQoVMsYkywtVr1tr3y3bvMEY07Ts8aaSNrqqDwGnSbrAGLNS0puSzjLGvCb2lR+tkbTGWvt12XyCvKDFvvKXsyWtsNbmWWuLJL0rqZvYT35X2f5ZI6lFyPOay1varTEEK+zHGGPknQeyxFr795CH3pM0pGw8RNLk2q4N5Vlr/2CtbW6tbSXvBM1PrLVXiH3lO9ba9ZJ+Msa0LdvUU9Jisa/8ZrWkU4wxaWV/F/aUd54p+8nfKts/70kabIxJNca0lnSspFk1WQgNQrEfY8zpkv4l6b8KnrfzR3nnWb0lqaW8v3wustbuewIhHDHG9JA03FrbxxiTKfaV7xhjOsq7yCBF0o+Srpb3D1z2lY8YY/5H0iXyrpCeJ+laSeliP/mCMWacpB6SGkvaIOl+SZNUyf4xxtwj6bfy9uft1toPa7Q+ghUAAEBksBQIAAAQIQQrAACACCFYAQAARAjBCgAAIEIIVgAAABFCsAIAAIgQghUAAECEEKwAAAAi5P8BCCnCa4CFlZ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/5], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/5], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/5], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/5], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/5], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/5], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/5], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/5], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/5], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/5], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/5], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/5], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/5], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/5], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/5], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/5], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/5], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/5], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/5], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/5], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/5], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/5], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/5], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/5], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/5], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/5], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/5], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/5], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/5], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/5], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/5], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/5], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/5], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/5], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/5], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/5], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/5], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/5], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/5], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/5], Step [900/941], Loss: 0.6396\n",
      "Test accuracy of the network: 68.48341232227489 %\n",
      "Train accuracy of the network: 77.77039596066967 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/10], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/10], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/10], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/10], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/10], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/10], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/10], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/10], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/10], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/10], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/10], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/10], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/10], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/10], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/10], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/10], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/10], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/10], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/10], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/10], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/10], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/10], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/10], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/10], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/10], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/10], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/10], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/10], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/10], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/10], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/10], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/10], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/10], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/10], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/10], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/10], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/10], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/10], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/10], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/10], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/10], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/10], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/10], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/10], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/10], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/10], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/10], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/10], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/10], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/10], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/10], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/10], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/10], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/10], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/10], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/10], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/10], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/10], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/10], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/10], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/10], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/10], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/10], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/10], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/10], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/10], Step [900/941], Loss: 0.6391\n",
      "Test accuracy of the network: 68.00947867298578 %\n",
      "Train accuracy of the network: 78.22880680308265 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/20], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/20], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/20], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/20], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/20], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/20], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/20], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/20], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/20], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/20], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/20], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/20], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/20], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/20], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/20], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/20], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/20], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/20], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/20], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/20], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/20], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/20], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/20], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/20], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/20], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/20], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/20], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/20], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/20], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/20], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/20], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/20], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/20], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/20], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/20], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/20], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/20], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/20], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/20], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/20], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/20], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/20], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/20], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/20], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/20], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/20], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/20], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/20], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/20], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/20], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/20], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/20], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/20], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/20], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/20], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/20], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/20], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/20], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/20], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/20], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/20], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/20], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/20], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/20], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/20], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/20], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/20], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/20], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/20], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/20], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/20], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/20], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/20], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/20], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/20], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/20], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/20], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/20], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/20], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/20], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/20], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/20], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/20], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/20], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/20], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/20], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/20], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/20], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/20], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/20], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/20], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/20], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/20], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/20], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/20], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/20], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/20], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/20], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/20], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/20], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/20], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/20], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/20], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/20], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/20], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/20], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/20], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/20], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/20], Step [900/941], Loss: 0.6399\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 83.23810789263885 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/30], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/30], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/30], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/30], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/30], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/30], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/30], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/30], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/30], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/30], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/30], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/30], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/30], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/30], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/30], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/30], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/30], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/30], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/30], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/30], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/30], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/30], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/30], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/30], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/30], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/30], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/30], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/30], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/30], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/30], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/30], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/30], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/30], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/30], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/30], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/30], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/30], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/30], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/30], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/30], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/30], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/30], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/30], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/30], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/30], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/30], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/30], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/30], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/30], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/30], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/30], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/30], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/30], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/30], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/30], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/30], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/30], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/30], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/30], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/30], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/30], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/30], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/30], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/30], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/30], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/30], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/30], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/30], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/30], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/30], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/30], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/30], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/30], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/30], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/30], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/30], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/30], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/30], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/30], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/30], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/30], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/30], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/30], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/30], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/30], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/30], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/30], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/30], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/30], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/30], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/30], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/30], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/30], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/30], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/30], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/30], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/30], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/30], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/30], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/30], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/30], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/30], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/30], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/30], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/30], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/30], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/30], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/30], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/30], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/30], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/30], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/30], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/30], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/30], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/30], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/30], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/30], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/30], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/30], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/30], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/30], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/30], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/30], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/30], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/30], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/30], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/30], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/30], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/30], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/30], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/30], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/30], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/30], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/30], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/30], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/30], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/30], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/30], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/30], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/30], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/30], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/30], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/30], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/30], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/30], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/30], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/30], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/30], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/30], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/30], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/30], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/30], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/30], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/30], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/30], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/30], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/30], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/30], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/30], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/30], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/30], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/30], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/30], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/30], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/30], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/30], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/30], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/30], Step [900/941], Loss: 0.6356\n",
      "Test accuracy of the network: 66.82464454976304 %\n",
      "Train accuracy of the network: 90.10762689343609 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/40], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/40], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/40], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/40], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/40], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/40], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/40], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/40], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/40], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/40], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/40], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/40], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/40], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/40], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/40], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/40], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/40], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/40], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/40], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/40], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/40], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/40], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/40], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/40], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/40], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/40], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/40], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/40], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/40], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/40], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/40], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/40], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/40], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/40], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/40], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/40], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/40], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/40], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/40], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/40], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/40], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/40], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/40], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/40], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/40], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/40], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/40], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/40], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/40], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/40], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/40], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/40], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/40], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/40], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/40], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/40], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/40], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/40], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/40], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/40], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/40], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/40], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/40], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/40], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/40], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/40], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/40], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/40], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/40], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/40], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/40], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/40], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/40], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/40], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/40], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/40], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/40], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/40], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/40], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/40], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/40], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/40], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/40], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/40], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/40], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/40], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/40], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/40], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/40], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/40], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/40], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/40], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/40], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/40], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/40], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/40], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/40], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/40], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/40], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/40], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/40], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/40], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/40], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/40], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/40], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/40], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/40], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/40], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/40], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/40], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/40], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/40], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/40], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/40], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/40], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/40], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/40], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/40], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/40], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/40], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/40], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/40], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/40], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/40], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/40], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/40], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/40], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/40], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/40], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/40], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/40], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/40], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/40], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/40], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/40], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/40], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/40], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/40], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/40], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/40], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/40], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/40], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/40], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/40], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/40], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/40], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/40], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/40], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/40], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/40], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/40], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/40], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/40], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/40], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/40], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/40], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/40], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/40], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/40], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/40], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/40], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/40], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/40], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/40], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/40], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/40], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/40], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/40], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/40], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/40], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/40], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/40], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/40], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/40], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/40], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/40], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/40], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/40], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/40], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/40], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/40], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/40], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/40], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/40], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/40], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/40], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/40], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/40], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/40], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/40], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/40], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/40], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/40], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/40], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/40], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/40], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/40], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/40], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/40], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/40], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/40], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/40], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/40], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/40], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/40], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/40], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/40], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 67.53554502369668 %\n",
      "Train accuracy of the network: 92.33988838692532 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/50], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/50], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/50], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/50], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/50], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/50], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/50], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/50], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/50], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/50], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/50], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/50], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/50], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/50], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/50], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/50], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/50], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/50], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/50], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/50], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/50], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/50], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/50], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/50], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/50], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/50], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/50], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/50], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/50], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/50], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/50], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/50], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/50], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/50], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/50], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/50], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/50], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/50], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/50], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/50], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/50], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/50], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/50], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/50], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/50], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/50], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/50], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/50], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/50], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/50], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/50], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/50], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/50], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/50], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/50], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/50], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/50], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/50], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/50], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/50], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/50], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/50], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/50], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/50], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/50], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/50], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/50], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/50], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/50], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/50], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/50], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/50], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/50], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/50], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/50], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/50], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/50], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/50], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/50], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/50], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/50], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/50], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/50], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/50], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/50], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/50], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/50], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/50], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/50], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/50], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/50], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/50], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/50], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/50], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/50], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/50], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/50], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/50], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/50], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/50], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/50], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/50], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/50], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/50], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/50], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/50], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/50], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/50], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/50], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/50], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/50], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/50], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/50], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/50], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/50], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/50], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/50], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/50], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/50], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/50], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/50], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/50], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/50], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/50], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/50], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/50], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/50], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/50], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/50], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/50], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/50], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/50], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/50], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/50], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/50], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/50], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/50], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/50], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/50], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/50], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/50], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/50], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/50], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/50], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/50], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/50], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/50], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/50], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/50], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/50], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/50], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/50], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/50], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/50], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/50], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/50], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/50], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/50], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/50], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/50], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/50], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/50], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/50], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/50], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/50], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/50], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/50], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/50], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/50], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/50], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/50], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/50], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/50], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/50], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/50], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/50], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/50], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/50], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/50], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/50], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/50], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/50], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/50], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/50], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/50], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/50], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/50], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/50], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/50], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/50], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/50], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/50], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/50], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/50], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/50], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/50], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/50], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/50], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/50], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/50], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/50], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/50], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/50], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/50], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/50], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/50], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/50], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/50], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/50], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/50], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/50], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/50], Step [900/941], Loss: 0.6352\n",
      "Test accuracy of the network: 66.66666666666667 %\n",
      "Train accuracy of the network: 93.58224820621844 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/60], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/60], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/60], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/60], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/60], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/60], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/60], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/60], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/60], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/60], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/60], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/60], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/60], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/60], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/60], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/60], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/60], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/60], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/60], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/60], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/60], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/60], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/60], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/60], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/60], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/60], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/60], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/60], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/60], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/60], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/60], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/60], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/60], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/60], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/60], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/60], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/60], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/60], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/60], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/60], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/60], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/60], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/60], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/60], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/60], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/60], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/60], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/60], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/60], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/60], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/60], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/60], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/60], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/60], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/60], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/60], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/60], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/60], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/60], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/60], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/60], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/60], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/60], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/60], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/60], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/60], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/60], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/60], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/60], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/60], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/60], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/60], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/60], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/60], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/60], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/60], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/60], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/60], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/60], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/60], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/60], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/60], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/60], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/60], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/60], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/60], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/60], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/60], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/60], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/60], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/60], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/60], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/60], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/60], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/60], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/60], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/60], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/60], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/60], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/60], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/60], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/60], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/60], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/60], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/60], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/60], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/60], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/60], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/60], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/60], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/60], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/60], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/60], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/60], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/60], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/60], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/60], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/60], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/60], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/60], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/60], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/60], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/60], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/60], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/60], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/60], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/60], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/60], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/60], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/60], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/60], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/60], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/60], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/60], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/60], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/60], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/60], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/60], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/60], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/60], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/60], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/60], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/60], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/60], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/60], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/60], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/60], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/60], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/60], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/60], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/60], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/60], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/60], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/60], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/60], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/60], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/60], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/60], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/60], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/60], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/60], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/60], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/60], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/60], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/60], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/60], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/60], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/60], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/60], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/60], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/60], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/60], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/60], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/60], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/60], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/60], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/60], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/60], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/60], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/60], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/60], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/60], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/60], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/60], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/60], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/60], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/60], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/60], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/60], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/60], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/60], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/60], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/60], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/60], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/60], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/60], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/60], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/60], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/60], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/60], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/60], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/60], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/60], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/60], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/60], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/60], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/60], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/60], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/60], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/60], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/60], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/60], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/60], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/60], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/60], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/60], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/60], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/60], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 66.5086887835703 %\n",
      "Train accuracy of the network: 94.41270263087962 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/75], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/75], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/75], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/75], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/75], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/75], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/75], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/75], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/75], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/75], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/75], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/75], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/75], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/75], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/75], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/75], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/75], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/75], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/75], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/75], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/75], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/75], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/75], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/75], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/75], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/75], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/75], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/75], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/75], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/75], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/75], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/75], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/75], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/75], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/75], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/75], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/75], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/75], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/75], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/75], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/75], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/75], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/75], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/75], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/75], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/75], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/75], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/75], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/75], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/75], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/75], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/75], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/75], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/75], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/75], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/75], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/75], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/75], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/75], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/75], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/75], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/75], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/75], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/75], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/75], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/75], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/75], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/75], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/75], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/75], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/75], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/75], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/75], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/75], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/75], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/75], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/75], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/75], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/75], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/75], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/75], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/75], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/75], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/75], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/75], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/75], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/75], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/75], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/75], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/75], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/75], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/75], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/75], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/75], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/75], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/75], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/75], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/75], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/75], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/75], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/75], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/75], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/75], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/75], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/75], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/75], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/75], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/75], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/75], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/75], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/75], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/75], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/75], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/75], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/75], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/75], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/75], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/75], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/75], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/75], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/75], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/75], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/75], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/75], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/75], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/75], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/75], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/75], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/75], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/75], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/75], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/75], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/75], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/75], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/75], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/75], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/75], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/75], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/75], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/75], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/75], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/75], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/75], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/75], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/75], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/75], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/75], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/75], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/75], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/75], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/75], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/75], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/75], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/75], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/75], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/75], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/75], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/75], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/75], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/75], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/75], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/75], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/75], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/75], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/75], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/75], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/75], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/75], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/75], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/75], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/75], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/75], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/75], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/75], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/75], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/75], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/75], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/75], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/75], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/75], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/75], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/75], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/75], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/75], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/75], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/75], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/75], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/75], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/75], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/75], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/75], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/75], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/75], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/75], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/75], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/75], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/75], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/75], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/75], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/75], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/75], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/75], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/75], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/75], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/75], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/75], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/75], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/75], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/75], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/75], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/75], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/75], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/75], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/75], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/75], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/75], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/75], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/75], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/75], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/75], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/75], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/75], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/75], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/75], Step [900/941], Loss: 0.6351\n",
      "Test accuracy of the network: 64.77093206951027 %\n",
      "Train accuracy of the network: 94.93090619186819 %\n",
      "\n",
      "Extracting tokens....\n",
      "\n",
      "Extracting tokens....\n",
      "Data shape for text:  (15052, 4893)\n",
      "Epoch [1/100], Step [100/941], Loss: 0.7026\n",
      "Epoch [1/100], Step [200/941], Loss: 0.6900\n",
      "Epoch [1/100], Step [300/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [400/941], Loss: 0.6444\n",
      "Epoch [1/100], Step [500/941], Loss: 0.6436\n",
      "Epoch [1/100], Step [600/941], Loss: 0.6892\n",
      "Epoch [1/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [1/100], Step [800/941], Loss: 0.6828\n",
      "Epoch [1/100], Step [900/941], Loss: 0.6434\n",
      "Epoch [2/100], Step [100/941], Loss: 0.7045\n",
      "Epoch [2/100], Step [200/941], Loss: 0.6598\n",
      "Epoch [2/100], Step [300/941], Loss: 0.6456\n",
      "Epoch [2/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [2/100], Step [500/941], Loss: 0.6967\n",
      "Epoch [2/100], Step [600/941], Loss: 0.6460\n",
      "Epoch [2/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [2/100], Step [800/941], Loss: 0.6424\n",
      "Epoch [2/100], Step [900/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [100/941], Loss: 0.6596\n",
      "Epoch [3/100], Step [200/941], Loss: 0.6411\n",
      "Epoch [3/100], Step [300/941], Loss: 0.6426\n",
      "Epoch [3/100], Step [400/941], Loss: 0.6402\n",
      "Epoch [3/100], Step [500/941], Loss: 0.6415\n",
      "Epoch [3/100], Step [600/941], Loss: 0.6439\n",
      "Epoch [3/100], Step [700/941], Loss: 0.7038\n",
      "Epoch [3/100], Step [800/941], Loss: 0.6409\n",
      "Epoch [3/100], Step [900/941], Loss: 0.6401\n",
      "Epoch [4/100], Step [100/941], Loss: 0.6408\n",
      "Epoch [4/100], Step [200/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [300/941], Loss: 0.6427\n",
      "Epoch [4/100], Step [400/941], Loss: 0.6400\n",
      "Epoch [4/100], Step [500/941], Loss: 0.6405\n",
      "Epoch [4/100], Step [600/941], Loss: 0.6517\n",
      "Epoch [4/100], Step [700/941], Loss: 0.7026\n",
      "Epoch [4/100], Step [800/941], Loss: 0.6395\n",
      "Epoch [4/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [5/100], Step [100/941], Loss: 0.6418\n",
      "Epoch [5/100], Step [200/941], Loss: 0.6414\n",
      "Epoch [5/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [5/100], Step [400/941], Loss: 0.6378\n",
      "Epoch [5/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [5/100], Step [600/941], Loss: 0.6408\n",
      "Epoch [5/100], Step [700/941], Loss: 0.7021\n",
      "Epoch [5/100], Step [800/941], Loss: 0.6394\n",
      "Epoch [5/100], Step [900/941], Loss: 0.6396\n",
      "Epoch [6/100], Step [100/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [6/100], Step [300/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [6/100], Step [500/941], Loss: 0.6397\n",
      "Epoch [6/100], Step [600/941], Loss: 0.6412\n",
      "Epoch [6/100], Step [700/941], Loss: 0.7018\n",
      "Epoch [6/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [6/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [7/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [7/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [7/100], Step [500/941], Loss: 0.6371\n",
      "Epoch [7/100], Step [600/941], Loss: 0.6399\n",
      "Epoch [7/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [7/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [7/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [8/100], Step [200/941], Loss: 0.6392\n",
      "Epoch [8/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [8/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [8/100], Step [500/941], Loss: 0.6382\n",
      "Epoch [8/100], Step [600/941], Loss: 0.6401\n",
      "Epoch [8/100], Step [700/941], Loss: 0.7016\n",
      "Epoch [8/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [8/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [9/100], Step [100/941], Loss: 0.6396\n",
      "Epoch [9/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [9/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [9/100], Step [400/941], Loss: 0.6394\n",
      "Epoch [9/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [9/100], Step [600/941], Loss: 0.6398\n",
      "Epoch [9/100], Step [700/941], Loss: 0.7017\n",
      "Epoch [9/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [9/100], Step [900/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [10/100], Step [300/941], Loss: 0.6402\n",
      "Epoch [10/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [10/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [10/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [10/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [10/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [10/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [11/100], Step [100/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [200/941], Loss: 0.6395\n",
      "Epoch [11/100], Step [300/941], Loss: 0.6399\n",
      "Epoch [11/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [11/100], Step [600/941], Loss: 0.7004\n",
      "Epoch [11/100], Step [700/941], Loss: 0.7015\n",
      "Epoch [11/100], Step [800/941], Loss: 0.6392\n",
      "Epoch [11/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [12/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [12/100], Step [300/941], Loss: 0.6400\n",
      "Epoch [12/100], Step [400/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [500/941], Loss: 0.6392\n",
      "Epoch [12/100], Step [600/941], Loss: 0.6395\n",
      "Epoch [12/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [12/100], Step [800/941], Loss: 0.6391\n",
      "Epoch [12/100], Step [900/941], Loss: 0.6389\n",
      "Epoch [13/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [13/100], Step [200/941], Loss: 0.6394\n",
      "Epoch [13/100], Step [300/941], Loss: 0.6403\n",
      "Epoch [13/100], Step [400/941], Loss: 0.6356\n",
      "Epoch [13/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [600/941], Loss: 0.6397\n",
      "Epoch [13/100], Step [700/941], Loss: 0.7014\n",
      "Epoch [13/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [13/100], Step [900/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [200/941], Loss: 0.6393\n",
      "Epoch [14/100], Step [300/941], Loss: 0.6398\n",
      "Epoch [14/100], Step [400/941], Loss: 0.6376\n",
      "Epoch [14/100], Step [500/941], Loss: 0.6352\n",
      "Epoch [14/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [14/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [14/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [14/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [15/100], Step [100/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [15/100], Step [300/941], Loss: 0.6351\n",
      "Epoch [15/100], Step [400/941], Loss: 0.6384\n",
      "Epoch [15/100], Step [500/941], Loss: 0.6395\n",
      "Epoch [15/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [15/100], Step [700/941], Loss: 0.7013\n",
      "Epoch [15/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [15/100], Step [900/941], Loss: 0.6377\n",
      "Epoch [16/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [16/100], Step [200/941], Loss: 0.6376\n",
      "Epoch [16/100], Step [300/941], Loss: 0.6353\n",
      "Epoch [16/100], Step [400/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [500/941], Loss: 0.6372\n",
      "Epoch [16/100], Step [600/941], Loss: 0.6391\n",
      "Epoch [16/100], Step [700/941], Loss: 0.7008\n",
      "Epoch [16/100], Step [800/941], Loss: 0.6383\n",
      "Epoch [16/100], Step [900/941], Loss: 0.6367\n",
      "Epoch [17/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [17/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [17/100], Step [300/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [500/941], Loss: 0.6390\n",
      "Epoch [17/100], Step [600/941], Loss: 0.6394\n",
      "Epoch [17/100], Step [700/941], Loss: 0.6995\n",
      "Epoch [17/100], Step [800/941], Loss: 0.6387\n",
      "Epoch [17/100], Step [900/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [18/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [18/100], Step [300/941], Loss: 0.6397\n",
      "Epoch [18/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [18/100], Step [500/941], Loss: 0.6355\n",
      "Epoch [18/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [18/100], Step [700/941], Loss: 0.7001\n",
      "Epoch [18/100], Step [800/941], Loss: 0.6375\n",
      "Epoch [18/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [200/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [300/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [19/100], Step [500/941], Loss: 0.6383\n",
      "Epoch [19/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [19/100], Step [700/941], Loss: 0.6994\n",
      "Epoch [19/100], Step [800/941], Loss: 0.6390\n",
      "Epoch [19/100], Step [900/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [200/941], Loss: 0.6390\n",
      "Epoch [20/100], Step [300/941], Loss: 0.6393\n",
      "Epoch [20/100], Step [400/941], Loss: 0.6398\n",
      "Epoch [20/100], Step [500/941], Loss: 0.6373\n",
      "Epoch [20/100], Step [600/941], Loss: 0.6392\n",
      "Epoch [20/100], Step [700/941], Loss: 0.6963\n",
      "Epoch [20/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [20/100], Step [900/941], Loss: 0.6399\n",
      "Epoch [21/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [21/100], Step [200/941], Loss: 0.6407\n",
      "Epoch [21/100], Step [300/941], Loss: 0.6369\n",
      "Epoch [21/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [21/100], Step [500/941], Loss: 0.6345\n",
      "Epoch [21/100], Step [600/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [700/941], Loss: 0.6964\n",
      "Epoch [21/100], Step [800/941], Loss: 0.6388\n",
      "Epoch [21/100], Step [900/941], Loss: 0.6381\n",
      "Epoch [22/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [300/941], Loss: 0.6363\n",
      "Epoch [22/100], Step [400/941], Loss: 0.6391\n",
      "Epoch [22/100], Step [500/941], Loss: 0.6347\n",
      "Epoch [22/100], Step [600/941], Loss: 0.6393\n",
      "Epoch [22/100], Step [700/941], Loss: 0.7002\n",
      "Epoch [22/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [22/100], Step [900/941], Loss: 0.6386\n",
      "Epoch [23/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [200/941], Loss: 0.6356\n",
      "Epoch [23/100], Step [300/941], Loss: 0.6368\n",
      "Epoch [23/100], Step [400/941], Loss: 0.6390\n",
      "Epoch [23/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [23/100], Step [600/941], Loss: 0.6361\n",
      "Epoch [23/100], Step [700/941], Loss: 0.6837\n",
      "Epoch [23/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [23/100], Step [900/941], Loss: 0.6363\n",
      "Epoch [24/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [24/100], Step [300/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [400/941], Loss: 0.6388\n",
      "Epoch [24/100], Step [500/941], Loss: 0.6349\n",
      "Epoch [24/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [24/100], Step [700/941], Loss: 0.6393\n",
      "Epoch [24/100], Step [800/941], Loss: 0.6384\n",
      "Epoch [24/100], Step [900/941], Loss: 0.6374\n",
      "Epoch [25/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [25/100], Step [300/941], Loss: 0.6352\n",
      "Epoch [25/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [25/100], Step [500/941], Loss: 0.6348\n",
      "Epoch [25/100], Step [600/941], Loss: 0.6390\n",
      "Epoch [25/100], Step [700/941], Loss: 0.6459\n",
      "Epoch [25/100], Step [800/941], Loss: 0.6357\n",
      "Epoch [25/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [26/100], Step [100/941], Loss: 0.6391\n",
      "Epoch [26/100], Step [200/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [300/941], Loss: 0.6343\n",
      "Epoch [26/100], Step [400/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [500/941], Loss: 0.6357\n",
      "Epoch [26/100], Step [600/941], Loss: 0.6354\n",
      "Epoch [26/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [26/100], Step [800/941], Loss: 0.6389\n",
      "Epoch [26/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [27/100], Step [100/941], Loss: 0.6393\n",
      "Epoch [27/100], Step [200/941], Loss: 0.6373\n",
      "Epoch [27/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [27/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [27/100], Step [600/941], Loss: 0.6382\n",
      "Epoch [27/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [27/100], Step [800/941], Loss: 0.6377\n",
      "Epoch [27/100], Step [900/941], Loss: 0.6357\n",
      "Epoch [28/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [28/100], Step [200/941], Loss: 0.6386\n",
      "Epoch [28/100], Step [300/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [400/941], Loss: 0.6355\n",
      "Epoch [28/100], Step [500/941], Loss: 0.6340\n",
      "Epoch [28/100], Step [600/941], Loss: 0.6358\n",
      "Epoch [28/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [28/100], Step [800/941], Loss: 0.6353\n",
      "Epoch [28/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [29/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [29/100], Step [200/941], Loss: 0.6372\n",
      "Epoch [29/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [29/100], Step [400/941], Loss: 0.6351\n",
      "Epoch [29/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [29/100], Step [600/941], Loss: 0.6379\n",
      "Epoch [29/100], Step [700/941], Loss: 0.6355\n",
      "Epoch [29/100], Step [800/941], Loss: 0.6356\n",
      "Epoch [29/100], Step [900/941], Loss: 0.6358\n",
      "Epoch [30/100], Step [100/941], Loss: 0.6389\n",
      "Epoch [30/100], Step [200/941], Loss: 0.6367\n",
      "Epoch [30/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [30/100], Step [400/941], Loss: 0.6363\n",
      "Epoch [30/100], Step [500/941], Loss: 0.6337\n",
      "Epoch [30/100], Step [600/941], Loss: 0.6344\n",
      "Epoch [30/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [30/100], Step [800/941], Loss: 0.6369\n",
      "Epoch [30/100], Step [900/941], Loss: 0.6356\n",
      "Epoch [31/100], Step [100/941], Loss: 0.6390\n",
      "Epoch [31/100], Step [200/941], Loss: 0.6375\n",
      "Epoch [31/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [31/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [31/100], Step [500/941], Loss: 0.6341\n",
      "Epoch [31/100], Step [600/941], Loss: 0.6345\n",
      "Epoch [31/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [31/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [31/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [32/100], Step [100/941], Loss: 0.6385\n",
      "Epoch [32/100], Step [200/941], Loss: 0.6379\n",
      "Epoch [32/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [32/100], Step [400/941], Loss: 0.6345\n",
      "Epoch [32/100], Step [500/941], Loss: 0.6344\n",
      "Epoch [32/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [32/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [32/100], Step [800/941], Loss: 0.6343\n",
      "Epoch [32/100], Step [900/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [100/941], Loss: 0.6394\n",
      "Epoch [33/100], Step [200/941], Loss: 0.6344\n",
      "Epoch [33/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [33/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [33/100], Step [500/941], Loss: 0.6335\n",
      "Epoch [33/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [33/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [33/100], Step [800/941], Loss: 0.6355\n",
      "Epoch [33/100], Step [900/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [34/100], Step [300/941], Loss: 0.6331\n",
      "Epoch [34/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [500/941], Loss: 0.6342\n",
      "Epoch [34/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [34/100], Step [700/941], Loss: 0.6385\n",
      "Epoch [34/100], Step [800/941], Loss: 0.6344\n",
      "Epoch [34/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [35/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [200/941], Loss: 0.6353\n",
      "Epoch [35/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [35/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [35/100], Step [500/941], Loss: 0.6333\n",
      "Epoch [35/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [35/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [35/100], Step [800/941], Loss: 0.6340\n",
      "Epoch [35/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [36/100], Step [200/941], Loss: 0.6348\n",
      "Epoch [36/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [36/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [36/100], Step [500/941], Loss: 0.6332\n",
      "Epoch [36/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [36/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [36/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [36/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [37/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [37/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [37/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [37/100], Step [400/941], Loss: 0.6341\n",
      "Epoch [37/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [37/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [37/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [37/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [37/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [38/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [38/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [38/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [400/941], Loss: 0.6342\n",
      "Epoch [38/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [38/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [38/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [38/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [38/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [39/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [39/100], Step [400/941], Loss: 0.6340\n",
      "Epoch [39/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [39/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [39/100], Step [700/941], Loss: 0.6384\n",
      "Epoch [39/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [39/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [40/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [40/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [40/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [40/100], Step [400/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [40/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [40/100], Step [700/941], Loss: 0.6379\n",
      "Epoch [40/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [40/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [41/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [41/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [41/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [400/941], Loss: 0.6334\n",
      "Epoch [41/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [41/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [41/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [41/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [41/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [42/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [42/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [42/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [42/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [42/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [42/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [42/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [42/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [43/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [400/941], Loss: 0.6335\n",
      "Epoch [43/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [43/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [43/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [43/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [43/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [44/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [44/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [44/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [44/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [44/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [44/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [44/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [45/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [45/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [45/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [45/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [45/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [45/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [45/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [45/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [46/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [46/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [46/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [46/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [46/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [46/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [46/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [47/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [47/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [47/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [47/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [47/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [47/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [47/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [47/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [100/941], Loss: 0.6354\n",
      "Epoch [48/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [48/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [48/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [500/941], Loss: 0.6330\n",
      "Epoch [48/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [48/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [48/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [48/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [49/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [49/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [49/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [49/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [49/100], Step [600/941], Loss: 0.6338\n",
      "Epoch [49/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [49/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [49/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [50/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [50/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [50/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [50/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [50/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [50/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [51/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [51/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [51/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [51/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [51/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [51/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [52/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [52/100], Step [300/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [52/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [52/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [52/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [52/100], Step [800/941], Loss: 0.6336\n",
      "Epoch [52/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [300/941], Loss: 0.6335\n",
      "Epoch [53/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [53/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [53/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [53/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [53/100], Step [800/941], Loss: 0.6339\n",
      "Epoch [53/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [54/100], Step [300/941], Loss: 0.6333\n",
      "Epoch [54/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [54/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [54/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [54/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [54/100], Step [800/941], Loss: 0.6335\n",
      "Epoch [54/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [55/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [55/100], Step [300/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [55/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [55/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [55/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [55/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [55/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [56/100], Step [200/941], Loss: 0.6382\n",
      "Epoch [56/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [56/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [56/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [56/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [56/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [56/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [56/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [57/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [57/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [57/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [57/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [57/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [57/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [57/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [58/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [58/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [58/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [58/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [58/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [58/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [58/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [59/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [59/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [59/100], Step [500/941], Loss: 0.6328\n",
      "Epoch [59/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [59/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [59/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [59/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [60/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [60/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [60/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [60/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [60/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [60/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [60/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [60/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [61/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [61/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [61/100], Step [400/941], Loss: 0.6331\n",
      "Epoch [61/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [61/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [61/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [61/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [61/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [62/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [62/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [62/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [62/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [62/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [62/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [62/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [63/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [63/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [63/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [63/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [63/100], Step [600/941], Loss: 0.6339\n",
      "Epoch [63/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [63/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [63/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [64/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [64/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [64/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [64/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [64/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [64/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [64/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [64/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [64/100], Step [900/941], Loss: 0.6354\n",
      "Epoch [65/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [65/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [65/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [65/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [65/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [65/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [65/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [65/100], Step [800/941], Loss: 0.6342\n",
      "Epoch [65/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [66/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [66/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [66/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [66/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [66/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [66/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [66/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [66/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [66/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [67/100], Step [100/941], Loss: 0.6365\n",
      "Epoch [67/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [67/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [67/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [67/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [67/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [67/100], Step [700/941], Loss: 0.6388\n",
      "Epoch [67/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [67/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [68/100], Step [100/941], Loss: 0.6352\n",
      "Epoch [68/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [68/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [68/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [68/100], Step [500/941], Loss: 0.6329\n",
      "Epoch [68/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [68/100], Step [700/941], Loss: 0.6387\n",
      "Epoch [68/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [68/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [69/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [69/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [69/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [69/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [69/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [69/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [69/100], Step [700/941], Loss: 0.6357\n",
      "Epoch [69/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [69/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [70/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [70/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [70/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [70/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [70/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [70/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [70/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [71/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [71/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [71/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [71/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [71/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [71/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [71/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [72/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [72/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [72/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [72/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [72/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [72/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [72/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [73/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [73/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [73/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [73/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [73/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [73/100], Step [600/941], Loss: 0.6337\n",
      "Epoch [73/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [73/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [73/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [74/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [74/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [74/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [74/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [74/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [74/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [74/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [75/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [75/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [75/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [75/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [75/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [75/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [75/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [76/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [76/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [76/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [76/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [76/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [76/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [76/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [77/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [77/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [77/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [77/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [77/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [77/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [77/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [77/100], Step [900/941], Loss: 0.6352\n",
      "Epoch [78/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [78/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [78/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [78/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [78/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [78/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [78/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [78/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [200/941], Loss: 0.6343\n",
      "Epoch [79/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [79/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [79/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [79/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [79/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [79/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [79/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [80/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [80/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [80/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [80/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [80/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [80/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [80/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [81/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [81/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [81/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [81/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [81/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [81/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [81/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [82/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [82/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [82/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [82/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [82/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [82/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [82/100], Step [900/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [83/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [83/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [83/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [83/100], Step [600/941], Loss: 0.6336\n",
      "Epoch [83/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [83/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [83/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [84/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [84/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [84/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [84/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [84/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [84/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [84/100], Step [800/941], Loss: 0.6337\n",
      "Epoch [84/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [85/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [85/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [85/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [85/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [85/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [85/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [85/100], Step [900/941], Loss: 0.6348\n",
      "Epoch [86/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [200/941], Loss: 0.6340\n",
      "Epoch [86/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [86/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [86/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [86/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [86/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [86/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [86/100], Step [900/941], Loss: 0.6349\n",
      "Epoch [87/100], Step [100/941], Loss: 0.6355\n",
      "Epoch [87/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [87/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [87/100], Step [400/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [87/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [87/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [87/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [87/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [88/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [88/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [88/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [88/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [88/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [88/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [88/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [88/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [89/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [89/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [89/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [89/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [89/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [89/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [89/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [89/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [90/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [90/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [90/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [90/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [90/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [90/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [90/100], Step [900/941], Loss: 0.6347\n",
      "Epoch [91/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [200/941], Loss: 0.6349\n",
      "Epoch [91/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [91/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [91/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [91/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [91/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [91/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [91/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [92/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [92/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [92/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [92/100], Step [500/941], Loss: 0.6327\n",
      "Epoch [92/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [92/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [92/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [92/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [93/100], Step [100/941], Loss: 0.6353\n",
      "Epoch [93/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [93/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [93/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [93/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [93/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [93/100], Step [800/941], Loss: 0.6334\n",
      "Epoch [93/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [94/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [200/941], Loss: 0.6342\n",
      "Epoch [94/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [94/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [94/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [94/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [94/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [94/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [94/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [95/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [95/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [95/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [95/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [95/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [95/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [95/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [95/100], Step [800/941], Loss: 0.6332\n",
      "Epoch [95/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [96/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [96/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [96/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [96/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [96/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [96/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [96/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [96/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [97/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [97/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [97/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [97/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [97/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [97/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [97/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [97/100], Step [900/941], Loss: 0.6338\n",
      "Epoch [98/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [98/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [98/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [98/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [98/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [98/100], Step [700/941], Loss: 0.6351\n",
      "Epoch [98/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [98/100], Step [900/941], Loss: 0.6379\n",
      "Epoch [99/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [99/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [99/100], Step [300/941], Loss: 0.6328\n",
      "Epoch [99/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [99/100], Step [500/941], Loss: 0.6334\n",
      "Epoch [99/100], Step [600/941], Loss: 0.6335\n",
      "Epoch [99/100], Step [700/941], Loss: 0.6352\n",
      "Epoch [99/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [99/100], Step [900/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [100/941], Loss: 0.6351\n",
      "Epoch [100/100], Step [200/941], Loss: 0.6339\n",
      "Epoch [100/100], Step [300/941], Loss: 0.6329\n",
      "Epoch [100/100], Step [400/941], Loss: 0.6330\n",
      "Epoch [100/100], Step [500/941], Loss: 0.6326\n",
      "Epoch [100/100], Step [600/941], Loss: 0.6334\n",
      "Epoch [100/100], Step [700/941], Loss: 0.6353\n",
      "Epoch [100/100], Step [800/941], Loss: 0.6333\n",
      "Epoch [100/100], Step [900/941], Loss: 0.6339\n",
      "Test accuracy of the network: 61.45339652448657 %\n",
      "Train accuracy of the network: 95.31623704491098 %\n"
     ]
    }
   ],
   "source": [
    "liar_test_accuracies = []\n",
    "liar_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('liar', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    liar_test_accuracies.append(test_accuracy)\n",
    "    liar_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, liar_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, liar_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_test_accuracies = []\n",
    "fnn_train_accuracies = []\n",
    "\n",
    "for num_epoch in num_epochs_used:\n",
    "    test_accuracy, train_accuracy = trainAndTestSimpleModel('fnn', num_epochs=num_epoch, print_epoch_mod=100)\n",
    "    fnn_test_accuracies.append(test_accuracy)\n",
    "    fnn_train_accuracies.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(num_epochs_used, fnn_train_accuracies, 'r-', lw=3, label='Train Accuracy')\n",
    "ax.plot(num_epochs_used, fnn_test_accuracies, 'g-', lw=3, label='Test Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
